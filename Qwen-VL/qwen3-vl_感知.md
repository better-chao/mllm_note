# Qwen3-VL Embodied/Spatial Understandingèƒ½åŠ›æ¥æºä¸æå‡æ–¹æ¡ˆ

## ä¸€ã€å½“å‰èƒ½åŠ›æ¥æºåˆ†æ

### 1.1 é¢„è®­ç»ƒæ•°æ®æ¥æº

æ ¹æ®PDFç¬¬7é¡µSection 3.2.5çš„æè¿°ï¼ŒQwen3-VLçš„Embodied/Spatial Understandingèƒ½åŠ›ä¸»è¦æ¥è‡ª**é¢„è®­ç»ƒStage 1å’ŒStage 2é˜¶æ®µ**çš„ä»¥ä¸‹æ•°æ®ï¼š

#### **Spatial Understandingæ•°æ®**
```
æ•°æ®è§„æ¨¡ï¼šæœªæ˜ç¡®è¯´æ˜æ€»é‡
æ•°æ®ç±»å‹ï¼š
â”œâ”€ å…³ç³»æ ‡æ³¨ (Relational Annotations)
â”‚  â””â”€ ä¾‹ï¼š"æ¯å­åœ¨ç¬”è®°æœ¬ç”µè„‘å·¦ä¾§"
â”œâ”€ åŠŸèƒ½å±æ€§æ ‡ç­¾ (Affordance Labels)  
â”‚  â””â”€ ä¾‹ï¼š"å¯æŠ“å–"ã€"å¯æŒ‰å‹"ã€"å¯å"
â””â”€ åŠ¨ä½œæ¡ä»¶æŸ¥è¯¢ (Action-conditioned Queries)
   â””â”€ ä¾‹ï¼š"è¦æ‹¿åˆ°æ˜¾ç¤ºå™¨åé¢çš„ä¹¦ï¼Œæˆ‘åº”è¯¥å…ˆç§»åŠ¨ä»€ä¹ˆï¼Ÿ"

æ•°æ®æ¥æºï¼š
â”œâ”€ ç­–åˆ’çš„çœŸå®ä¸–ç•Œåœºæ™¯
â””â”€ åˆæˆç”Ÿæˆçš„å¸ƒå±€

æŸ¥è¯¢ç”Ÿæˆæ–¹å¼ï¼š
â”œâ”€ æ¨¡æ¿åŒ–æ–¹æ³•
â””â”€ LLMç”Ÿæˆï¼ˆç¡®ä¿å¤šæ ·æ€§å’Œå¤æ‚æ€§ï¼‰
```

#### **3D Groundingæ•°æ®**
```
æ•°æ®æ¥æºï¼šå…¬å¼€å®¤å†…/å®¤å¤–åœºæ™¯é›†åˆï¼ˆæœªæ˜ç¡®è¯´æ˜å…·ä½“æ•°æ®é›†ï¼‰
æ•°æ®æ ¼å¼ï¼š
â”œâ”€ å•è§†è§’ç›¸æœºå›¾åƒ
â”œâ”€ è‡ªç„¶è¯­è¨€æŒ‡ä»£è¡¨è¾¾
â””â”€ 9-DoF 3Dè¾¹ç•Œæ¡†ï¼ˆJSONæ ¼å¼ï¼‰
   â”œâ”€ x_center, y_center, z_center
   â”œâ”€ x_size, y_size, z_size
   â””â”€ roll, pitch, yaw

æ•°æ®å¤„ç†ï¼š
â”œâ”€ è¿‡æ»¤ä¸¥é‡é®æŒ¡å’Œä¸å‡†ç¡®æ ‡ç­¾
â”œâ”€ ä½¿ç”¨Omni3Dæ–¹æ³•ç»Ÿä¸€åˆ°è™šæ‹Ÿç›¸æœºåæ ‡ç³»
â””â”€ åˆæˆä¸°å¯Œçš„æè¿°æ€§captions
```

### 1.2 è®­ç»ƒé˜¶æ®µåˆ†å¸ƒ

| é˜¶æ®µ | æ•°æ®ç±»å‹ | åºåˆ—é•¿åº¦ | æ•°æ®é‡ |
|-----|---------|---------|--------|
| **Stage 0** | ä¸åŒ…å«ç©ºé—´æ•°æ® | 8K | 67B tokens |
| **Stage 1** | å¼•å…¥Spatial + 3Dæ•°æ® | 8K | ~1T tokens |
| **Stage 2** | å¢åŠ agentå¯¼å‘æ•°æ® | 32K | ~1T tokens |
| **Stage 3** | é•¿æ–‡æ¡£/è§†é¢‘ä¸ºä¸» | 256K | 100B tokens |

**å…³é”®å‘ç°**ï¼š
- ç©ºé—´ç†è§£æ•°æ®ä¸»è¦åœ¨**Stage 1çš„1T tokens**ä¸­ï¼Œä½†å…·ä½“å æ¯”æœªé€éœ²
- Stage 2å¢åŠ çš„agentæ•°æ®å¯èƒ½è¿›ä¸€æ­¥å¢å¼ºäº†ç©ºé—´æ¨ç†èƒ½åŠ›

---

## äºŒã€é’ˆå¯¹åœ°å›¾å…¬å¸çš„æå‡æ–¹æ¡ˆ

### æ–¹æ¡ˆæ¦‚è§ˆ

è€ƒè™‘åˆ°è´µå…¬å¸æ‹¥æœ‰ä¸°å¯Œçš„é“è·¯æ•°æ®èµ„æºï¼Œä»¥ä¸‹æ–¹æ¡ˆå°†**å……åˆ†åˆ©ç”¨åœ°å›¾æ•°æ®çš„ç‹¬ç‰¹ä¼˜åŠ¿**ï¼š

```
æå‡æ–¹å‘
â”œâ”€ æ•°æ®ç«¯ï¼ˆDataï¼‰
â”‚  â”œâ”€ é“è·¯åœºæ™¯æ•°æ®æ„å»º
â”‚  â”œâ”€ å¯¼èˆªä»»åŠ¡æ•°æ®åˆæˆ
â”‚  â””â”€ åœ°å›¾è¦ç´ è¯†åˆ«æ•°æ®
â”œâ”€ æ¨¡å‹ç»“æ„ç«¯ï¼ˆArchitectureï¼‰
â”‚  â”œâ”€ åœ°å›¾ç‰¹åŒ–çš„ä½ç½®ç¼–ç 
â”‚  â”œâ”€ å¤šå°ºåº¦ç©ºé—´ç‰¹å¾èåˆ
â”‚  â””â”€ æ‹“æ‰‘å…³ç³»å»ºæ¨¡æ¨¡å—
â””â”€ è®­ç»ƒç­–ç•¥ç«¯ï¼ˆTrainingï¼‰
   â”œâ”€ è¯¾ç¨‹å­¦ä¹ ç­–ç•¥
   â”œâ”€ å¯¹æ¯”å­¦ä¹ å¢å¼º
   â””â”€ å¤šä»»åŠ¡è”åˆè®­ç»ƒ
```

---

## ä¸‰ã€æ•°æ®ç«¯æå‡æ–¹æ¡ˆ

### 3.1 é“è·¯åœºæ™¯ç©ºé—´ç†è§£æ•°æ®æ„å»º

#### **æ–¹æ¡ˆAï¼šè¡—æ™¯å›¾åƒ + åœ°å›¾æ ‡æ³¨èåˆ**

**æ•°æ®æ„å»ºæµç¨‹**ï¼š

```python
# ä¼ªä»£ç ç¤ºä¾‹
æ•°æ®æ„å»ºPipeline:
1. é‡‡é›†è¡—æ™¯å›¾åƒåºåˆ—
   - è½¦è½½æ‘„åƒå¤´é‡‡é›†
   - è¦†ç›–åŸå¸‚é“è·¯ã€é«˜é€Ÿã€ä¹¡æ‘é“è·¯
   - åŒ…å«å¤šç§å¤©æ°”/å…‰ç…§æ¡ä»¶
   
2. åœ°å›¾æ•°æ®å¯¹é½
   - GPSè½¨è¿¹åŒ¹é…åˆ°è·¯ç½‘
   - æå–å‘¨è¾¹POIä¿¡æ¯
   - å…³è”è½¦é“çº¿ã€äº¤é€šæ ‡å¿—ã€çº¢ç»¿ç¯ç­‰è¦ç´ 
   
3. è‡ªåŠ¨æ ‡æ³¨ç”Ÿæˆ
   å¯¹äºæ¯ä¸ªè¡—æ™¯å›¾åƒï¼Œç”Ÿæˆï¼š
   {
     "image": "street_view_001.jpg",
     "gps": [116.4074, 39.9042],
     "road_type": "urban_main_road",
     "spatial_relations": [
       "åŠ æ²¹ç«™ä½äºé“è·¯å³ä¾§50ç±³å¤„",
       "å‰æ–¹200ç±³æœ‰äº¤å‰è·¯å£",
       "ç›®æ ‡å»ºç­‘åœ¨ç¬¬äºŒä¸ªè·¯å£å·¦è½¬åå³ä¾§"
     ],
     "affordances": [
       "å½“å‰è½¦é“å¯ç›´è¡Œæˆ–å³è½¬",
       "å·¦ä¾§è½¦é“ä»…é™å·¦è½¬",
       "å³ä¾§æœ‰åœè½¦ä½å¯ç”¨"
     ],
     "navigation_queries": [
       {
         "query": "å¦‚ä½•åˆ°è¾¾å‰æ–¹çš„æ˜Ÿå·´å…‹ï¼Ÿ",
         "answer": "ç»§ç»­ç›´è¡Œ100ç±³ï¼Œåœ¨ç¬¬ä¸€ä¸ªè·¯å£å³è½¬ï¼Œç›®æ ‡åœ¨å³æ‰‹è¾¹",
         "reasoning": "åŸºäºå½“å‰ä½ç½®å’Œç›®æ ‡POIä½ç½®è®¡ç®—è·¯å¾„"
       }
     ],
     "3d_elements": [
       {
         "type": "traffic_light",
         "bbox_3d": [...],
         "distance": "15ç±³",
         "state": "çº¢ç¯"
       }
     ]
   }
```

**æ•°æ®è§„æ¨¡å»ºè®®**ï¼š
- **æ ¸å¿ƒè®­ç»ƒé›†**ï¼š100ä¸‡+ è¡—æ™¯å›¾åƒ
- **å¤šæ ·æ€§è¦†ç›–**ï¼š
  - 50+ åŸå¸‚
  - 10+ é“è·¯ç±»å‹ï¼ˆåŸå¸‚å¿«é€Ÿè·¯ã€ä¸»å¹²é“ã€æ¬¡å¹²é“ã€æ”¯è·¯ã€é«˜é€Ÿã€ä¹¡æ‘è·¯ç­‰ï¼‰
  - 4å­£èŠ‚ Ã— 3æ—¶æ®µï¼ˆç™½å¤©/é»„æ˜/å¤œæ™šï¼‰Ã— 5å¤©æ°”æ¡ä»¶

**æ ‡æ³¨æ–¹å¼**ï¼š
1. **è‡ªåŠ¨æ ‡æ³¨**ï¼ˆ80%ï¼‰ï¼š
   - åˆ©ç”¨ç°æœ‰åœ°å›¾æ•°æ®è‡ªåŠ¨ç”Ÿæˆç©ºé—´å…³ç³»æè¿°
   - ä½¿ç”¨Qwen3-VLæœ¬èº«ç”Ÿæˆåˆå§‹æ ‡æ³¨
   - ä½¿ç”¨SAM/Grounding-DINOç­‰å·¥å…·è¾…åŠ©å®šä½

2. **äººå·¥æ ¡éªŒ**ï¼ˆ20%æ ¸å¿ƒæ ·æœ¬ï¼‰ï¼š
   - å¤æ‚è·¯å£åœºæ™¯
   - ç‰¹æ®Šå¤©æ°”æ¡ä»¶
   - é•¿è·ç¦»å¯¼èˆªè§„åˆ’

#### **æ–¹æ¡ˆBï¼šBEVï¼ˆé¸Ÿç°å›¾ï¼‰+ å¤šè§†è§’èåˆæ•°æ®**

**æ ¸å¿ƒæ€è·¯**ï¼šç»“åˆBEVåœ°å›¾å’Œè¡—æ™¯å›¾åƒï¼Œæ„å»º**ç©ºé—´æ¨ç†æ•°æ®é›†**

```
æ•°æ®ç»„æˆï¼š
<image_front>  # å‰è§†æ‘„åƒå¤´
<image_left>   # å·¦è§†æ‘„åƒå¤´  
<image_right>  # å³è§†æ‘„åƒå¤´
<image_bev>    # BEVåœ°å›¾ï¼ˆä»åœ°å›¾æ•°æ®æ¸²æŸ“ï¼‰

Query: "å‰æ–¹è·¯å£å¦‚ä½•è§„åˆ’å·¦è½¬è·¯å¾„ï¼Ÿ"
Answer: 
"å½“å‰åœ¨ä¸»å¹²é“æœ€å³ä¾§è½¦é“ã€‚éœ€æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
1. å‘å·¦å˜é“2æ¬¡åˆ°è¾¾æœ€å·¦ä¾§è½¦é“
2. åœ¨è·¯å£å‰50ç±³å‡é€Ÿ
3. ç¡®è®¤å¯¹å‘æ— æ¥è½¦åå·¦è½¬è¿›å…¥ç›®æ ‡é“è·¯
4. ä¿æŒåœ¨ç›®æ ‡é“è·¯å³ä¾§è½¦é“"

ç©ºé—´æ¨ç†é“¾ï¼š
- å½“å‰ä½ç½®è¯†åˆ«ï¼šå³ä¾§è½¦é“
- ç›®æ ‡ä½ç½®è§„åˆ’ï¼šå·¦è½¬éœ€è¦æœ€å·¦è½¦é“
- ä¸­é—´æ­¥éª¤ï¼šå˜é“2æ¬¡
- æ—¶æœºåˆ¤æ–­ï¼šè·¯å£å‰50ç±³
- å®‰å…¨æ£€æŸ¥ï¼šå¯¹å‘è½¦è¾†
```

**æ•°æ®ç”Ÿæˆå·¥å…·é“¾**ï¼š
```python
# BEVæ¸²æŸ“ + è¡—æ™¯å›¾åƒ + è·¯å¾„è§„åˆ’
class RoadSceneDataGenerator:
    def __init__(self, map_db, street_view_db):
        self.map_db = map_db
        self.street_view_db = street_view_db
    
    def generate_sample(self, start_gps, end_gps):
        # 1. è·¯å¾„è§„åˆ’
        path = self.map_db.plan_route(start_gps, end_gps)
        
        # 2. å…³é”®å†³ç­–ç‚¹é‡‡æ ·
        decision_points = self.identify_decision_points(path)
        
        # 3. ä¸ºæ¯ä¸ªå†³ç­–ç‚¹ç”Ÿæˆæ•°æ®
        samples = []
        for point in decision_points:
            # è·å–è¡—æ™¯å›¾åƒ
            images = self.street_view_db.query(point.gps)
            
            # æ¸²æŸ“BEVåœ°å›¾
            bev_map = self.render_bev(point, radius=100m)
            
            # ç”Ÿæˆç©ºé—´æ¨ç†é—®ç­”
            qa = self.generate_spatial_reasoning_qa(
                point, path, images, bev_map
            )
            
            samples.append(qa)
        
        return samples
```

### 3.2 å¯¼èˆªä»»åŠ¡æ•°æ®åˆæˆ

#### **æ ¸å¿ƒæ•°æ®ç±»å‹**

**ç±»å‹1ï¼šå¤æ‚è·¯å£å¯¼èˆªæ¨ç†**
```json
{
  "scenario": "äº”å²”è·¯å£",
  "images": ["front_view.jpg", "bev_map.jpg"],
  "context": "å½“å‰åœ¨ç¯å²›å¤–å›´ï¼Œéœ€è¦å‰å¾€ç¬¬3ä¸ªå‡ºå£",
  "query": "å¦‚ä½•å®‰å…¨é€šè¿‡è¿™ä¸ªç¯å²›ï¼Ÿ",
  "reasoning_chain": [
    "è§‚å¯Ÿï¼šç¯å²›æœ‰5ä¸ªå‡ºå£ï¼Œç›®æ ‡æ˜¯ç¬¬3ä¸ª",
    "è§„åˆ’ï¼šéœ€è¦åœ¨ç¯å²›å†…è¡Œé©¶çº¦180åº¦",
    "æ‰§è¡Œï¼šè¿›å…¥ç¯å²›åä¿æŒå¤–ä¾§è½¦é“",
    "æ—¶æœºï¼šç»è¿‡ç¬¬2ä¸ªå‡ºå£åå‡†å¤‡é©¶å‡º",
    "æ³¨æ„ï¼šç¤¼è®©ç¯å²›å†…è½¦è¾†ï¼Œè§‚å¯Ÿå³ä¾§æ¥è½¦"
  ],
  "answer": "è¿›å…¥ç¯å²›åä¿æŒå¤–ä¾§è½¦é“ï¼Œç»è¿‡2ä¸ªå‡ºå£åä»ç¬¬3ä¸ªå‡ºå£é©¶å‡º"
}
```

**ç±»å‹2ï¼šå¤šç›®æ ‡ç‚¹åºåˆ—å¯¼èˆª**
```json
{
  "task": "ä¾æ¬¡è®¿é—®Aâ†’Bâ†’Cä¸‰ä¸ªåœ°ç‚¹",
  "images": ["èµ·ç‚¹è¡—æ™¯", "åœ°å›¾å…¨å±€è§†å›¾"],
  "query": "è§„åˆ’æœ€ä¼˜è®¿é—®è·¯çº¿å¹¶è¯´æ˜æ¯ä¸ªè½¬å¼¯ç‚¹çš„æ“ä½œ",
  "answer": {
    "route": "èµ·ç‚¹â†’è·¯å£1å³è½¬â†’è·¯å£2å·¦è½¬â†’Aâ†’è·¯å£3ç›´è¡Œâ†’Bâ†’...",
    "instructions": [
      "å‰è¡Œ500ç±³åˆ°è¾¾è·¯å£1ï¼Œå³è½¬è¿›å…¥XXè·¯",
      "æ²¿XXè·¯è¡Œé©¶1.2å…¬é‡Œåˆ°è¾¾è·¯å£2ï¼Œå·¦è½¬è¿›å…¥YYè·¯",
      "ç›®æ ‡Aåœ¨å³æ‰‹è¾¹ï¼Œæ³¨æ„è·¯è¾¹åœè½¦æ ‡å¿—"
    ],
    "reasoning": "é€‰æ‹©æ­¤è·¯çº¿å› ä¸ºé¿å¼€äº†æ‹¥å µè·¯æ®µï¼Œæ€»è·ç¦»5.3å…¬é‡Œ"
  }
}
```

**ç±»å‹3ï¼šåœ°å›¾è¦ç´ ç©ºé—´å…³ç³»æ¨ç†**
```json
{
  "image": "street_view.jpg",
  "elements": {
    "identified": ["é“¶è¡Œ", "ä¾¿åˆ©åº—", "å…¬äº¤ç«™", "äººè¡Œæ¨ªé“"],
    "hidden": ["åœ°é“ç«™å…¥å£"]
  },
  "query": "æœ€è¿‘çš„åœ°é“ç«™å…¥å£åœ¨å“ªé‡Œï¼Ÿ",
  "reasoning": [
    "è§‚å¯Ÿï¼šç”»é¢ä¸­çœ‹åˆ°å…¬äº¤ç«™ç‰Œ",
    "æ¨ç†ï¼šåœ°é“ç«™é€šå¸¸ä¸å…¬äº¤æ¢çº½ç›¸é‚»",
    "éªŒè¯ï¼šå…¬äº¤ç«™å³ä¾§æœ‰åœ°é“æ ‡è¯†",
    "ç»“è®ºï¼šåœ°é“å…¥å£åœ¨å…¬äº¤ç«™å³ä¾§20ç±³å¤„"
  ],
  "answer": "åœ°é“å…¥å£åœ¨å‰æ–¹å³ä¾§ï¼Œå…¬äº¤ç«™æ—è¾¹çº¦20ç±³å¤„"
}
```

**æ•°æ®è§„æ¨¡å»ºè®®**ï¼š
- å¤æ‚è·¯å£åœºæ™¯ï¼š10ä¸‡+ æ ·æœ¬
- å¤šç‚¹å¯¼èˆªä»»åŠ¡ï¼š5ä¸‡+ æ ·æœ¬
- åœ°å›¾è¦ç´ æ¨ç†ï¼š20ä¸‡+ æ ·æœ¬

### 3.3 åœ°å›¾ç‰¹å®šçš„3Dç©ºé—´æ•°æ®

#### **æ–¹æ¡ˆCï¼šé“è·¯3Dè¦ç´ æ ‡æ³¨æ•°æ®**

åˆ©ç”¨åœ°å›¾å…¬å¸çš„**é«˜ç²¾åº¦åœ°å›¾æ•°æ®**ï¼Œæ„å»ºé“è·¯åœºæ™¯çš„3Dç†è§£æ•°æ®ï¼š

```json
{
  "image": "street_view_3d.jpg",
  "3d_annotations": [
    {
      "type": "traffic_sign",
      "category": "é™é€Ÿ60",
      "bbox_3d": {
        "center": [x, y, z],
        "size": [w, h, d],
        "rotation": [roll, pitch, yaw]
      },
      "distance_from_camera": 15.3,
      "position_description": "é“è·¯å³ä¾§ï¼Œé«˜åº¦3ç±³"
    },
    {
      "type": "lane_marking",
      "category": "å®çº¿",
      "points_3d": [[x1,y1,z1], [x2,y2,z2], ...],
      "semantic": "ç¦æ­¢è·¨è¶Š"
    },
    {
      "type": "road_boundary",
      "category": "äººè¡Œé“è¾¹ç¼˜",
      "points_3d": [...],
      "height": 0.15
    }
  ],
  "spatial_queries": [
    {
      "query": "å‰æ–¹çš„é™é€Ÿæ ‡å¿—ç¦»å½“å‰ä½ç½®å¤šè¿œï¼Ÿ",
      "answer": "çº¦15ç±³ï¼Œä½äºé“è·¯å³ä¾§3ç±³é«˜å¤„"
    },
    {
      "query": "å½“å‰å¯ä»¥å˜é“å—ï¼Ÿ",
      "answer": "ä¸å¯ä»¥ï¼Œå› ä¸ºè½¦é“çº¿æ˜¯å®çº¿ï¼Œç¦æ­¢è·¨è¶Š"
    }
  ]
}
```

**æ•°æ®æ¥æº**ï¼š
1. **é«˜ç²¾åœ°å›¾è‡ªåŠ¨è½¬æ¢**ï¼š
   - HD Mapçš„è½¦é“çº¿ã€äº¤é€šæ ‡å¿—ç­‰è¦ç´ 
   - è‡ªåŠ¨ç”Ÿæˆ3Dè¾¹ç•Œæ¡†å’Œç‚¹äº‘
   
2. **æ¿€å…‰é›·è¾¾æ•°æ®èåˆ**ï¼š
   - ç‚¹äº‘ + å›¾åƒé…å‡†
   - ç”Ÿæˆç²¾ç¡®çš„3Dç©ºé—´æ ‡æ³¨

3. **ä¼—åŒ…æ•°æ®æŒ–æ˜**ï¼š
   - ç”¨æˆ·è¡Œè½¦è½¨è¿¹
   - æå–éšå«çš„ç©ºé—´æ¨ç†æ¨¡å¼

#### **æ–¹æ¡ˆDï¼šæ—¶ç©ºåºåˆ—å¯¼èˆªæ•°æ®**

**æ ¸å¿ƒåˆ›æ–°**ï¼šæ„å»º**è¿ç»­æ—¶ç©ºåºåˆ—**æ•°æ®ï¼Œæ¨¡æ‹ŸçœŸå®å¯¼èˆªåœºæ™¯

```python
# æ—¶ç©ºåºåˆ—æ•°æ®ç»“æ„
{
  "trajectory_id": "traj_001",
  "duration": "5åˆ†30ç§’",
  "frames": [
    {
      "timestamp": 0.0,
      "image": "frame_000.jpg",
      "gps": [116.407, 39.904],
      "speed": 30,  # km/h
      "action": "ç›´è¡Œ",
      "observation": "å‰æ–¹500ç±³æœ‰çº¢ç»¿ç¯",
      "planning": "ä¿æŒå½“å‰è½¦é“ï¼Œå‡†å¤‡åœ¨è·¯å£å·¦è½¬"
    },
    {
      "timestamp": 10.0,
      "image": "frame_010.jpg",
      "gps": [116.408, 39.904],
      "speed": 25,
      "action": "å‡é€Ÿ",
      "observation": "çº¢ç»¿ç¯å˜çº¢ï¼Œå‰è½¦å‡é€Ÿ",
      "planning": "åœåœ¨åœæ­¢çº¿å‰"
    },
    // ... æ›´å¤šå¸§
  ],
  "long_term_query": "ä»èµ·ç‚¹åˆ°ç»ˆç‚¹çš„å®Œæ•´å¯¼èˆªè§„åˆ’æ˜¯ä»€ä¹ˆï¼Ÿ",
  "short_term_queries": [
    {"time": 10.0, "query": "ç°åœ¨åº”è¯¥åšä»€ä¹ˆï¼Ÿ", "answer": "å‡é€Ÿå¹¶åœåœ¨çº¢ç»¿ç¯å‰"},
    {"time": 35.0, "query": "ç»¿ç¯äº®èµ·åå¦‚ä½•æ“ä½œï¼Ÿ", "answer": "å·¦è½¬è¿›å…¥ç›®æ ‡é“è·¯"}
  ]
}
```

**è®­ç»ƒä»·å€¼**ï¼š
- æå‡**é•¿æœŸè§„åˆ’**èƒ½åŠ›ï¼ˆç±»ä¼¼äºMMLongBenchï¼‰
- å¢å¼º**åŠ¨æ€å†³ç­–**èƒ½åŠ›ï¼ˆçº¢ç»¿ç¯ã€å…¶ä»–è½¦è¾†ï¼‰
- å­¦ä¹ **æ—¶é—´-ç©ºé—´è”åˆæ¨ç†**

### 3.4 æ•°æ®å¢å¼ºç­–ç•¥

#### **ç­–ç•¥1ï¼šå¤šæ¨¡æ€æ•°æ®åˆæˆ**

```python
# åˆ©ç”¨åœ°å›¾æ•°æ®åˆæˆè®­ç»ƒæ ·æœ¬
class MapDataAugmentation:
    
    def synthesize_sample(self, map_region):
        # 1. ä»åœ°å›¾æå–è·¯ç½‘æ‹“æ‰‘
        road_network = self.extract_topology(map_region)
        
        # 2. éšæœºé‡‡æ ·èµ·ç‚¹å’Œç»ˆç‚¹
        start, end = self.sample_od_pair(road_network)
        
        # 3. ç”Ÿæˆå¤šæ¡å€™é€‰è·¯å¾„
        paths = self.generate_alternative_routes(start, end)
        
        # 4. ä¸ºæ¯æ¡è·¯å¾„ç”Ÿæˆè§£é‡Š
        for path in paths:
            reasoning = self.explain_route_choice(path, {
                "distance": path.length,
                "time": path.estimated_time,
                "traffic": path.traffic_level,
                "complexity": path.num_turns
            })
        
        # 5. æ¸²æŸ“ä¸ºå›¾åƒ + æ–‡æœ¬å¯¹
        return self.render_qa_pair(paths, reasoning)
```

#### **ç­–ç•¥2ï¼šéš¾åº¦è¯¾ç¨‹è®¾è®¡**

```
Level 1ï¼ˆç®€å•ï¼‰ï¼š
â”œâ”€ ç›´è·¯å¯¼èˆª
â”œâ”€ å•ä¸ªPOIå®šä½
â””â”€ ç®€å•è·¯å£ï¼ˆåå­—è·¯å£ï¼‰

Level 2ï¼ˆä¸­ç­‰ï¼‰ï¼š
â”œâ”€ å¤šæ¬¡è½¬å¼¯å¯¼èˆª
â”œâ”€ å¤šä¸ªPOIç›¸å¯¹ä½ç½®
â””â”€ å¤æ‚è·¯å£ï¼ˆç¯å²›ã€äº”å²”è·¯å£ï¼‰

Level 3ï¼ˆå›°éš¾ï¼‰ï¼š
â”œâ”€ å¤šç›®æ ‡åºåˆ—è§„åˆ’
â”œâ”€ åŠ¨æ€éšœç¢ç‰©å¤„ç†
â””â”€ æ—¶ç©ºçº¦æŸä¼˜åŒ–ï¼ˆå¦‚"30åˆ†é’Ÿå†…åˆ°è¾¾"ï¼‰

Level 4ï¼ˆæéš¾ï¼‰ï¼š
â”œâ”€ é•¿è·ç¦»è·¨åŸå¯¼èˆª
â”œâ”€ å®æ—¶è·¯å†µè°ƒæ•´
â””â”€ å¤šæ¨¡æ€ä¿¡æ¯èåˆï¼ˆåœ°å›¾+è¡—æ™¯+å«æ˜Ÿå›¾ï¼‰
```

---

## å››ã€æ¨¡å‹ç»“æ„ç«¯æå‡æ–¹æ¡ˆ

### 4.1 åœ°å›¾ç‰¹åŒ–çš„ä½ç½®ç¼–ç 

#### **æ–¹æ¡ˆEï¼šRoad Network RoPE**

**æ ¸å¿ƒæ€æƒ³**ï¼šå°†é“è·¯ç½‘ç»œçš„**æ‹“æ‰‘ç»“æ„**èå…¥ä½ç½®ç¼–ç 

```python
class RoadNetworkRoPE(nn.Module):
    """
    ä¼ ç»ŸRoPEï¼šåŸºäºæ¬§å¼è·ç¦»
    Road-RoPEï¼šåŸºäºè·¯ç½‘è·ç¦» + æ‹“æ‰‘å…³ç³»
    """
    def __init__(self, d_model):
        super().__init__()
        self.d_model = d_model
        
    def forward(self, x, road_graph):
        # 1. è®¡ç®—è·¯ç½‘è·ç¦»çŸ©é˜µ
        # ä¸æ˜¯ç›´çº¿è·ç¦»ï¼Œè€Œæ˜¯æ²¿é“è·¯çš„å®é™…è·ç¦»
        road_distance = self.compute_road_distance(road_graph)
        
        # 2. ç¼–ç æ‹“æ‰‘å…³ç³»
        # ä¾‹å¦‚ï¼šæ˜¯å¦åœ¨åŒä¸€æ¡è·¯ä¸Šã€æ˜¯å¦å¯ç›´è¾¾
        topology_encoding = self.encode_topology(road_graph)
        
        # 3. èåˆåˆ°RoPE
        pos_encoding = self.rope(road_distance) + \
                       self.topology_encoding(topology_encoding)
        
        return x * pos_encoding
```

**ä¼˜åŠ¿**ï¼š
- æ›´å‡†ç¡®åœ°è¡¨ç¤ºé“è·¯åœºæ™¯ä¸­çš„ç©ºé—´å…³ç³»
- åŒºåˆ†"ç›´çº¿è¿‘ä½†è·¯ç½‘è¿œ"çš„ä½ç½®ï¼ˆå¦‚ç«‹äº¤æ¡¥ä¸¤ä¾§ï¼‰

#### **æ–¹æ¡ˆFï¼šMulti-Scale Spatial Encoding**

**åŠ¨æœº**ï¼šåœ°å›¾æ•°æ®æ¶‰åŠå¤šä¸ªç©ºé—´å°ºåº¦

```
ç©ºé—´å°ºåº¦å±‚çº§ï¼š
â”œâ”€ å…¨å±€å°ºåº¦ï¼ˆåŸå¸‚çº§ï¼‰ï¼š10+ å…¬é‡Œ
â”œâ”€ åŒºåŸŸå°ºåº¦ï¼ˆè¡—åŒºçº§ï¼‰ï¼š1-10 å…¬é‡Œ  
â”œâ”€ å±€éƒ¨å°ºåº¦ï¼ˆè·¯æ®µçº§ï¼‰ï¼š100ç±³-1å…¬é‡Œ
â””â”€ ç²¾ç»†å°ºåº¦ï¼ˆè½¦é“çº§ï¼‰ï¼š<100ç±³
```

**å®ç°**ï¼š
```python
class MultiScaleSpatialEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        # ä¸åŒå°ºåº¦çš„ç¼–ç å™¨
        self.global_encoder = GlobalScaleEncoder()    # åŸå¸‚æ‹“æ‰‘
        self.regional_encoder = RegionalScaleEncoder()  # è¡—åŒºè·¯ç½‘
        self.local_encoder = LocalScaleEncoder()      # é“è·¯ç»†èŠ‚
        self.lane_encoder = LaneScaleEncoder()        # è½¦é“çº§
        
    def forward(self, features, scale_info):
        # æ ¹æ®æŸ¥è¯¢çš„ç©ºé—´å°ºåº¦ï¼ŒåŠ¨æ€é€‰æ‹©ç¼–ç å™¨
        if scale_info['range'] > 10000:  # >10km
            return self.global_encoder(features)
        elif scale_info['range'] > 1000:
            return self.regional_encoder(features)
        elif scale_info['range'] > 100:
            return self.local_encoder(features)
        else:
            return self.lane_encoder(features)
```

### 4.2 æ‹“æ‰‘å…³ç³»å»ºæ¨¡æ¨¡å—

#### **æ–¹æ¡ˆGï¼šGraph Neural Network Integration**

**æ ¸å¿ƒæ€æƒ³**ï¼šåœ¨LLMä¸­é›†æˆ**å›¾ç¥ç»ç½‘ç»œ**æ¨¡å—ï¼Œæ˜¾å¼å»ºæ¨¡é“è·¯æ‹“æ‰‘

```python
class TopologyAwareVLM(nn.Module):
    """
    åœ¨Qwen3-VLåŸºç¡€ä¸Šæ·»åŠ æ‹“æ‰‘å»ºæ¨¡æ¨¡å—
    """
    def __init__(self, base_vlm):
        super().__init__()
        self.base_vlm = base_vlm  # Qwen3-VL
        
        # æ–°å¢ï¼šæ‹“æ‰‘å…³ç³»å»ºæ¨¡
        self.road_graph_encoder = RoadGraphGNN(
            node_dim=256,
            edge_dim=128,
            num_layers=3
        )
        
        # æ–°å¢ï¼šæ‹“æ‰‘-è§†è§‰èåˆ
        self.topo_vision_fusion = CrossAttention(
            dim=base_vlm.hidden_size
        )
        
    def forward(self, images, text, road_graph=None):
        # 1. åŸºç¡€VLMç¼–ç 
        vision_features = self.base_vlm.vision_encoder(images)
        text_features = self.base_vlm.llm.embed(text)
        
        # 2. å¦‚æœæä¾›äº†road_graphï¼Œè¿›è¡Œæ‹“æ‰‘å»ºæ¨¡
        if road_graph is not None:
            # 2.1 ç¼–ç é“è·¯æ‹“æ‰‘
            topo_features = self.road_graph_encoder(road_graph)
            
            # 2.2 èåˆè§†è§‰ç‰¹å¾å’Œæ‹“æ‰‘ç‰¹å¾
            vision_features = self.topo_vision_fusion(
                query=vision_features,
                key=topo_features,
                value=topo_features
            )
        
        # 3. é€å…¥LLM
        output = self.base_vlm.llm(
            vision_features, text_features
        )
        
        return output
```

**Road Graphç»“æ„**ï¼š
```python
# é“è·¯å›¾å®šä¹‰
road_graph = {
    "nodes": [
        {"id": 0, "type": "intersection", "gps": [116.4, 39.9]},
        {"id": 1, "type": "road_segment", "length": 500},
        ...
    ],
    "edges": [
        {"from": 0, "to": 1, "type": "direct", "lanes": 3},
        {"from": 1, "to": 2, "type": "left_turn", "lanes": 1},
        ...
    ]
}
```

#### **æ–¹æ¡ˆHï¼šSpatial Relation Attention**

**åŠ¨æœº**ï¼šæ˜¾å¼å»ºæ¨¡ç©ºé—´å…³ç³»ï¼ˆå‰åã€å·¦å³ã€è¿œè¿‘ç­‰ï¼‰

```python
class SpatialRelationAttention(nn.Module):
    """
    åœ¨æ ‡å‡†attentionåŸºç¡€ä¸Šï¼Œæ³¨å…¥ç©ºé—´å…³ç³»å…ˆéªŒ
    """
    def __init__(self, hidden_size, num_relation_types=8):
        super().__init__()
        self.hidden_size = hidden_size
        
        # ç©ºé—´å…³ç³»åµŒå…¥
        self.relation_embeddings = nn.Embedding(
            num_relation_types, hidden_size
        )
        # å…³ç³»ç±»å‹ï¼šå‰ã€åã€å·¦ã€å³ã€ä¸Šã€ä¸‹ã€è¿œã€è¿‘
        
    def forward(self, query, key, value, spatial_relations):
        """
        spatial_relations: [B, N, N] çŸ©é˜µ
        spatial_relations[i,j] = ç‰©ä½“iå’Œç‰©ä½“jçš„ç©ºé—´å…³ç³»ç±»å‹
        """
        # 1. æ ‡å‡†attention
        attn_scores = torch.matmul(query, key.transpose(-2, -1))
        
        # 2. ç©ºé—´å…³ç³»è°ƒåˆ¶
        relation_bias = self.relation_embeddings(spatial_relations)
        attn_scores = attn_scores + relation_bias
        
        # 3. softmax + output
        attn_weights = F.softmax(attn_scores, dim=-1)
        output = torch.matmul(attn_weights, value)
        
        return output
```

**åº”ç”¨åœºæ™¯**ï¼š
- ç†è§£"å·¦ä¾§ç¬¬äºŒä¸ªè·¯å£"è¿™ç±»ç›¸å¯¹ä½ç½®æè¿°
- å»ºæ¨¡"Aåœ¨Bå’ŒCä¹‹é—´"çš„ä¸‰å…ƒç©ºé—´å…³ç³»

### 4.3 BEVç‰¹å¾æå–å™¨

#### **æ–¹æ¡ˆIï¼šBEV Feature Encoder**

**åŠ¨æœº**ï¼šåœ°å›¾æ•°æ®å¤©ç„¶æ˜¯BEVè§†è§’ï¼Œåº”è¯¥æœ‰ä¸“é—¨çš„ç¼–ç å™¨

```python
class BEVFeatureEncoder(nn.Module):
    """
    ä¸“é—¨å¤„ç†BEVåœ°å›¾å›¾åƒçš„ç¼–ç å™¨
    """
    def __init__(self):
        super().__init__()
        # åŸºäºCNNçš„BEVç¼–ç å™¨
        self.bev_backbone = nn.Sequential(
            # æå–ä¸åŒå°ºåº¦çš„ç‰¹å¾
            ConvBlock(3, 64, stride=2),    # 1/2
            ConvBlock(64, 128, stride=2),   # 1/4
            ConvBlock(128, 256, stride=2),  # 1/8
            ConvBlock(256, 512, stride=2),  # 1/16
        )
        
        # BEVç‰¹å®šçš„ä½ç½®ç¼–ç ï¼ˆx-y gridï¼‰
        self.bev_pos_encoding = BEVPositionalEncoding()
        
    def forward(self, bev_image):
        # 1. ç‰¹å¾æå–
        features = self.bev_backbone(bev_image)
        
        # 2. æ·»åŠ BEVä½ç½®ç¼–ç 
        features = features + self.bev_pos_encoding(features)
        
        return features
```

**æ•´åˆåˆ°Qwen3-VL**ï¼š
```python
class Qwen3VL_WithBEV(nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.vision_encoder = base_model.vision_encoder  # è¡—æ™¯å›¾åƒ
        self.bev_encoder = BEVFeatureEncoder()           # BEVåœ°å›¾
        self.merger = base_model.merger
        self.llm = base_model.llm
        
        # å¤šè§†è§’èåˆ
        self.multiview_fusion = nn.MultiheadAttention(
            embed_dim=base_model.hidden_size,
            num_heads=16
        )
        
    def forward(self, street_images, bev_images, text):
        # 1. åˆ†åˆ«ç¼–ç 
        street_features = self.vision_encoder(street_images)
        bev_features = self.bev_encoder(bev_images)
        
        # 2. å¤šè§†è§’èåˆ
        fused_features = self.multiview_fusion(
            query=street_features,
            key=bev_features,
            value=bev_features
        )
        
        # 3. é€å…¥LLM
        visual_tokens = self.merger(fused_features)
        output = self.llm(visual_tokens, text)
        
        return output
```

### 4.4 åŠ¨æ€è·¯å¾„è§„åˆ’æ¨¡å—

#### **æ–¹æ¡ˆJï¼šDifferentiable Path Planner**

**æ ¸å¿ƒæ€æƒ³**ï¼šåœ¨æ¨¡å‹ä¸­åµŒå…¥**å¯å¾®åˆ†çš„è·¯å¾„è§„åˆ’å™¨**

```python
class DifferentiablePathPlanner(nn.Module):
    """
    å°†A*ç®—æ³•æ”¹é€ ä¸ºå¯å¾®åˆ†ç‰ˆæœ¬ï¼ŒåµŒå…¥ç¥ç»ç½‘ç»œ
    """
    def __init__(self, hidden_size):
        super().__init__()
        # å­¦ä¹ ä»£ä»·å‡½æ•°
        self.cost_network = nn.Sequential(
            nn.Linear(hidden_size, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )
        
    def forward(self, road_graph, start, goal, context):
        """
        context: æ¥è‡ªLLMçš„ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼ˆç†è§£äº†"é¿å¼€æ‹¥å µ"ç­‰çº¦æŸï¼‰
        """
        # 1. ä¸ºæ¯æ¡è¾¹è®¡ç®—ä»£ä»·ï¼ˆåŸºäºå­¦ä¹ çš„ä»£ä»·å‡½æ•°ï¼‰
        edge_costs = []
        for edge in road_graph.edges:
            edge_feature = self.extract_edge_feature(edge, context)
            cost = self.cost_network(edge_feature)
            edge_costs.append(cost)
        
        # 2. å¯å¾®åˆ†çš„æœ€çŸ­è·¯å¾„ç®—æ³•ï¼ˆå¦‚Soft Dijkstraï¼‰
        path, path_cost = self.soft_shortest_path(
            road_graph, start, goal, edge_costs
        )
        
        return path, path_cost
    
    def soft_shortest_path(self, graph, start, goal, costs):
        """
        ä½¿ç”¨Gumbel-Softmaxç­‰æŠ€æœ¯å®ç°å¯å¾®åˆ†çš„è·¯å¾„æœç´¢
        """
        # å®ç°ç»†èŠ‚çœç•¥
        pass
```

**è®­ç»ƒä¼˜åŠ¿**ï¼š
- ç«¯åˆ°ç«¯å­¦ä¹ è·¯å¾„è§„åˆ’
- å¯ä»¥å­¦ä¹ éšå¼çš„ç”¨æˆ·åå¥½ï¼ˆå¦‚"å–œæ¬¢å¤§è·¯"ï¼‰
- æ”¯æŒå¤æ‚çº¦æŸï¼ˆå¦‚"30åˆ†é’Ÿå†…åˆ°è¾¾"ï¼‰

---

## äº”ã€è®­ç»ƒç­–ç•¥ç«¯æå‡æ–¹æ¡ˆ

### 5.1 è¯¾ç¨‹å­¦ä¹ ç­–ç•¥

#### **ç­–ç•¥1ï¼šç©ºé—´æ¨ç†éš¾åº¦é€’è¿›**

```python
class SpatialReasoningCurriculum:
    """
    ä»ç®€å•åˆ°å¤æ‚çš„è¯¾ç¨‹å­¦ä¹ 
    """
    def __init__(self):
        self.stages = [
            # Stage 1: å•ç‰©ä½“å®šä½ï¼ˆ5k stepsï¼‰
            {
                "task": "single_object_localization",
                "difficulty": "easy",
                "data_ratio": {"simple_poi": 1.0},
                "steps": 5000
            },
            
            # Stage 2: åŒç‰©ä½“å…³ç³»ï¼ˆ10k stepsï¼‰
            {
                "task": "pairwise_relation",
                "difficulty": "medium",
                "data_ratio": {
                    "simple_poi": 0.3,
                    "pairwise": 0.7
                },
                "steps": 10000
            },
            
            # Stage 3: å¤šç‰©ä½“æ¨ç†ï¼ˆ15k stepsï¼‰
            {
                "task": "multi_object_reasoning",
                "difficulty": "medium-hard",
                "data_ratio": {
                    "pairwise": 0.2,
                    "multi_object": 0.6,
                    "simple_navigation": 0.2
                },
                "steps": 15000
            },
            
            # Stage 4: å¤æ‚å¯¼èˆªï¼ˆ20k stepsï¼‰
            {
                "task": "complex_navigation",
                "difficulty": "hard",
                "data_ratio": {
                    "multi_object": 0.2,
                    "simple_navigation": 0.3,
                    "complex_navigation": 0.5
                },
                "steps": 20000
            },
            
            # Stage 5: æ··åˆæ‰€æœ‰ä»»åŠ¡
            {
                "task": "mixed",
                "difficulty": "all",
                "data_ratio": {
                    "simple_poi": 0.1,
                    "pairwise": 0.1,
                    "multi_object": 0.2,
                    "simple_navigation": 0.3,
                    "complex_navigation": 0.3
                },
                "steps": 30000
            }
        ]
    
    def get_current_stage(self, step):
        cumulative_steps = 0
        for stage in self.stages:
            cumulative_steps += stage['steps']
            if step < cumulative_steps:
                return stage
        return self.stages[-1]  # æœ€åé˜¶æ®µ
```

### 5.2 å¯¹æ¯”å­¦ä¹ å¢å¼º

#### **ç­–ç•¥2ï¼šç©ºé—´å…³ç³»å¯¹æ¯”å­¦ä¹ **

```python
class SpatialContrastiveLearning:
    """
    é€šè¿‡å¯¹æ¯”å­¦ä¹ å¢å¼ºç©ºé—´å…³ç³»ç†è§£
    """
    def __init__(self, model, temperature=0.07):
        self.model = model
        self.temperature = temperature
        
    def compute_loss(self, batch):
        """
        batchåŒ…å«ï¼š
        - anchor: å‚è€ƒåœºæ™¯
        - positive: ç©ºé—´å…³ç³»ç›¸ä¼¼çš„åœºæ™¯
        - negative: ç©ºé—´å…³ç³»ä¸åŒçš„åœºæ™¯
        """
        # 1. ç¼–ç 
        anchor_emb = self.model.encode(batch['anchor'])
        pos_emb = self.model.encode(batch['positive'])
        neg_emb = self.model.encode(batch['negative'])
        
        # 2. å¯¹æ¯”æŸå¤±
        pos_sim = F.cosine_similarity(anchor_emb, pos_emb)
        neg_sim = F.cosine_similarity(anchor_emb, neg_emb)
        
        loss = -torch.log(
            torch.exp(pos_sim / self.temperature) /
            (torch.exp(pos_sim / self.temperature) + 
             torch.exp(neg_sim / self.temperature))
        )
        
        return loss.mean()
```

**å¯¹æ¯”æ ·æœ¬æ„å»º**ï¼š
```python
# Positiveæ ·æœ¬ï¼šç©ºé—´å…³ç³»ç›¸ä¼¼
anchor = "Aåœ¨Bçš„åŒ—ä¾§50ç±³"
positive = "Aåœ¨Bçš„åŒ—è¾¹ä¸è¿œå¤„"  # ç›¸ä¼¼æè¿°

# Negativeæ ·æœ¬ï¼šç©ºé—´å…³ç³»ä¸åŒ
negative = "Aåœ¨Bçš„å—ä¾§å¾ˆè¿œçš„åœ°æ–¹"  # ç›¸åæ–¹å‘
```

### 5.3 å¤šä»»åŠ¡è”åˆè®­ç»ƒ

#### **ç­–ç•¥3ï¼šJoint Training Framework**

```python
class MultiTaskJointTraining:
    """
    åŒæ—¶è®­ç»ƒå¤šä¸ªç©ºé—´ç†è§£ç›¸å…³ä»»åŠ¡
    """
    def __init__(self, model):
        self.model = model
        
        # å¤šä¸ªä»»åŠ¡å¤´
        self.task_heads = {
            "poi_localization": POILocalizationHead(),
            "path_planning": PathPlanningHead(),
            "spatial_relation": SpatialRelationHead(),
            "3d_grounding": Grounding3DHead(),
            "navigation_qa": NavigationQAHead()
        }
        
        # ä»»åŠ¡æƒé‡ï¼ˆå¯å­¦ä¹ ï¼‰
        self.task_weights = nn.Parameter(
            torch.ones(len(self.task_heads))
        )
        
    def forward(self, batch):
        # 1. å…±äº«çš„ç‰¹å¾æå–
        shared_features = self.model.encode(batch)
        
        # 2. å„ä»»åŠ¡è®¡ç®—æŸå¤±
        losses = {}
        for task_name, task_head in self.task_heads.items():
            if task_name in batch['tasks']:
                task_loss = task_head(
                    shared_features, 
                    batch[task_name]
                )
                losses[task_name] = task_loss
        
        # 3. åŠ æƒæ±‚å’Œï¼ˆåŠ¨æ€ä»»åŠ¡å¹³è¡¡ï¼‰
        total_loss = sum(
            self.task_weights[i] * loss 
            for i, loss in enumerate(losses.values())
        )
        
        return total_loss, losses
```

**ä»»åŠ¡ååŒæ•ˆåº”**ï¼š
- POIå®šä½ â†” å¯¼èˆªQAï¼šäº’ç›¸æä¾›ç›‘ç£ä¿¡å·
- 3Då®šä½ â†” ç©ºé—´å…³ç³»ï¼š3Dä¿¡æ¯å¸®åŠ©ç†è§£ç›¸å¯¹ä½ç½®
- è·¯å¾„è§„åˆ’ â†” æ‰€æœ‰ä»»åŠ¡ï¼šä½œä¸ºæœ€ç»ˆçš„æ•´åˆä»»åŠ¡

### 5.4 å¼ºåŒ–å­¦ä¹ å¾®è°ƒ

#### **ç­–ç•¥4ï¼šNavigation RL**

**åŠ¨æœº**ï¼šå¯¼èˆªä»»åŠ¡å¤©ç„¶é€‚åˆRLï¼ˆæœ‰æ˜ç¡®çš„å¥–åŠ±ä¿¡å·ï¼‰

```python
class NavigationRLTrainer:
    """
    ä½¿ç”¨RLå¾®è°ƒå¯¼èˆªè§„åˆ’èƒ½åŠ›
    """
    def __init__(self, model, env):
        self.model = model
        self.env = env  # å¯¼èˆªç¯å¢ƒï¼ˆåŸºäºåœ°å›¾æ•°æ®ï¼‰
        
    def train_episode(self):
        state = self.env.reset()  # åˆå§‹ä½ç½® + ç›®æ ‡
        trajectory = []
        
        done = False
        while not done:
            # 1. æ¨¡å‹ç”ŸæˆåŠ¨ä½œï¼ˆä¸‹ä¸€æ­¥å¯¼èˆªæŒ‡ä»¤ï¼‰
            action = self.model.generate_action(state)
            
            # 2. ç¯å¢ƒåé¦ˆ
            next_state, reward, done, info = self.env.step(action)
            
            # 3. è®°å½•è½¨è¿¹
            trajectory.append({
                "state": state,
                "action": action,
                "reward": reward
            })
            
            state = next_state
        
        # 4. è®¡ç®—å›æŠ¥å¹¶æ›´æ–°æ¨¡å‹
        returns = self.compute_returns(trajectory)
        loss = self.compute_policy_loss(trajectory, returns)
        
        return loss
    
    def compute_reward(self, action, state, next_state):
        """
        å¥–åŠ±è®¾è®¡
        """
        reward = 0
        
        # R1: æ˜¯å¦æœç›®æ ‡å‰è¿›
        if self.distance_to_goal(next_state) < self.distance_to_goal(state):
            reward += 1.0
        
        # R2: è·¯å¾„æ•ˆç‡
        if self.is_optimal_action(action, state):
            reward += 0.5
        
        # R3: å®‰å…¨æ€§ï¼ˆé¿å¼€å¤æ‚è·¯å£ï¼Ÿï¼‰
        if self.is_safe_route(action):
            reward += 0.3
        
        # R4: åˆ°è¾¾ç›®æ ‡
        if self.reached_goal(next_state):
            reward += 10.0
        
        return reward
```

### 5.5 æ•°æ®æ··åˆç­–ç•¥

#### **ç­–ç•¥5ï¼šOptimal Data Mixing**

```python
class DataMixingStrategy:
    """
    åŠ¨æ€è°ƒæ•´ä¸åŒæ•°æ®æºçš„æ··åˆæ¯”ä¾‹
    """
    def __init__(self):
        self.data_sources = {
            # åŸå§‹Qwen3-VLæ•°æ®
            "original_spatial": 0.3,  # 30%
            
            # æ–°å¢çš„åœ°å›¾æ•°æ®
            "road_scene": 0.25,       # 25% - é“è·¯åœºæ™¯
            "navigation": 0.20,       # 20% - å¯¼èˆªä»»åŠ¡
            "bev_multiview": 0.15,    # 15% - BEVå¤šè§†è§’
            "3d_road_elements": 0.10  # 10% - 3Dé“è·¯è¦ç´ 
        }
        
    def sample_batch(self, batch_size):
        """
        æ ¹æ®æ··åˆæ¯”ä¾‹é‡‡æ ·batch
        """
        batch = []
        for source, ratio in self.data_sources.items():
            n_samples = int(batch_size * ratio)
            samples = self.load_from_source(source, n_samples)
            batch.extend(samples)
        
        random.shuffle(batch)
        return batch
    
    def update_mixing_ratio(self, validation_performance):
        """
        æ ¹æ®éªŒè¯é›†æ€§èƒ½åŠ¨æ€è°ƒæ•´æ··åˆæ¯”ä¾‹
        """
        # å¦‚æœæŸç±»æ•°æ®æå‡æ˜æ˜¾ï¼Œå¢åŠ å…¶æ¯”ä¾‹
        for source in self.data_sources:
            if validation_performance[source] > threshold:
                self.data_sources[source] *= 1.1  # å¢åŠ 10%
        
        # å½’ä¸€åŒ–
        total = sum(self.data_sources.values())
        for source in self.data_sources:
            self.data_sources[source] /= total
```

---

## å…­ã€å®Œæ•´è®­ç»ƒPipeline

### 6.1 æ•´ä½“æµç¨‹

```
é˜¶æ®µåˆ’åˆ†ï¼š
â”œâ”€ Phase 1: ç©ºé—´åŸºç¡€èƒ½åŠ›é¢„è®­ç»ƒï¼ˆåŸºäºåŸå§‹Qwen3-VLï¼‰
â”‚  â”œâ”€ æ•°æ®ï¼šåŸå§‹spatialæ•°æ® + ç®€å•åœ°å›¾æ•°æ®
â”‚  â”œâ”€ æ—¶é•¿ï¼š20k steps
â”‚  â””â”€ ç›®æ ‡ï¼šå»ºç«‹åŸºç¡€ç©ºé—´ç†è§£
â”‚
â”œâ”€ Phase 2: åœ°å›¾ç‰¹åŒ–èƒ½åŠ›æ³¨å…¥
â”‚  â”œâ”€ æ•°æ®ï¼šé“è·¯åœºæ™¯ + å¯¼èˆªä»»åŠ¡ + BEVæ•°æ®
â”‚  â”œâ”€ æ—¶é•¿ï¼š50k steps
â”‚  â”œâ”€ ç­–ç•¥ï¼šè¯¾ç¨‹å­¦ä¹  + å¤šä»»åŠ¡è®­ç»ƒ
â”‚  â””â”€ ç›®æ ‡ï¼šè·å¾—åœ°å›¾ç‰¹å®šçš„ç©ºé—´æ¨ç†èƒ½åŠ›
â”‚
â”œâ”€ Phase 3: å¼ºåŒ–å­¦ä¹ å¾®è°ƒ
â”‚  â”œâ”€ ç¯å¢ƒï¼šåŸºäºçœŸå®åœ°å›¾çš„å¯¼èˆªæ¨¡æ‹Ÿå™¨
â”‚  â”œâ”€ æ—¶é•¿ï¼š10k episodes
â”‚  â””â”€ ç›®æ ‡ï¼šä¼˜åŒ–å¯¼èˆªå†³ç­–èƒ½åŠ›
â”‚
â””â”€ Phase 4: é¢†åŸŸé€‚é…
   â”œâ”€ æ•°æ®ï¼šå…¬å¸å†…éƒ¨å®é™…ä¸šåŠ¡æ•°æ®
   â”œâ”€ æ—¶é•¿ï¼š5k steps
   â””â”€ ç›®æ ‡ï¼šé€‚é…å…·ä½“åº”ç”¨åœºæ™¯
```

### 6.2 è®­ç»ƒé…ç½®å»ºè®®

```yaml
# è®­ç»ƒè¶…å‚æ•°
training_config:
  # åŸºç¡€è®¾ç½®
  base_model: "Qwen3-VL-8B"  # èµ·ç‚¹æ¨¡å‹
  learning_rate: 1e-5
  batch_size: 32
  gradient_accumulation: 4
  
  # Phase 1: ç©ºé—´åŸºç¡€ï¼ˆ20k stepsï¼‰
  phase1:
    steps: 20000
    data_mix:
      original_spatial: 0.6
      simple_map: 0.4
    curriculum: false
    
  # Phase 2: åœ°å›¾ç‰¹åŒ–ï¼ˆ50k stepsï¼‰  
  phase2:
    steps: 50000
    data_mix:
      original_spatial: 0.2
      road_scene: 0.3
      navigation: 0.25
      bev_multiview: 0.15
      3d_elements: 0.1
    curriculum: true
    curriculum_stages: [5000, 10000, 15000, 20000]
    multi_task: true
    task_weights: [1.0, 1.0, 1.0, 1.0, 1.0]
    
  # Phase 3: RLå¾®è°ƒï¼ˆ10k episodesï¼‰
  phase3:
    episodes: 10000
    rl_algorithm: "PPO"
    discount_factor: 0.99
    entropy_coef: 0.01
    
  # Phase 4: é¢†åŸŸé€‚é…ï¼ˆ5k stepsï¼‰
  phase4:
    steps: 5000
    data: "company_internal"
    learning_rate: 5e-6  # æ›´å°çš„lr
```

### 6.3 è¯„ä¼°æŒ‡æ ‡ä½“ç³»

```python
evaluation_metrics = {
    # ç©ºé—´ç†è§£åŸºç¡€èƒ½åŠ›
    "spatial_basics": {
        "EmbSpatialBench": "åˆ†æ•°",
        "RefSpatialBench": "åˆ†æ•°",
        "RoboSpatialHome": "åˆ†æ•°"
    },
    
    # åœ°å›¾ç‰¹å®šèƒ½åŠ›
    "map_specific": {
        "POI_Localization": "å‡†ç¡®ç‡",
        "Route_Planning": "æœ€ä¼˜ç‡",
        "Road_Element_Grounding": "mAP",
        "Intersection_Understanding": "å‡†ç¡®ç‡"
    },
    
    # å¯¼èˆªä»»åŠ¡
    "navigation": {
        "Simple_Navigation": "æˆåŠŸç‡",
        "Complex_Navigation": "æˆåŠŸç‡",
        "Multi_Destination": "æ•ˆç‡åˆ†æ•°",
        "Real_Time_Rerouting": "å“åº”é€Ÿåº¦"
    },
    
    # ä¸šåŠ¡æŒ‡æ ‡ï¼ˆé‡è¦ï¼ï¼‰
    "business": {
        "User_Satisfaction": "è¯„åˆ†",
        "Task_Completion_Rate": "ç™¾åˆ†æ¯”",
        "Error_Rate": "ç™¾åˆ†æ¯”",
        "Response_Time": "æ¯«ç§’"
    }
}
```

---

## ä¸ƒã€å®æ–½å»ºè®®ä¸é¢„æœŸæ•ˆæœ

### 7.1 å®æ–½è·¯çº¿å›¾

```
æ—¶é—´çº¿ï¼ˆå»ºè®®ï¼‰ï¼š
â”œâ”€ Month 1-2: æ•°æ®æ„å»º
â”‚  â”œâ”€ Week 1-2: Pipelineå¼€å‘
â”‚  â”œâ”€ Week 3-6: æ•°æ®é‡‡é›†ä¸æ ‡æ³¨
â”‚  â””â”€ Week 7-8: æ•°æ®è´¨é‡éªŒè¯
â”‚
â”œâ”€ Month 3-4: æ¨¡å‹å¼€å‘
â”‚  â”œâ”€ Week 1-2: æ¶æ„æ”¹è¿›å®ç°
â”‚  â”œâ”€ Week 3-4: è®­ç»ƒç¯å¢ƒæ­å»º
â”‚  â””â”€ Week 5-8: Phase 1-2è®­ç»ƒ
â”‚
â”œâ”€ Month 5: RLå¾®è°ƒ
â”‚  â”œâ”€ Week 1-2: æ¨¡æ‹Ÿå™¨å¼€å‘
â”‚  â””â”€ Week 3-4: Phase 3è®­ç»ƒ
â”‚
â”œâ”€ Month 6: é¢†åŸŸé€‚é…ä¸æµ‹è¯•
â”‚  â”œâ”€ Week 1-2: Phase 4è®­ç»ƒ
â”‚  â”œâ”€ Week 3: å†…éƒ¨æµ‹è¯•
â”‚  â””â”€ Week 4: ç°åº¦å‘å¸ƒ
â”‚
â””â”€ Month 7+: è¿­ä»£ä¼˜åŒ–
   â””â”€ æŒç»­æ•°æ®æ”¶é›†ä¸æ¨¡å‹æ›´æ–°
```

### 7.2 èµ„æºéœ€æ±‚ä¼°ç®—

```
è®¡ç®—èµ„æºï¼š
â”œâ”€ è®­ç»ƒï¼š8Ã—A100 (80GB) Ã— 2-3ä¸ªæœˆ
â”œâ”€ æ•°æ®å¤„ç†ï¼š16Ã—CPUèŠ‚ç‚¹
â””â”€ æ¨ç†æµ‹è¯•ï¼š4Ã—A100

äººåŠ›èµ„æºï¼š
â”œâ”€ ç®—æ³•å·¥ç¨‹å¸ˆï¼š3-4äºº
â”œâ”€ æ•°æ®å·¥ç¨‹å¸ˆï¼š2-3äºº
â”œâ”€ æ ‡æ³¨äººå‘˜ï¼š5-10äººï¼ˆå¤–åŒ…ï¼‰
â””â”€ æµ‹è¯•å·¥ç¨‹å¸ˆï¼š1-2äºº

æ•°æ®èµ„æºï¼š
â”œâ”€ è¡—æ™¯å›¾åƒï¼š100ä¸‡+ å¼ 
â”œâ”€ BEVåœ°å›¾ï¼šå¯¹åº”åŒºåŸŸå…¨è¦†ç›–
â”œâ”€ å¯¼èˆªè½¨è¿¹ï¼š10ä¸‡+ æ¡
â””â”€ å­˜å‚¨ï¼š50TB+
```

### 7.3 é¢„æœŸæ•ˆæœ

**å®šé‡æŒ‡æ ‡**ï¼š
```
Spatial Understanding Benchmarks:
â”œâ”€ EmbSpatialBench: 83.1 â†’ 87+ (ç›®æ ‡æå‡5%)
â”œâ”€ RefSpatialBench: 65.5 â†’ 75+ (ç›®æ ‡æå‡15%)
â””â”€ RoboSpatialHome: 69.4 â†’ 80+ (ç›®æ ‡æå‡15%)

Map-Specific Tasks (æ–°å¢):
â”œâ”€ POI Localization: 85%+ å‡†ç¡®ç‡
â”œâ”€ Route Planning Optimality: 90%+
â””â”€ Complex Intersection: 80%+ å‡†ç¡®ç‡
```

**å®šæ€§æå‡**ï¼š
1. **ç†è§£èƒ½åŠ›**ï¼š
   - å‡†ç¡®ç†è§£"ç¬¬äºŒä¸ªè·¯å£å·¦è½¬"ç­‰å¤æ‚æŒ‡ä»¤
   - è¯†åˆ«é“è·¯æ‹“æ‰‘ï¼ˆç¯å²›ã€ç«‹äº¤æ¡¥ï¼‰
   - æ¨ç†éšè—çš„ç©ºé—´å…³ç³»

2. **è§„åˆ’èƒ½åŠ›**ï¼š
   - å¤šçº¦æŸæ¡ä»¶ä¸‹çš„æœ€ä¼˜è·¯å¾„è§„åˆ’
   - åŠ¨æ€è·¯å†µä¸‹çš„å®æ—¶é‡æ–°è§„åˆ’
   - å¤šç›®æ ‡åºåˆ—ä¼˜åŒ–

3. **æ³›åŒ–èƒ½åŠ›**ï¼š
   - è·¨åŸå¸‚æ³›åŒ–
   - ä¸åŒå¤©æ°”/æ—¶æ®µçš„é²æ£’æ€§
   - å¤„ç†ç½•è§è·¯å£ç±»å‹

### 7.4 é£é™©ä¸åº”å¯¹

| é£é™© | å½±å“ | åº”å¯¹æªæ–½ |
|------|------|---------|
| æ•°æ®æ ‡æ³¨è´¨é‡ä¸è¶³ | é«˜ | å¤šè½®è´¨æ£€ + ä¸“å®¶å®¡æ ¸ |
| æ¨¡å‹è¿‡æ‹Ÿåˆåœ°å›¾æ•°æ® | ä¸­ | ä¿æŒåŸå§‹æ•°æ®æ··åˆ + æ­£åˆ™åŒ– |
| è®¡ç®—èµ„æºè¶…é¢„ç®— | ä¸­ | ä¼˜å…ˆè®­ç»ƒå°æ¨¡å‹(8B) + çŸ¥è¯†è’¸é¦ |
| ä¸šåŠ¡åœºæ™¯é€‚é…å›°éš¾ | é«˜ | Phase 4ç•™è¶³é€‚é…æ—¶é—´ |
| æ¨ç†é€Ÿåº¦ä¸è¾¾æ ‡ | ä¸­ | æ¨¡å‹é‡åŒ– + éƒ¨ç½²ä¼˜åŒ– |

---

## å…«ã€æ€»ç»“

é’ˆå¯¹è´µå…¬å¸ï¼ˆåœ°å›¾å…¬å¸ï¼‰æå‡Qwen3-VLçš„Embodied/Spatial Understandingèƒ½åŠ›ï¼Œæ ¸å¿ƒå»ºè®®æ˜¯ï¼š

### 8.1 å……åˆ†åˆ©ç”¨åœ°å›¾æ•°æ®ä¼˜åŠ¿

1. **é“è·¯åœºæ™¯æ•°æ®**ï¼šè¡—æ™¯ + BEV + è½¨è¿¹çš„å¤šæ¨¡æ€èåˆ
2. **æ‹“æ‰‘ç»“æ„æ•°æ®**ï¼šæ˜¾å¼å»ºæ¨¡è·¯ç½‘æ‹“æ‰‘å…³ç³»
3. **å¯¼èˆªä»»åŠ¡æ•°æ®**ï¼šçœŸå®çš„ç«¯åˆ°ç«¯å¯¼èˆªåœºæ™¯

### 8.2 ä¸‰ç«¯ååŒæå‡

- **æ•°æ®ç«¯**ï¼šæ„å»º100ä¸‡+è§„æ¨¡çš„åœ°å›¾ç‰¹å®šç©ºé—´ç†è§£æ•°æ®
- **æ¨¡å‹ç«¯**ï¼šé›†æˆæ‹“æ‰‘å»ºæ¨¡ã€BEVç¼–ç ã€è·¯å¾„è§„åˆ’ç­‰æ¨¡å—
- **è®­ç»ƒç«¯**ï¼šè¯¾ç¨‹å­¦ä¹  + å¤šä»»åŠ¡è®­ç»ƒ + RLå¾®è°ƒ

### 8.3 å…³é”®æˆåŠŸå› ç´ 

1. **æ•°æ®è´¨é‡**ï¼šé«˜è´¨é‡æ ‡æ³¨ > å¤§è§„æ¨¡ä½è´¨é‡æ•°æ®
2. **ä¸šåŠ¡å¯¹é½**ï¼šè®­ç»ƒä»»åŠ¡ä¸å®é™…ä¸šåŠ¡åœºæ™¯æ·±åº¦ç»“åˆ
3. **è¿­ä»£ä¼˜åŒ–**ï¼šæŒç»­æ”¶é›†ç”¨æˆ·åé¦ˆï¼Œä¸æ–­ä¼˜åŒ–

### 8.4 å·®å¼‚åŒ–ç«äº‰åŠ›

ç›¸æ¯”é€šç”¨VLMï¼Œè´µå…¬å¸è®­ç»ƒçš„æ¨¡å‹å°†åœ¨ä»¥ä¸‹æ–¹é¢å…·æœ‰**æ˜¾è‘—ä¼˜åŠ¿**ï¼š

- âœ… é“è·¯åœºæ™¯ç†è§£ï¼ˆè·¯å£ç±»å‹ã€è½¦é“é…ç½®ç­‰ï¼‰
- âœ… å¯¼èˆªè§„åˆ’èƒ½åŠ›ï¼ˆå¤æ‚çº¦æŸä¸‹çš„æœ€ä¼˜è·¯å¾„ï¼‰
- âœ… åœ°å›¾è¦ç´ è¯†åˆ«ï¼ˆPOIã€äº¤é€šæ ‡å¿—ç­‰ï¼‰
- âœ… æ—¶ç©ºåºåˆ—æ¨ç†ï¼ˆé•¿è·ç¦»å¯¼èˆªçš„é•¿æœŸè§„åˆ’ï¼‰

è¿™å°†æ˜¯ä¸€ä¸ª**é«˜åº¦å®šåˆ¶åŒ–**ä¸”**éš¾ä»¥å¤åˆ¶**çš„æ ¸å¿ƒç«äº‰åŠ›ï¼

---

## ğŸ‘¤ User

è¯·é˜…è¯»VSI_bench.pdfè¿™ç¯‡æ–‡ç« ,ç»™å‡ºæŠ€æœ¯æŠ¥å‘Š,å¹¶ä¸”é’ˆå¯¹è¿™ä¸ªå·¥ä½œä¸­çš„å‡ ä¸ªç©ºé—´æ™ºèƒ½çš„ä»»åŠ¡,é‡æ–°è®¾è®¡ä¸€ä¸‹é—®é¢˜

---

## ğŸ‘¤ User

This session is being continued from a previous conversation that ran out of context. The summary below covers the earlier portion of the conversation.

Analysis:
Let me analyze this conversation chronologically:

1. **Initial Request**: User asked to read Qwen3-VL.pdf and provide the network structure breakdown
   - I attempted to read Qwen3.pdf but file didn't exist
   - Found Qwen3-VL.pdf and read it successfully
   - Provided initial summary of 3-module architecture

2. **Second Request**: User interrupted and asked for comprehensive technical report of Qwen3-VL
   - I provided extensive technical report covering all aspects of the paper
   - Included architecture, training methods, data construction, evaluation results
   - Report was approximately 10,000+ words covering 9 major sections

3. **Third Request**: User asked about Embodied/Spatial Understanding capabilities
   - Specifically asked: which pretraining data contributes to this capability
   - How to improve this capability with multiple approaches (data, architecture, training)
   - **Key context**: User mentioned their company is a map company with rich road data
   - I provided detailed analysis with specific solutions leveraging map data:
     * Data construction methods (street view + map annotation, BEV fusion, navigation tasks)
     * Architecture improvements (Road Network RoPE, Graph Neural Networks, BEV encoders)
     * Training strategies (curriculum learning, contrastive learning, RL fine-tuning)
     * Complete implementation pipeline with code examples
     * Resource requirements and expected outcomes

4. **Current Request**: User now wants to:
   - Read VSI_bench.pdf 
   - Provide technical report of this paper
   - **Redesign** spatial intelligence tasks based on this work

The user has NOT yet provided the VSI_bench.pdf file or indicated where it is located. I should attempt to read it from the same directory as previous PDFs.

Key technical concepts covered:
- Qwen3-VL architecture (Vision Encoder, Merger, LLM)
- Interleaved MRoPE, DeepStack, Text-based timestamps
- Training stages (4-stage pretraining, 3-stage post-training)
- Embodied/Spatial understanding data sources
- Map-specific enhancements for spatial reasoning

Important files:
- Qwen3-VL.pdf (successfully read, 42 pages)
- VSI_bench.pdf (need to read)

No errors encountered so far in the conversation.

Summary:
1. **Primary Request and Intent**:
   - **Request 1**: Read Qwen3-VL.pdf and explain the network structure breakdown (how many modules)
   - **Request 2**: Provide comprehensive technical report of Qwen3-VL paper covering all aspects
   - **Request 3**: Analyze Qwen3-VL's Embodied/Spatial Understanding capabilities:
     * Identify which pretraining data/processes contribute to these capabilities
     * Propose improvement methods from multiple angles (data, architecture, training)
     * **Critical context**: User's company is a map company with rich road data resources
     * Design specific solutions leveraging this map data advantage
   - **Request 4** (Current): Read VSI_bench.pdf, provide technical report, and redesign spatial intelligence tasks based on this work

2. **Key Technical Concepts**:
   - **Qwen3-VL Architecture**: 3-module design (Vision Encoder + Merger + LLM)
   - **Architectural Innovations**: Interleaved MRoPE, DeepStack, Text-based timestamps
   - **Training Pipeline**: 4-stage pretraining (8Kâ†’32Kâ†’256K contexts), 3-stage post-training (SFTâ†’Distillationâ†’RL)
   - **Spatial Understanding Data**: Relational annotations, affordance labels, action-conditioned queries, 3D grounding
   - **Map-Specific Enhancements**: Road Network RoPE, BEV encoders, Graph Neural Networks for topology
   - **Training Strategies**: Curriculum learning, contrastive learning, multi-task training, reinforcement learning
   - **Data Synthesis**: Street view + map annotation fusion, BEV multi-view data, navigation task sequences
   - **Evaluation Metrics**: EmbSpatialBench, RefSpatialBench, RoboSpatialHome, plus custom map-specific metrics

3. **Files and Code Sections**:
   
   - **Qwen3-VL.pdf** (42 pages)
     - **Why important**: Core technical paper describing state-of-the-art vision-language model
     - **Key sections analyzed**:
       * Section 2 (Model Architecture): 3-module structure documented
       * Section 3.2.5 (Spatial Understanding & 3D Recognition): Source of spatial capabilities
       * Training stages and data construction methods
     - **Key findings**: Spatial understanding comes from Stage 1-2 pretraining with specific spatial relation and 3D grounding data
   
   - **Code snippets provided** (extensive pseudo-code examples):
     
     ```python
     # Road scene data construction pipeline
     class RoadSceneDataGenerator:
         def generate_sample(self, start_gps, end_gps):
             path = self.map_db.plan_route(start_gps, end_gps)
             decision_points = self.identify_decision_points(path)
             # Generate spatial reasoning QA for each decision point
     ```
     
     ```python
     # Road Network RoPE - topology-aware positional encoding
     class RoadNetworkRoPE(nn.Module):
         def forward(self, x, road_graph):
             road_distance = self.compute_road_distance(road_graph)
             topology_encoding = self.encode_topology(road_graph)
             pos_encoding = self.rope(road_distance) + self.topology_encoding(topology_encoding)
     ```
     
     ```python
     # Graph Neural Network for topology modeling
     class TopologyAwareVLM(nn.Module):
         def __init__(self, base_vlm):
             self.road_graph_encoder = RoadGraphGNN(node_dim=256, edge_dim=128, num_layers=3)
             self.topo_vision_fusion = CrossAttention(dim=base_vlm.hidden_size)
     ```
     
     ```python
     # BEV feature encoder for map data
     class BEVFeatureEncoder(nn.Module):
         def __init__(self):
             self.bev_backbone = nn.Sequential(
                 ConvBlock(3, 64, stride=2),
                 ConvBlock(64, 128, stride=2),
                 ConvBlock(128, 256, stride=2)
             )
     ```
     
     ```python
     # Curriculum learning strategy
     class SpatialReasoningCurriculum:
         self.stages = [
             {"task": "single_object_localization", "difficulty": "easy", "steps": 5000},
             {"task": "pairwise_relation", "difficulty": "medium", "steps": 10000},
             {"task": "complex_navigation", "difficulty": "hard", "steps": 20000}
         ]
     ```

   - **VSI_bench.pdf** (Not yet read - pending current request)

4. **Errors and Fixes**:
   - **Error 1**: Initially tried to read "Qwen3.pdf" but file didn't exist
     - **Fix**: Used glob search to find available PDFs, discovered correct filename was "Qwen3-VL.pdf"
     - **No user feedback on this error** - successfully recovered automatically

5. **Problem Solving**:
   - **Problem 1**: How to leverage map company's data advantage for spatial understanding
     - **Solution**: Designed comprehensive data construction pipeline using:
       * Street view images + GPS-aligned map data
       * BEV rendering from map database
       * Navigation trajectory synthesis
       * 3D road element annotations from HD maps
   - **Problem 2**: How to improve model architecture for map-specific tasks
     - **Solution**: Proposed multiple architectural enhancements:
       * Road Network RoPE (topology-aware positional encoding)
       * Graph Neural Network integration for road network modeling
       * Dedicated BEV encoder for map imagery
       * Differentiable path planner module
   - **Problem 3**: Training strategy for spatial capabilities
     - **Solution**: Multi-phase approach:
       * Phase 1: Spatial basics (20k steps)
       * Phase 2: Map-specific injection (50k steps with curriculum)
       * Phase 3: RL fine-tuning (10k episodes)
       * Phase 4: Domain adaptation (5k steps)
   - **Ongoing**: Need to read VSI_bench.pdf and analyze its spatial intelligence tasks

6. **All User Messages**:
   - Message 1: "è¯·é˜…è¯»Qwen3.pdf,ç»™å‡ºè¿™ä¸ªQwen3çš„ç½‘ç»œç»“æ„åˆ†ä¸ºå‡ å—" (Read Qwen3.pdf and explain network structure breakdown)
   - Message 2: "[Request interrupted by user]è¯·é˜…è¯»Qwen3-VL.pdf,ç»™å‡ºè¿™ä¸ªQwen3-VLçš„ç½‘ç»œç»“æ„åˆ†ä¸ºå‡ å—" (Read Qwen3-VL.pdf instead)
   - Message 3: "è¯·ç»™å‡ºè¿™ç¯‡æ–‡ç« çš„è¯¦ç»†æŠ€æœ¯æŠ¥å‘Š" (Provide detailed technical report of this article)
   - Message 4: "æˆ‘æƒ³çŸ¥é“Qwen3-VLçš„Embodied/Spatial Understandingèƒ½åŠ›æ˜¯æ¥è‡ªäºé¢„è®­ç»ƒçš„é‚£ä¸€ä¸ªè¿‡ç¨‹å“ªäº›æ•°æ®?å¦‚æœæƒ³è¿›ä¸€æ­¥æå‡æ¨¡å‹è¿™ä¸€æ–¹é¢çš„èƒ½åŠ›,å¯ä»¥ä»å“ªäº›æ–¹é¢åŠªåŠ›?" (interrupted and restarted with more details)
   - Message 5: "æˆ‘æƒ³çŸ¥é“Qwen3-VLçš„Embodied/Spatial Understandingèƒ½åŠ›æ˜¯æ¥è‡ªäºé¢„è®­ç»ƒçš„å“ªä¸€ä¸ªè¿‡ç¨‹å“ªäº›æ•°æ®?å¦‚æœæƒ³è¿›ä¸€æ­¥æå‡æ¨¡å‹è¿™ä¸€æ–¹é¢çš„èƒ½åŠ›,å¯ä»¥ä»å“ªäº›æ–¹é¢åŠªåŠ›?ç»™å‡ºå…·ä½“çš„ä¸€äº›æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ä»æ•°æ®ç«¯å’Œç½‘ç»œç»“æ„ç«¯è¿˜æœ‰è®­ç»ƒç­–ç•¥ç«¯ç­‰å¤šæ–¹é¢åˆ†æè¿™ç§å¯èƒ½çš„æ–¹æ¡ˆã€‚åœ¨è¿™é‡Œéœ€è¦å¼ºè°ƒçš„æ˜¯ï¼Œæˆ‘çš„å…¬å¸æ˜¯ä¸€å®¶åœ°å›¾å…¬å¸ï¼Œå…¬å¸å†…éƒ¨å«æœ‰ä¸°å¯Œçš„è¿™ç§é“è·¯æ•°æ®ï¼Œå› æ­¤åœ¨è€ƒè™‘æå‡æ–¹æ¡ˆæ—¶å¯ä»¥è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ã€‚" (Detailed request for improvement solutions considering map company context)
   - Message 6 (Current): "è¯·é˜…è¯»VSI_bench.pdfè¿™ç¯‡æ–‡ç« ,ç»™å‡ºæŠ€æœ¯æŠ¥å‘Š,å¹¶ä¸”é’ˆå¯¹è¿™ä¸ªå·¥ä½œä¸­çš„å‡ ä¸ªç©ºé—´æ™ºèƒ½çš„ä»»åŠ¡,é‡æ–°è®¾è®¡ä¸€ä¸‹é—®é¢˜" (Read VSI_bench.pdf, provide technical report, and redesign spatial intelligence tasks based on this work)

7. **Pending Tasks**:
   - **Task 1**: Read VSI_bench.pdf from the working directory
   - **Task 2**: Provide comprehensive technical report of VSI_bench paper
   - **Task 3**: Analyze the spatial intelligence tasks in VSI_bench
   - **Task 4**: Redesign these spatial intelligence tasks (likely incorporating insights from previous map company discussion)

8. **Current Work**:
   The user just requested to read and analyze VSI_bench.pdf. This appears to be a paper about spatial intelligence benchmarks (based on the filename "VSI_bench.pdf" which likely stands for "Visual Spatial Intelligence Benchmark"). The user wants:
   1. A technical report similar to what was provided for Qwen3-VL
   2. Analysis of the spatial intelligence tasks in this benchmark
   3. Redesign proposals for these tasks (potentially leveraging the map company context from previous discussion)
   
   The immediate next action is to locate and read the VSI_bench.pdf file from the same directory where Qwen3-VL.pdf was found (/Users/fyc/Documents/projects/test/).

9. **Optional Next Step**:
   Read the VSI_bench.pdf file to begin analysis. Based on the user's explicit request: "è¯·é˜…è¯»VSI_bench.pdfè¿™ç¯‡æ–‡ç« ,ç»™å‡ºæŠ€æœ¯æŠ¥å‘Š,å¹¶ä¸”é’ˆå¯¹è¿™ä¸ªå·¥ä½œä¸­çš„å‡ ä¸ªç©ºé—´æ™ºèƒ½çš„ä»»åŠ¡,é‡æ–°è®¾è®¡ä¸€ä¸‹é—®é¢˜" (Read VSI_bench.pdf, provide technical report, and redesign spatial intelligence tasks).
   
   The next step should be to execute a Read command on the file path: /Users/fyc/Documents/projects/test/VSI_bench.pdf

If you need specific details from before compaction (like exact code snippets, error messages, or content you generated), read the full transcript at: /Users/fyc/.claude/projects/-Users-fyc-Documents-projects-test/b9edadd0-3f50-4393-ac0f-074acef2136a.jsonl
Please continue the conversation from where we left it off without asking the user any further questions. Continue with the last task that you were asked to work on.

---

## ğŸ¤– Claude

I'll continue by reading the VSI_bench.pdf file and providing the technical report along with redesigned spatial intelligence tasks.

---

## ğŸ‘¤ User

[Request interrupted by user]

---

## ğŸ‘¤ User

è¯·é˜…è¯»VSI_bench.pdfè¿™ç¯‡æ–‡ç« ,ç»™å‡ºæŠ€æœ¯æŠ¥å‘Š,å¹¶ä¸”é’ˆå¯¹è¿™ä¸ªå·¥ä½œä¸­çš„å‡ ä¸ªç©ºé—´æ™ºèƒ½çš„ä»»åŠ¡,é‡æ–°è®¾è®¡ä¸€ä¸‹é—®é¢˜

---

## ğŸ¤– Claude

# VSI-Bench æŠ€æœ¯æŠ¥å‘Š

## ä¸€ã€æ ¸å¿ƒè´¡çŒ®

VSI-Benchï¼ˆVisual-Spatial Intelligence Benchmarkï¼‰æ˜¯é¦–ä¸ªåŸºäºè§†é¢‘çš„ç©ºé—´æ™ºèƒ½è¯„ä¼°åŸºå‡†ï¼ŒåŒ…å«**5,000+é—®ç­”å¯¹**ï¼Œè¦†ç›–**288ä¸ªçœŸå®å®¤å†…åœºæ™¯è§†é¢‘**ã€‚

### ä¸»è¦å‘ç°ï¼š
1. **MLLMså±•ç°å‡ºç«äº‰åŠ›ä½†æ¬¡äºäººç±»çš„ç©ºé—´æ™ºèƒ½**ï¼šæœ€ä½³æ¨¡å‹ï¼ˆGemini-1.5 Proï¼‰å¹³å‡å‡†ç¡®ç‡45.4%ï¼Œäººç±»è¾¾åˆ°79.2%
2. **ç©ºé—´æ¨ç†æ˜¯ä¸»è¦ç“¶é¢ˆ**ï¼š71%çš„é”™è¯¯æºäºç©ºé—´æ¨ç†èƒ½åŠ›ä¸è¶³
3. **è¯­è¨€æ¨ç†æŠ€æœ¯å¤±æ•ˆ**ï¼šCoTã€è‡ªä¸€è‡´æ€§ã€æ€ç»´æ ‘ç­‰æ–¹æ³•åœ¨ç©ºé—´ä»»åŠ¡ä¸Šåè€Œå¯¼è‡´æ€§èƒ½ä¸‹é™
4. **è®¤çŸ¥åœ°å›¾æœ‰åŠ©äºç©ºé—´è·ç¦»æ¨ç†**ï¼šæ˜¾å¼ç”Ÿæˆè®¤çŸ¥åœ°å›¾å¯æå‡10%çš„ç›¸å¯¹è·ç¦»å‡†ç¡®ç‡
5. **MLLMsæ„å»ºå±€éƒ¨è€Œéå…¨å±€ç©ºé—´æ¨¡å‹**ï¼šæ¨¡å‹å¯¹ç›¸é‚»ç‰©ä½“çš„ä½ç½®å‡†ç¡®ç‡è¾¾64%ï¼Œä½†è¿œè·ç¦»ç‰©ä½“å‡†ç¡®ç‡æ€¥å‰§ä¸‹é™

---

## äºŒã€ç©ºé—´æ™ºèƒ½åˆ†ç±»ä½“ç³»

è®ºæ–‡æå‡ºäº†**å››å¤§æ ¸å¿ƒèƒ½åŠ›**çš„åˆ†ç±»æ¡†æ¶ï¼š

### 1. **è§†è§‰æ„ŸçŸ¥** (Visual Perception)
- ç‰©ä½“è¯†åˆ«ä¸åˆ†ç±»

### 2. **æ—¶åºå¤„ç†** (Temporal Processing)  
- è§†é¢‘å¸§åºåˆ—ç†è§£

### 3. **è¯­è¨€æ™ºèƒ½** (Linguistic Intelligence)
- é€»è¾‘æ¨ç†ã€æ•°å­¦è®¡ç®—

### 4. **ç©ºé—´æ¨ç†** (Spatial Reasoning) â­æ ¸å¿ƒ
   - **å…³ç³»æ¨ç†**ï¼šè·ç¦»ã€æ–¹å‘ã€ç‰©ä½“å¤§å°å¸¸è¯†
   - **è‡ªæˆ‘ä¸­å¿ƒ-å¼‚æˆ‘ä¸­å¿ƒè½¬æ¢**ï¼šä»ç¬¬ä¸€äººç§°è§†è§’åˆ°é¸Ÿç°è§†è§’çš„è½¬æ¢èƒ½åŠ›

---

## ä¸‰ã€8é¡¹ç©ºé—´æ™ºèƒ½ä»»åŠ¡

### **é…ç½®å‹ä»»åŠ¡ (Configurational)**
1. **ç‰©ä½“è®¡æ•°** (Object Count)ï¼šæˆ¿é—´å†…ç‰¹å®šç‰©ä½“æ•°é‡
2. **ç›¸å¯¹è·ç¦»** (Relative Distance)ï¼šå¤šä¸ªç‰©ä½“ä¸­å“ªä¸ªç¦»ç›®æ ‡æœ€è¿‘
3. **ç›¸å¯¹æ–¹å‘** (Relative Direction)ï¼šåŸºäºç¬›å¡å°”åæ ‡ç³»çš„æ–¹ä½åˆ¤æ–­
4. **è·¯å¾„è§„åˆ’** (Route Plan)ï¼šå¡«å……å¯¼èˆªæŒ‡ä»¤ä¸­çš„è½¬å‘åŠ¨ä½œ

### **æµ‹é‡ä¼°è®¡ä»»åŠ¡ (Measurement Estimation)**
5. **ç‰©ä½“å°ºå¯¸** (Object Size)ï¼šç‰©ä½“æœ€é•¿ç»´åº¦ï¼ˆå˜ç±³ï¼‰
6. **æˆ¿é—´å¤§å°** (Room Size)ï¼šæˆ¿é—´é¢ç§¯ï¼ˆå¹³æ–¹ç±³ï¼‰
7. **ç»å¯¹è·ç¦»** (Absolute Distance)ï¼šä¸¤ç‰©ä½“é—´ç›´çº¿è·ç¦»ï¼ˆç±³ï¼‰

### **æ—¶ç©ºä»»åŠ¡ (Spatiotemporal)**
8. **å‡ºç°é¡ºåº** (Appearance Order)ï¼šç‰©ä½“åœ¨è§†é¢‘ä¸­çš„é¦–æ¬¡å‡ºç°é¡ºåº

---

## å››ã€åŸºå‡†æ„å»ºæ–¹æ³•

### æ•°æ®æ¥æº
- **ScanNet**ï¼ˆ88ä¸ªåœºæ™¯ï¼‰
- **ScanNet++**ï¼ˆ50ä¸ªåœºæ™¯ï¼‰
- **ARKitScenes**ï¼ˆ150ä¸ªåœºæ™¯ï¼‰

### æ„å»ºæµç¨‹
1. **æ•°æ®ç»Ÿä¸€åŒ–**ï¼šæ ‡å‡†åŒ–3Dæ ‡æ³¨ã€ç‰©ä½“è¾¹ç•Œæ¡†ã€è§†é¢‘è§„æ ¼
2. **QAç”Ÿæˆ**ï¼šåŸºäºæ¨¡æ¿è‡ªåŠ¨ç”Ÿæˆï¼ˆè·¯å¾„è§„åˆ’äººå·¥æ ‡æ³¨ï¼‰
3. **äººå·¥è´¨é‡å®¡æ ¸**ï¼šè¿­ä»£è¿‡æ»¤æ¨¡ç³Šé—®é¢˜å’Œé”™è¯¯æ ‡æ³¨

### è¯„ä¼°æŒ‡æ ‡
- **å¤šé€‰é¢˜**ï¼šå‡†ç¡®ç‡ (ACC)
- **æ•°å€¼é¢˜**ï¼šå¹³å‡ç›¸å¯¹å‡†ç¡®ç‡ (MRA)ï¼Œåœ¨å¤šä¸ªç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆ0.5-0.95ï¼‰ä¸Šå¹³å‡

---

## äº”ã€æ¨¡å‹è¯„ä¼°ç»“æœ

### é¡¶çº§æ¨¡å‹è¡¨ç°

| æ¨¡å‹ | å¹³å‡åˆ† | ç‰©ä½“è®¡æ•° | ç»å¯¹è·ç¦» | ç›¸å¯¹æ–¹å‘ | è·¯å¾„è§„åˆ’ |
|------|--------|----------|----------|----------|----------|
| **Human** | 79.2 | 94.3 | 47.0 | 94.7 | 95.8 |
| Gemini-1.5 Pro | 45.4 | 56.2 | 30.9 | 46.3 | 36.0 |
| LLaVA-Video-72B | 40.9 | 48.9 | 22.8 | 36.7 | 35.0 |
| GPT-4o | 34.0 | 46.2 | 5.3 | 41.3 | 31.5 |

### é”™è¯¯åˆ†æ
- **ç©ºé—´æ¨ç†é”™è¯¯**ï¼š71%ï¼ˆå…¶ä¸­å…³ç³»æ¨ç†å’Œè‡ªæˆ‘ä¸­å¿ƒ-å¼‚æˆ‘ä¸­å¿ƒè½¬æ¢å„å ä¸€åŠï¼‰
- **è§†è§‰æ„ŸçŸ¥é”™è¯¯**ï¼š15%
- **è¯­è¨€æ™ºèƒ½é”™è¯¯**ï¼š10%
- **æ—¶åºå¤„ç†é”™è¯¯**ï¼š4%

---

## å…­ã€è®¤çŸ¥åœ°å›¾å®éªŒ

### æ–¹æ³•
æç¤ºMLLMsåœ¨10Ã—10ç½‘æ ¼ä¸Šé¢„æµ‹ç‰©ä½“ä¸­å¿ƒä½ç½®ï¼Œè¯„ä¼°éšå¼ç©ºé—´ä¸–ç•Œæ¨¡å‹ã€‚

### å…³é”®å‘ç°
1. **å±€éƒ¨å‡†ç¡®æ€§é«˜**ï¼šç›¸é‚»ç‰©ä½“ï¼ˆè·ç¦»1.0-2.1ç½‘æ ¼å•ä½ï¼‰å‡†ç¡®ç‡64%
2. **å…¨å±€å‡†ç¡®æ€§å·®**ï¼šè¿œè·ç¦»ç‰©ä½“ï¼ˆ8.9-10.0ç½‘æ ¼å•ä½ï¼‰å‡†ç¡®ç‡æ¥è¿‘0%
3. **è®¤çŸ¥åœ°å›¾æå‡è·ç¦»æ¨ç†**ï¼š
   - æ— è®¤çŸ¥åœ°å›¾ï¼š46.0%
   - ä½¿ç”¨é¢„æµ‹è®¤çŸ¥åœ°å›¾ï¼š56.0% (+10%)
   - ä½¿ç”¨çœŸå®è®¤çŸ¥åœ°å›¾ï¼š66.0% (+20%)

---

## ä¸ƒã€é’ˆå¯¹åœ°å›¾å…¬å¸çš„ä»»åŠ¡é‡æ–°è®¾è®¡

åŸºäºè´µå…¬å¸çš„**é“è·¯æ•°æ®ä¼˜åŠ¿**ï¼Œæˆ‘é‡æ–°è®¾è®¡äº†8é¡¹ç©ºé—´æ™ºèƒ½ä»»åŠ¡ï¼Œå°†å®¤å†…åœºæ™¯ä»»åŠ¡è½¬åŒ–ä¸º**é“è·¯åœºæ™¯ä»»åŠ¡**ï¼š

### **ä»»åŠ¡1ï¼šé“è·¯å…ƒç´ è®¡æ•°** (Road Element Count)
**åŸä»»åŠ¡**ï¼šæˆ¿é—´å†…æœ‰å¤šå°‘ä¸ªæ²™å‘ï¼Ÿ  
**æ–°è®¾è®¡**ï¼š
```
é—®é¢˜ï¼šè¿™æ®µé“è·¯è§†é¢‘ä¸­æœ‰å¤šå°‘ä¸ªäº¤é€šä¿¡å·ç¯ï¼Ÿ
è¾“å…¥ï¼šè½¦è½½æ‘„åƒå¤´è§†é¢‘ï¼ˆè¡—æ™¯ï¼‰
ç­”æ¡ˆï¼š3ä¸ª
æ•°æ®æ¥æºï¼šHDåœ°å›¾çš„äº¤é€šè®¾æ–½æ ‡æ³¨ + è¡—æ™¯è§†é¢‘
```

**å¢å¼ºç‚¹**ï¼š
- åˆ©ç”¨å…¬å¸HDåœ°å›¾çš„ç²¾ç¡®äº¤é€šè®¾æ–½ä½ç½®
- ç»“åˆå¤šä¼ æ„Ÿå™¨èåˆï¼ˆæ‘„åƒå¤´+æ¿€å…‰é›·è¾¾ï¼‰æä¾›ground truth

---

### **ä»»åŠ¡2ï¼šè·¯ç½‘ç›¸å¯¹è·ç¦»** (Road Network Relative Distance)
**åŸä»»åŠ¡**ï¼šå†°ç®±ã€æ²™å‘ã€åŠç¯ã€ç §æ¿ä¸­å“ªä¸ªç¦»æ‰“å°æœºæœ€è¿‘ï¼Ÿ  
**æ–°è®¾è®¡**ï¼š
```
é—®é¢˜ï¼šä»å½“å‰ä½ç½®å‡ºå‘ï¼Œä»¥ä¸‹è·¯å£ï¼ˆAè·¯å£ã€Bè·¯å£ã€Cè·¯å£ã€Dè·¯å£ï¼‰ä¸­ï¼Œ
      å“ªä¸ªè·ç¦»ç›®æ ‡åœ°ç‚¹"XXå•†åœº"æœ€è¿‘ï¼Ÿï¼ˆæŒ‰é“è·¯ç½‘ç»œè·ç¦»ï¼‰
è¾“å…¥ï¼šå¯¼èˆªè§†è§’è§†é¢‘ + èµ·ç‚¹GPS
é€‰é¡¹ï¼šA. ä¸œæ–¹è·¯å£  B. äººæ°‘å¹¿åœº  C. å—äº¬è·¯å£  D. æ·®æµ·è·¯å£
ç­”æ¡ˆï¼šC
æ•°æ®æ¥æºï¼šè·¯ç½‘æ‹“æ‰‘ + æœ€çŸ­è·¯å¾„ç®—æ³•
```

**å¢å¼ºç‚¹**ï¼š
- è€ƒè™‘**è·¯ç½‘æ‹“æ‰‘çº¦æŸ**ï¼ˆä¸æ˜¯æ¬§æ°è·ç¦»è€Œæ˜¯é“è·¯è·ç¦»ï¼‰
- è¯„ä¼°æ¨¡å‹å¯¹é“è·¯è¿é€šæ€§çš„ç†è§£
- åˆ©ç”¨å…¬å¸è·¯å¾„è§„åˆ’æ•°æ®ç”Ÿæˆground truth

---

### **ä»»åŠ¡3ï¼šç›¸å¯¹æ–¹ä½åˆ¤æ–­** (Relative Direction in Navigation)
**åŸä»»åŠ¡**ï¼šæˆ‘ç«™åœ¨å†°ç®±æ—é¢å‘æ¡Œå­ï¼Œæ´—ç¢—æœºåœ¨æˆ‘çš„å·¦å‰ã€å³å‰ã€å·¦åè¿˜æ˜¯å³åï¼Ÿ  
**æ–°è®¾è®¡**ï¼š
```
é—®é¢˜ï¼šè½¦è¾†å½“å‰åœ¨"XXè·¯"å‘åŒ—è¡Œé©¶ï¼Œå‰æ–¹å³å°†åˆ°è¾¾"YYè·¯å£"ã€‚
      å¦‚æœç›®æ ‡æ˜¯"ZZå¤§å¦"ï¼ˆä½äºè·¯å£ä¸œå—æ–¹å‘ï¼‰ï¼Œ
      åˆ°è¾¾è·¯å£ååº”è¯¥å·¦è½¬ã€å³è½¬è¿˜æ˜¯ç›´è¡Œï¼Ÿ
è¾“å…¥ï¼šè½¦è½½è§†è§’è§†é¢‘ + èµ·ç‚¹/ç»ˆç‚¹POI
ç­”æ¡ˆï¼šå³è½¬
æ•°æ®æ¥æºï¼šHDåœ°å›¾è·¯å£è½¬å‘è§„åˆ™ + å¯¼èˆªè½¨è¿¹
```

**å¢å¼ºç‚¹**ï¼š
- æµ‹è¯•**egocentricâ†’allocentricè½¬æ¢**åœ¨çœŸå®å¯¼èˆªåœºæ™¯çš„èƒ½åŠ›
- èåˆè·¯å£è½¬å‘é™åˆ¶ã€è½¦é“ä¿¡æ¯
- åˆ©ç”¨å…¬å¸å¯¼èˆªæ•°æ®æ„å»ºå¤æ‚å†³ç­–ç‚¹

---

### **ä»»åŠ¡4ï¼šè·¯å¾„è§„åˆ’** (Navigation Route Planning)
**åŸä»»åŠ¡**ï¼šæœºå™¨äººä»é—¨å£åˆ°çª—æˆ·ï¼š1.å‰è¿›åˆ°åºŠâ†’2.[å¡«ç©º]â†’3.å‰è¿›åˆ°å¢™â†’4.[å¡«ç©º]  
**æ–°è®¾è®¡**ï¼š
```
é—®é¢˜ï¼šä»Aç‚¹ï¼ˆXXè·¯XXå·ï¼‰å¯¼èˆªåˆ°Bç‚¹ï¼ˆYYè·¯YYå·ï¼‰ï¼Œéœ€è¦ç»è¿‡ä»¥ä¸‹å…³é”®è·¯å£ã€‚
      è¯·å¡«å……æ¯ä¸ªå†³ç­–ç‚¹çš„è½¬å‘æŒ‡ä»¤ï¼š
      1. å‰è¿›è‡³"äººæ°‘è·¯å£" 
      2. [å·¦è½¬/å³è½¬/æ‰å¤´] 
      3. å‰è¿›è‡³"å—äº¬è·¯å£"
      4. [å·¦è½¬/å³è½¬/æ‰å¤´]
      5. åˆ°è¾¾ç›®çš„åœ°
è¾“å…¥ï¼šèµ·ç‚¹/ç»ˆç‚¹GPS + è¡—æ™¯è§†é¢‘åºåˆ—
ç­”æ¡ˆï¼šå·¦è½¬, å³è½¬
æ•°æ®æ¥æºï¼šçœŸå®å¯¼èˆªè½¨è¿¹ + äººå·¥éªŒè¯
```

**å¢å¼ºç‚¹**ï¼š
- æµ‹è¯•**å¤šè·³è·¯å¾„è§„åˆ’**èƒ½åŠ›
- åˆ©ç”¨å…¬å¸æµ·é‡å¯¼èˆªè®¢å•æ•°æ®ç”ŸæˆçœŸå®è·¯å¾„
- è¯„ä¼°æ¨¡å‹å¯¹è·¯ç½‘å…¨å±€ç†è§£

---

### **ä»»åŠ¡5ï¼šè·¯é¢è®¾æ–½å°ºå¯¸** (Road Facility Size)
**åŸä»»åŠ¡**ï¼šå£ç‚‰æœ€é•¿ç»´åº¦æ˜¯å¤šå°‘å˜ç±³ï¼Ÿ  
**æ–°è®¾è®¡**ï¼š
```
é—®é¢˜ï¼šè§†é¢‘ä¸­çš„"é™é«˜æ ‡å¿—"æ˜¾ç¤ºçš„é™é«˜æ˜¯å¤šå°‘ç±³ï¼Ÿ
è¾“å…¥ï¼šè½¦è½½æ‘„åƒå¤´è§†é¢‘
ç­”æ¡ˆï¼š4.5ç±³
æ•°æ®æ¥æºï¼šHDåœ°å›¾çš„é“è·¯è®¾æ–½å±æ€§æ ‡æ³¨
```

**å¢å¼ºç‚¹**ï¼š
- æµ‹è¯•**OCR + ç©ºé—´ç†è§£**èåˆèƒ½åŠ›
- åˆ©ç”¨å…¬å¸äº¤é€šæ ‡å¿—æ•°æ®åº“éªŒè¯

---

### **ä»»åŠ¡6ï¼šé“è·¯å®½åº¦ä¼°è®¡** (Road Width Estimation)
**åŸä»»åŠ¡**ï¼šæˆ¿é—´é¢ç§¯æ˜¯å¤šå°‘å¹³æ–¹ç±³ï¼Ÿ  
**æ–°è®¾è®¡**ï¼š
```
é—®é¢˜ï¼šå½“å‰è¡Œé©¶çš„é“è·¯å®½åº¦æ˜¯å¤šå°‘ç±³ï¼Ÿ
      ï¼ˆåŒ…å«æ‰€æœ‰è½¦é“ + äººè¡Œé“ï¼‰
è¾“å…¥ï¼šè½¦è½½å¤šè§†è§’è§†é¢‘
ç­”æ¡ˆï¼š15.2ç±³
æ•°æ®æ¥æºï¼šHDåœ°å›¾çš„é“è·¯å®½åº¦å­—æ®µ
```

**å¢å¼ºç‚¹**ï¼š
- æµ‹è¯•å¯¹**3Dç©ºé—´åº¦é‡**çš„ç†è§£
- ç»“åˆæ¿€å…‰é›·è¾¾ç‚¹äº‘æä¾›ç²¾ç¡®ground truth

---

### **ä»»åŠ¡7ï¼šè½¦è¾†é—´è·ä¼°è®¡** (Vehicle Distance Estimation)
**åŸä»»åŠ¡**ï¼šåºŠå’Œæ¤…å­ä¹‹é—´çš„ç›´çº¿è·ç¦»æ˜¯å¤šå°‘ç±³ï¼Ÿ  
**æ–°è®¾è®¡**ï¼š
```
é—®é¢˜ï¼šæœ¬è½¦ä¸å‰æ–¹è½¦è¾†çš„è·ç¦»æ˜¯å¤šå°‘ç±³ï¼Ÿ
è¾“å…¥ï¼šè½¦è½½å‰è§†æ‘„åƒå¤´è§†é¢‘
ç­”æ¡ˆï¼š25.3ç±³
æ•°æ®æ¥æºï¼šæ¯«ç±³æ³¢é›·è¾¾ + æ¿€å…‰é›·è¾¾èåˆæµ‹è·
```

**å¢å¼ºç‚¹**ï¼š
- **å®‰å…¨å…³é”®ä»»åŠ¡**ï¼Œå¯¹è‡ªåŠ¨é©¾é©¶è‡³å…³é‡è¦
- åˆ©ç”¨å…¬å¸å¤šä¼ æ„Ÿå™¨èåˆæ•°æ®æä¾›é«˜ç²¾åº¦æ ‡æ³¨

---

### **ä»»åŠ¡8ï¼šPOIå‡ºç°é¡ºåº** (POI Appearance Order)
**åŸä»»åŠ¡**ï¼šç¯®å­ã€æ‰“å°æœºã€å†°ç®±ã€æ°´å£¶åœ¨è§†é¢‘ä¸­çš„é¦–æ¬¡å‡ºç°é¡ºåºï¼Ÿ  
**æ–°è®¾è®¡**ï¼š
```
é—®é¢˜ï¼šæ²¿å¯¼èˆªè·¯çº¿è¡Œé©¶ï¼Œä»¥ä¸‹POIçš„å‡ºç°é¡ºåºæ˜¯ä»€ä¹ˆï¼Ÿ
      A. æ˜Ÿå·´å…‹ â†’ ä¸­å›½é“¶è¡Œ â†’ éº¦å½“åŠ³ â†’ åœ°é“ç«™
      B. ä¸­å›½é“¶è¡Œ â†’ æ˜Ÿå·´å…‹ â†’ åœ°é“ç«™ â†’ éº¦å½“åŠ³
      C. éº¦å½“åŠ³ â†’ æ˜Ÿå·´å…‹ â†’ ä¸­å›½é“¶è¡Œ â†’ åœ°é“ç«™
      D. æ˜Ÿå·´å…‹ â†’ éº¦å½“åŠ³ â†’ ä¸­å›½é“¶è¡Œ â†’ åœ°é“ç«™
è¾“å…¥ï¼šå¯¼èˆªå…¨ç¨‹è§†é¢‘ + èµ·ç‚¹/ç»ˆç‚¹
ç­”æ¡ˆï¼šA
æ•°æ®æ¥æºï¼šå…¬å¸POIæ•°æ®åº“ + GPSè½¨è¿¹åŒ¹é…
```

**å¢å¼ºç‚¹**ï¼š
- æµ‹è¯•**æ—¶åºè®°å¿† + ç©ºé—´å®šä½**èåˆ
- åˆ©ç”¨å…¬å¸POIæ•°æ®å’Œè½¨è¿¹æ•°æ®è‡ªåŠ¨ç”Ÿæˆ

---

## å…«ã€æ•°æ®æ„å»ºä¼˜åŠ¿

### è´µå…¬å¸çš„ç‹¬ç‰¹ä¼˜åŠ¿

| æ•°æ®ç±»å‹ | VSI-BenchåŸæ–¹æ³• | åœ°å›¾å…¬å¸ä¼˜åŠ¿ |
|---------|----------------|-------------|
| **Ground Truthæ¥æº** | 3Dé‡å»ºæ ‡æ³¨ï¼ˆå­˜åœ¨è¯¯å·®ï¼‰ | HDåœ°å›¾å˜ç±³çº§ç²¾åº¦æ ‡æ³¨ |
| **åœºæ™¯è§„æ¨¡** | 288å®¤å†…è§†é¢‘ | ç™¾ä¸‡çº§è¡—æ™¯è§†é¢‘ |
| **ç©ºé—´å¤æ‚åº¦** | å•æˆ¿é—´æ‹“æ‰‘ | åŸå¸‚çº§è·¯ç½‘æ‹“æ‰‘ |
| **å®ç”¨ä»·å€¼** | å­¦æœ¯ç ”ç©¶ | ç›´æ¥èµ‹èƒ½å¯¼èˆª/è‡ªåŠ¨é©¾é©¶ |
| **å¤šæ¨¡æ€èåˆ** | ä»…RGBè§†é¢‘ | è§†é¢‘+GPS+é›·è¾¾+IMU |

### æ¨èæ•°æ®ç”Ÿæˆæµç¨‹

```python
# ä¼ªä»£ç ç¤ºä¾‹
class RoadVSIBenchGenerator:
    def __init__(self, hd_map_db, street_view_db, trajectory_db):
        self.hd_map = hd_map_db
        self.street_view = street_view_db
        self.trajectory = trajectory_db
    
    def generate_route_planning_task(self):
        # 1. ä»çœŸå®å¯¼èˆªè®¢å•ä¸­é‡‡æ ·èµ·ç»ˆç‚¹
        route = self.trajectory.sample_real_navigation()
        
        # 2. è¯†åˆ«å…³é”®å†³ç­–ç‚¹
        decision_points = self.hd_map.extract_decision_points(route)
        
        # 3. ç”Ÿæˆé—®é¢˜æ¨¡æ¿
        question = self.create_fill_in_blank_question(decision_points)
        
        # 4. åŒ¹é…è¡—æ™¯è§†é¢‘
        video = self.street_view.match_trajectory(route)
        
        return {
            'video': video,
            'question': question,
            'answer': ground_truth_actions,
            'metadata': {
                'route_length': route.length,
                'num_turns': len(decision_points),
                'road_types': route.road_types
            }
        }
```

---

## ä¹ã€é¢„æœŸæ•ˆæœä¸ä»·å€¼

### å­¦æœ¯ä»·å€¼
1. **é¦–ä¸ªå¤§è§„æ¨¡é“è·¯åœºæ™¯ç©ºé—´æ™ºèƒ½åŸºå‡†**
2. **æ›´è´´è¿‘å®é™…åº”ç”¨**ï¼ˆå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ï¼‰
3. **æ›´å¤æ‚çš„ç©ºé—´æ¨ç†**ï¼ˆè·¯ç½‘æ‹“æ‰‘ vs æ¬§æ°ç©ºé—´ï¼‰

### å•†ä¸šä»·å€¼
1. **ç›´æ¥è¯„ä¼°å¯¼èˆªåŠ©æ‰‹æ¨¡å‹**çš„ç©ºé—´ç†è§£èƒ½åŠ›
2. **ä¸ºè‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥æ¨¡å‹**æä¾›è¯„æµ‹æ ‡å‡†
3. **æ•°æ®é£è½®æ•ˆåº”**ï¼šè¯„ä¼°â†’æ”¹è¿›â†’å¯¼èˆªä½“éªŒæå‡â†’æ›´å¤šæ•°æ®

### æŠ€æœ¯ä¼˜åŠ¿
- **æ›´é«˜æ ‡æ³¨è´¨é‡**ï¼šHDåœ°å›¾ç²¾åº¦ >> 3Dé‡å»ºç²¾åº¦
- **æ›´å¤§æ•°æ®è§„æ¨¡**ï¼šåŸå¸‚çº§ >> å®¤å†…åœºæ™¯
- **æ›´å¼ºå®ç”¨æ€§**ï¼šå¯¼èˆª/é©¾é©¶ >> å®¤å†…æœºå™¨äºº

---

## æ€»ç»“

VSI-Benchä¸ºè§†è§‰-ç©ºé—´æ™ºèƒ½è¯„ä¼°æä¾›äº†å¼€åˆ›æ€§æ¡†æ¶ï¼Œä½†å…¶å®¤å†…åœºæ™¯è®¾å®šé™åˆ¶äº†å®é™…åº”ç”¨ä»·å€¼ã€‚**åŸºäºè´µå…¬å¸é“è·¯æ•°æ®ä¼˜åŠ¿**ï¼Œé‡æ–°è®¾è®¡çš„8é¡¹ä»»åŠ¡å¯ä»¥ï¼š

1. âœ… ä¿ç•™åŸå§‹ç©ºé—´æ¨ç†æ ¸å¿ƒï¼ˆå…³ç³»æ¨ç†ã€è§†è§’è½¬æ¢ï¼‰
2. âœ… æå‡å®ç”¨ä»·å€¼ï¼ˆç›´æ¥æœåŠ¡å¯¼èˆª/é©¾é©¶ä¸šåŠ¡ï¼‰
3. âœ… åˆ©ç”¨ç‹¬ç‰¹æ•°æ®ä¼˜åŠ¿ï¼ˆHDåœ°å›¾ã€æµ·é‡è½¨è¿¹ã€å¤šä¼ æ„Ÿå™¨ï¼‰
4. âœ… æ„å»ºæŠ€æœ¯æŠ¤åŸæ²³ï¼ˆé«˜è´¨é‡æ ‡æ³¨ + å¤§è§„æ¨¡æ•°æ®ï¼‰

å»ºè®®ä¼˜å…ˆå®ç°**è·¯å¾„è§„åˆ’**å’Œ**ç›¸å¯¹è·ç¦»**ä»»åŠ¡ï¼Œå› ä¸ºå®ƒä»¬å¯¹å¯¼èˆªä½“éªŒå½±å“æœ€å¤§ï¼Œä¸”å…¬å¸å·²æœ‰æˆç†Ÿçš„ground truthç”Ÿæˆèƒ½åŠ›ã€‚

---

## ğŸ‘¤ User

æˆ‘æƒ³çŸ¥é“Qwen3-VLçš„Embodied/Spatial Understandingèƒ½åŠ›åœ¨åŸæ–‡ä¸­æ˜¯é€šè¿‡å“ªå‡ ä¸ªæ•°æ®é›†çš„å“ªäº›æŒ‡æ ‡æ¥è¯„ä¼°çš„?ä¸ºäº†æå‡æ¨¡å‹åœ¨è¿™äº›æ•°æ®é›†ä¸Šçš„æ•ˆæœ,åº”è¯¥å¦‚ä½•å»åš?Qwen3-VLç›®å½•ä¸­å¯èƒ½åŒ…å«å…³é”®çš„ä»£ç ä¿¡æ¯,è¯·æŸ¥çœ‹è¯¥ç›®å½•å¹¶ä¸”å®¡æŸ¥ç›¸å…³ä»£ç 

---

## ğŸ¤– Claude

æˆ‘æ¥å¸®æ‚¨åˆ†æQwen3-VLçš„Embodied/Spatial Understandingèƒ½åŠ›è¯„ä¼°ï¼Œå¹¶æŸ¥çœ‹ç›¸å…³ä»£ç ã€‚

---

## ğŸ¤– Claude

ç°åœ¨è®©æˆ‘æœç´¢Qwen3-VL.pdfä¸­å…³äºEmbodied/Spatial Understandingçš„è¯„ä¼°ä¿¡æ¯ï¼š

---

## ğŸ¤– Claude

éå¸¸å¥½ï¼ç°åœ¨è®©æˆ‘æœç´¢è®­ç»ƒæ•°æ®å’Œæ–¹æ³•ç›¸å…³çš„ä¿¡æ¯ï¼š

---

## ğŸ¤– Claude

API Error: 402 {"error":{"message":"ä»Šæ—¥é…é¢å’Œé‡ç½®æ¬¡æ•°å‡å·²ç”¨å°½($15.14/$30.00),å°†äºæœªçŸ¥åè‡ªåŠ¨é‡ç½®,æˆ–æ‚¨å¯ä»¥å¼€å¯PAYGè‡ªåŠ¨æ‰£è´¹åŠŸèƒ½ç»§ç»­ä½¿ç”¨","type":"quota_exceeded","code":"daily_quota_exceeded_no_resets","param":null,"metadata":{"consumed_usd":"15.14","hint":"å¯å¼€å¯PAYGè‡ªåŠ¨æ‰£è´¹ç«‹å³ä½¿ç”¨","next_auto_reset_in":"æœªçŸ¥","quota_type":"daily","remaining_usd":"0.00","resets_remaining":0,"total_quota_usd":"30.00"}}}

---

## ğŸ‘¤ User

æˆ‘æƒ³çŸ¥é“Qwen3-VLçš„Embodied/Spatial Understandingèƒ½åŠ›åœ¨åŸæ–‡ä¸­æ˜¯é€šè¿‡å“ªå‡ ä¸ªæ•°æ®é›†çš„å“ªäº›æŒ‡æ ‡æ¥è¯„ä¼°çš„?ä¸ºäº†æå‡æ¨¡å‹åœ¨è¿™äº›æ•°æ®é›†ä¸Šçš„æ•ˆæœ,åº”è¯¥å¦‚ä½•å»åš?Qwen3-VLç›®å½•ä¸­å¯èƒ½åŒ…å«å…³é”®çš„ä»£ç ä¿¡æ¯,è¯·æŸ¥çœ‹è¯¥ç›®å½•å¹¶ä¸”å®¡æŸ¥ç›¸å…³ä»£ç 

---

## ğŸ¤– Claude

API Error: 400 {"error":{"type":"<nil>","message":"{\"type\":\"error\",\"error\":{\"type\":\"invalid_request_error\",\"message\":\"tools.0: Input tag 'function' found using 'type' does not match any of the expected tags: 'bash_20250124', 'custom', 'text_editor_20250124', 'text_editor_20250429', 'text_editor_20250728', 'web_search_20250305'\"},\"request_id\":\"req_011CX5FjGExM2D68me1U5ZuL\"}ï¼ˆtraceid: 066e41870a458a18eb80e0173b737348ï¼‰ (request id: 2026011310551429652777ZDqUq85Q) (request id: 20260113185450233523797Wldj1fhc)"},"type":"error"}

---

## ğŸ‘¤ User

<local-command-caveat>Caveat: The messages below were generated by the user while running local commands. DO NOT respond to these messages or otherwise consider them in your response unless the user explicitly asks you to.</local-command-caveat>

---

## ğŸ‘¤ User

<command-name>/exit</command-name>
            <command-message>exit</command-message>
            <command-args></command-args>

---

## ğŸ‘¤ User

<local-command-stdout>Catch you later!</local-command-stdout>

---

## ğŸ¤– Claude

No response requested.

---

## ğŸ‘¤ User

æˆ‘æƒ³çŸ¥é“Qwen3-VLçš„Embodied/Spatial Understandingèƒ½åŠ›åœ¨åŸæ–‡ä¸­æ˜¯é€šè¿‡å“ªå‡ ä¸ªæ•°æ®é›†çš„å“ªäº›æŒ‡æ ‡æ¥è¯„ä¼°çš„,è¿™äº›æ•°æ®é›†åˆ†åˆ«æ˜¯é’ˆå¯¹ä»€ä¹ˆåœºæ™¯,ä»€ä¹ˆæ ·çš„ç©ºé—´èƒ½åŠ›?ä¸ºäº†æå‡æ¨¡å‹åœ¨è¿™äº›æ•°æ®é›†ä¸Šçš„æ•ˆæœ,åº”è¯¥å¦‚ä½•å»åš?Qwen3-VLç›®å½•ä¸­å¯èƒ½åŒ…å«å…³é”®çš„ä»£ç ä¿¡æ¯,è¯·æŸ¥çœ‹è¯¥ç›®å½•å¹¶ä¸”å®¡æŸ¥ç›¸å…³ä»£ç 

---

## ğŸ¤– Claude

æˆ‘æ¥å¸®æ‚¨å…¨é¢åˆ†æQwen3-VLçš„Embodied/Spatial Understandingè¯„ä¼°ä½“ç³»ï¼Œå¹¶æŸ¥çœ‹ç›¸å…³ä»£ç ã€‚

---

## ğŸ‘¤ User

[Request interrupted by user]

---

## ğŸ‘¤ User

æˆ‘æƒ³çŸ¥é“Qwen3-VLçš„Embodied/Spatial Understandingèƒ½åŠ›åœ¨åŸæ–‡ä¸­æ˜¯é€šè¿‡å“ªå‡ ä¸ªæ•°æ®é›†çš„å“ªäº›æŒ‡æ ‡æ¥è¯„ä¼°çš„,è¿™äº›æ•°æ®é›†åˆ†åˆ«æ˜¯é’ˆå¯¹ä»€ä¹ˆåœºæ™¯,ä»€ä¹ˆæ ·çš„ç©ºé—´èƒ½åŠ›?ä¸ºäº†æå‡æ¨¡å‹åœ¨è¿™äº›æ•°æ®é›†ä¸Šçš„æ•ˆæœ,åº”è¯¥å¦‚ä½•å»åš?Qwen3-VLç›®å½•ä¸­å¯èƒ½åŒ…å«å…³é”®çš„ä»£ç ä¿¡æ¯,è¯·æŸ¥çœ‹è¯¥ç›®å½•å¹¶ä¸”å®¡æŸ¥ç›¸å…³ä»£ç 

---

## ğŸ¤– Claude

æˆ‘æ¥ä¸ºæ‚¨å…¨é¢åˆ†æQwen3-VLçš„Embodied/Spatial Understandingè¯„ä¼°ä½“ç³»å’Œä»£ç å®ç°ã€‚

---

## ğŸ¤– Claude

ç°åœ¨è®©æˆ‘æŸ¥çœ‹æ›´å¤šç›¸å…³ä»£ç å’Œæ–‡æ¡£ç»†èŠ‚ï¼š

---

## ğŸ‘¤ User

[Request interrupted by user]

---

## ğŸ‘¤ User

æˆ‘æƒ³çŸ¥é“Qwen3-VL.pdfçš„Embodied/Spatial Understandingèƒ½åŠ›åœ¨åŸæ–‡ä¸­æ˜¯é€šè¿‡å“ªå‡ ä¸ªæ•°æ®é›†çš„å“ªäº›æŒ‡æ ‡æ¥è¯„ä¼°çš„,è¿™äº›æ•°æ®é›†åˆ†åˆ«æ˜¯é’ˆå¯¹ä»€ä¹ˆåœºæ™¯,ä»€ä¹ˆæ ·çš„ç©ºé—´èƒ½åŠ›?ä¸ºäº†æå‡æ¨¡å‹åœ¨è¿™äº›æ•°æ®é›†ä¸Šçš„æ•ˆæœ,åº”è¯¥å¦‚ä½•å»åš?Qwen3-VLç›®å½•ä¸­å¯èƒ½åŒ…å«å…³é”®çš„ä»£ç ä¿¡æ¯,è¯·æŸ¥çœ‹è¯¥ç›®å½•å¹¶ä¸”å®¡æŸ¥ç›¸å…³ä»£ç 

---

## ğŸ¤– Claude

æˆ‘ä¸ºæ‚¨æ•´ç†ä¸€ä»½å®Œæ•´çš„Qwen3-VL Embodied/Spatial Understandingè¯„ä¼°åˆ†ææŠ¥å‘Šã€‚

# Qwen3-VL Embodied/Spatial Understanding è¯„ä¼°ä¸æå‡æ–¹æ¡ˆ

## ä¸€ã€è¯„ä¼°æ•°æ®é›†ä¸æŒ‡æ ‡ä½“ç³»

æ ¹æ®Qwen3-VL.pdfçš„åˆ†æï¼ŒEmbodied/Spatial Understandingèƒ½åŠ›é€šè¿‡ä»¥ä¸‹æ•°æ®é›†è¯„ä¼°ï¼š

### 1. **æ ¸å¿ƒSpatial Understandingæ•°æ®é›†**

| æ•°æ®é›†åç§° | è¯„ä¼°æŒ‡æ ‡ | Qwen3-VLæ€§èƒ½ | æ•°æ®é›†åœºæ™¯ | è¯„ä¼°çš„ç©ºé—´èƒ½åŠ› |
|-----------|---------|-------------|----------|--------------|
| **EmbSpatialBench** | å‡†ç¡®ç‡(%) | 84.3 (Thinking)<br>83.1 (Instruct) | å…·èº«AIåœºæ™¯<br>æœºå™¨äººäº¤äº’ç¯å¢ƒ | â€¢ ç‰©ä½“å…³ç³»æ¨ç†<br>â€¢ å¯æ“ä½œæ€§ç†è§£<br>â€¢ åŠ¨ä½œè§„åˆ’ |
| **RefSpatialBench** | å‡†ç¡®ç‡(%) | 69.9 (Thinking)<br>65.5 (Instruct) | å‚è€ƒå®šä½åœºæ™¯<br>ç©ºé—´å¼•ç”¨è¡¨è¾¾ | â€¢ ç›¸å¯¹ä½ç½®æ¨ç†<br>â€¢ å‚è€ƒè¡¨è¾¾ç†è§£<br>â€¢ ç©ºé—´å…³ç³»æŸ¥è¯¢ |
| **RoboSpatialHome** | å‡†ç¡®ç‡(%) | 73.9 (Thinking)<br>69.4 (Instruct) | å®¶åº­æœºå™¨äººåœºæ™¯<br>å®¤å†…ç¯å¢ƒå¯¼èˆª | â€¢ å®¶åº­åœºæ™¯ç©ºé—´ç†è§£<br>â€¢ å¯¼èˆªè§„åˆ’<br>â€¢ ç‰©ä½“äº¤äº’ |
| **VSI-Bench** | å‡†ç¡®ç‡(%) | 60.0 (Thinking)<br>62.7 (Instruct) | è§†é¢‘ç©ºé—´ç†è§£<br>å®¤å†…åœºæ™¯ | â€¢ è§†é¢‘ä¸­çš„ç©ºé—´è®°å¿†<br>â€¢ ç‰©ä½“å®šä½<br>â€¢ è·ç¦»ä¼°è®¡<br>â€¢ æ–¹ä½åˆ¤æ–­ |
| **ERQA** | å‡†ç¡®ç‡(%) | 52.5 (Thinking)<br>51.3 (Instruct) | å¤šå›¾åƒå…·èº«æ¨ç†<br>å¤æ‚åœºæ™¯ç†è§£ | â€¢ å¤šå›¾åƒå…³ç³»æ¨ç†<br>â€¢ å…·èº«é—®ç­”<br>â€¢ å¤æ‚ç©ºé—´æ¨ç† |

### 2. **2D/3D Groundingæ•°æ®é›†**

| æ•°æ®é›†åç§° | è¯„ä¼°æŒ‡æ ‡ | Qwen3-VLæ€§èƒ½ | åœºæ™¯ç±»å‹ | ç©ºé—´èƒ½åŠ› |
|-----------|---------|-------------|---------|---------|
| **RefCOCO-avg** | å‡†ç¡®ç‡(%) | 92.1 (Thinking)<br>91.9 (Instruct) | è‡ªç„¶å›¾åƒ<br>2Då®šä½ | â€¢ 2Dè¾¹ç•Œæ¡†å®šä½<br>â€¢ å¼•ç”¨è¡¨è¾¾ç†è§£<br>â€¢ IoUâ‰¥0.5çš„ç²¾ç¡®å®šä½ |
| **CountBench** | å‡†ç¡®ç‡(%) | 93.7 (Thinking)<br>93.0 (Instruct) | ç‰©ä½“è®¡æ•°åœºæ™¯ | â€¢ è§†è§‰è®¡æ•°<br>â€¢ ç©ºé—´åˆ†å¸ƒç†è§£ |
| **ARKitScenes** | mAP@0.15 (%) | 53.7 (Thinking)<br>56.9 (Instruct) | å®¤å†…3Dåœºæ™¯<br>ARKitæ‰«æ | â€¢ 3Dè¾¹ç•Œæ¡†å®šä½<br>â€¢ 9-DoFç©ºé—´ç†è§£<br>â€¢ å®¤å†…ç‰©ä½“3Dæ£€æµ‹ |
| **Hypersim** | mAP@0.15 (%) | 11.0 (Thinking)<br>13.0 (Instruct) | åˆæˆ3Då®¤å†…åœºæ™¯ | â€¢ å¤æ‚å…‰ç…§ä¸‹çš„3Dç†è§£<br>â€¢ åˆæˆåœºæ™¯æ³›åŒ– |
| **SUN RGB-D** | mAP@0.15 (%) | 34.9 (Thinking)<br>39.4 (Instruct) | RGB-Då®¤å†…åœºæ™¯ | â€¢ æ·±åº¦å›¾èåˆ<br>â€¢ 3Dç‰©ä½“æ£€æµ‹ |
| **ODinW-13** | mAP (%) | 43.2 (Thinking)<br>48.6 (Instruct) | é‡å¤–ç‰©ä½“æ£€æµ‹<br>13ä¸ªé¢†åŸŸ | â€¢ å¼€æ”¾åŸŸç‰©ä½“æ£€æµ‹<br>â€¢ è·¨é¢†åŸŸæ³›åŒ– |

---

## äºŒã€å„æ•°æ®é›†çš„å…·ä½“ç©ºé—´èƒ½åŠ›è¯„ä¼°

### **EmbSpatialBenchï¼ˆå…·èº«ç©ºé—´åŸºå‡†ï¼‰**
- **åœºæ™¯**ï¼šæœºå™¨äººäº¤äº’ã€å…·èº«AIä»»åŠ¡
- **ç©ºé—´èƒ½åŠ›æµ‹è¯•**ï¼š
  1. **å…³ç³»æ¨ç†**ï¼šç†è§£"å·¦è¾¹çš„æ¯å­"ã€"æ¡Œå­åé¢çš„ä¹¦"ç­‰ç›¸å¯¹å…³ç³»
  2. **å¯æ“ä½œæ€§ç†è§£**ï¼šåˆ¤æ–­ç‰©ä½“æ˜¯å¦"å¯æŠ“æ¡"ã€"å¯å"ã€"å¯æŒ‰å‹"
  3. **åŠ¨ä½œè§„åˆ’**ï¼šå›ç­”"ä¸ºäº†å¤Ÿåˆ°æ˜¾ç¤ºå™¨åé¢çš„ä¹¦ï¼Œåº”è¯¥å…ˆç§»åŠ¨ä»€ä¹ˆï¼Ÿ"
- **æ•°æ®ç‰¹ç‚¹**ï¼šæ‰€æœ‰ç©ºé—´å¼•ç”¨éƒ½æ˜¯**ç›¸å¯¹äºå…¶ä»–ç‰©ä½“**è¡¨è¿°ï¼Œè€Œéç»å¯¹åæ ‡

### **RefSpatialBenchï¼ˆå‚è€ƒç©ºé—´åŸºå‡†ï¼‰**
- **åœºæ™¯**ï¼šç©ºé—´å‚è€ƒè¡¨è¾¾ã€ç›¸å¯¹å®šä½
- **ç©ºé—´èƒ½åŠ›æµ‹è¯•**ï¼š
  1. **ç©ºé—´å…³ç³»æŸ¥è¯¢**ï¼šå›ç­”å¤æ‚çš„ç©ºé—´å…³ç³»é—®é¢˜
  2. **å‚è€ƒè¡¨è¾¾å®šä½**ï¼šæ ¹æ®æè¿°æ‰¾åˆ°ç›®æ ‡ç‰©ä½“
  3. **å¤šç‰©ä½“å…³ç³»**ï¼šç†è§£å¤šä¸ªç‰©ä½“ä¹‹é—´çš„ç›¸å¯¹ä½ç½®

### **RoboSpatialHomeï¼ˆæœºå™¨äººå®¶åº­ç©ºé—´åŸºå‡†ï¼‰**
- **åœºæ™¯**ï¼šå®¶åº­ç¯å¢ƒã€å®¤å†…å¯¼èˆª
- **ç©ºé—´èƒ½åŠ›æµ‹è¯•**ï¼š
  1. **å®¶åº­åœºæ™¯ç†è§£**ï¼šç†è§£å¨æˆ¿ã€å®¢å…ç­‰ä¸åŒåŒºåŸŸ
  2. **å¯¼èˆªè§„åˆ’**ï¼šè§„åˆ’ä»Aç‚¹åˆ°Bç‚¹çš„è·¯å¾„
  3. **ç‰©ä½“äº¤äº’**ï¼šç†è§£å¦‚ä½•ä¸å®¶åº­ç‰©ä½“äº¤äº’

### **VSI-Benchï¼ˆè§†è§‰ç©ºé—´æ™ºèƒ½åŸºå‡†ï¼‰**
- **åœºæ™¯**ï¼šå®¤å†…åœºæ™¯è§†é¢‘ï¼ˆå…¬å¯“ã€å®éªŒå®¤ã€å·¥å‚ï¼‰
- **ç©ºé—´èƒ½åŠ›æµ‹è¯•**ï¼ˆ8é¡¹ä»»åŠ¡ï¼‰ï¼š
  1. **ç‰©ä½“è®¡æ•°**ï¼šæˆ¿é—´å†…æœ‰å¤šå°‘ä¸ªç‰©ä½“
  2. **ç›¸å¯¹è·ç¦»**ï¼šå“ªä¸ªç‰©ä½“ç¦»ç›®æ ‡æœ€è¿‘
  3. **ç›¸å¯¹æ–¹å‘**ï¼šåŸºäºç¬›å¡å°”åæ ‡çš„æ–¹ä½åˆ¤æ–­
  4. **è·¯å¾„è§„åˆ’**ï¼šå¡«å……å¯¼èˆªæŒ‡ä»¤
  5. **ç‰©ä½“å°ºå¯¸**ï¼šä¼°è®¡ç‰©ä½“æœ€é•¿ç»´åº¦ï¼ˆå˜ç±³ï¼‰
  6. **æˆ¿é—´å¤§å°**ï¼šä¼°è®¡é¢ç§¯ï¼ˆå¹³æ–¹ç±³ï¼‰
  7. **ç»å¯¹è·ç¦»**ï¼šä¸¤ç‰©ä½“é—´è·ç¦»ï¼ˆç±³ï¼‰
  8. **å‡ºç°é¡ºåº**ï¼šè§†é¢‘ä¸­ç‰©ä½“é¦–æ¬¡å‡ºç°é¡ºåº

### **ERQAï¼ˆå…·èº«å…³ç³»é—®ç­”ï¼‰**
- **åœºæ™¯**ï¼šå¤šå›¾åƒåœºæ™¯ã€å¤æ‚ç©ºé—´æ¨ç†
- **ç©ºé—´èƒ½åŠ›æµ‹è¯•**ï¼š
  1. **å¤šå›¾åƒç©ºé—´å…³ç³»**ï¼šè·¨å¤šå¼ å›¾ç‰‡ç†è§£ç‰©ä½“å…³ç³»
  2. **å…·èº«æ¨ç†**ï¼šéœ€è¦"æƒ³è±¡è‡ªå·±åœ¨åœºæ™¯ä¸­"çš„æ¨ç†

### **3D Groundingæ•°æ®é›†**
- **ARKitScenes/Hypersim/SUN RGB-D**ï¼š
  - **åæ ‡æ ¼å¼**ï¼š9-DoF 3Dè¾¹ç•Œæ¡† (x, y, z, x_size, y_size, z_size, roll, pitch, yaw)
  - **åæ ‡ç³»ç»Ÿ**ï¼šç»Ÿä¸€ä¸ºè™šæ‹Ÿç›¸æœºåæ ‡ç³»ï¼ˆéµå¾ªOmni3Dæ–¹æ³•ï¼‰
  - **è¯„ä¼°æ ‡å‡†**ï¼šmAP@0.15ï¼ˆIoUé˜ˆå€¼ä¸º0.15ï¼‰

---

## ä¸‰ã€è®­ç»ƒæ•°æ®æ„å»ºæ–¹æ³•ï¼ˆæ¥è‡ªQwen3-VL.pdf Section 3.2.5ï¼‰

### **Spatial Understandingè®­ç»ƒæ•°æ®**

```python
# æ•°æ®æ ‡æ³¨çš„ä¸‰ç§ç±»å‹
spatial_annotations = {
    # 1. å…³ç³»æ ‡æ³¨
    "relational": "the cup to the left of the laptop",
    
    # 2. å¯æ“ä½œæ€§æ ‡ç­¾
    "affordance": ["graspable", "pressable", "sittable"],
    
    # 3. åŠ¨ä½œæ¡ä»¶æŸ¥è¯¢
    "action_query": "What should I move first to reach the book behind the monitor?"
}
```

**æ•°æ®ç”Ÿæˆæ–¹æ³•**ï¼š
1. **æ¨¡æ¿ç”Ÿæˆ**ï¼šä½¿ç”¨é¢„å®šä¹‰æ¨¡æ¿ç”Ÿæˆå¤šæ ·åŒ–æŸ¥è¯¢
2. **LLMå¢å¼º**ï¼šä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆæ›´å¤æ‚çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢
3. **ç›¸å¯¹è¡¨è¿°**ï¼šå¼ºåˆ¶æ‰€æœ‰ç©ºé—´å¼•ç”¨ä½¿ç”¨ç›¸å¯¹æè¿°ï¼Œè€Œéç»å¯¹åæ ‡

### **3D Groundingè®­ç»ƒæ•°æ®**

```json
// æ•°æ®æ ¼å¼ç¤ºä¾‹
{
    "image": "scene_0001.jpg",
    "query": "the red chair next to the desk with detailed wood texture",
    "bbox_3d": {
        "x_center": 1.5,
        "y_center": 0.8,
        "z_center": 2.3,
        "x_size": 0.5,
        "y_size": 0.6,
        "z_size": 0.9,
        "roll": 0.0,
        "pitch": 0.0,
        "yaw": 1.57
    },
    "category": "chair"
}
```

**æ•°æ®å¤„ç†æµç¨‹**ï¼š
1. **åæ ‡ç»Ÿä¸€åŒ–**ï¼šæ‰€æœ‰æ¥è‡ªä¸åŒä¼ æ„Ÿå™¨çš„3Dæ ‡æ³¨ç»Ÿä¸€åˆ°è™šæ‹Ÿç›¸æœºåæ ‡ç³»
2. **è´¨é‡è¿‡æ»¤**ï¼šè¿‡æ»¤ä¸¥é‡é®æŒ¡å’Œä¸å‡†ç¡®çš„æ ‡ç­¾
3. **æ–‡æœ¬å¢å¼º**ï¼šåˆæˆä¸°å¯Œçš„æè¿°æ€§æ ‡é¢˜ï¼ŒåŒ…å«ï¼š
   - è¯¦ç»†å±æ€§ï¼ˆé¢œè‰²ã€æè´¨ã€çº¹ç†ï¼‰
   - å¸ƒå±€å®‰æ’ï¼ˆç›¸å¯¹ä½ç½®ï¼‰
   - ç©ºé—´ä½ç½®ï¼ˆåœ¨æˆ¿é—´çš„å“ªä¸ªéƒ¨åˆ†ï¼‰
   - è§†è§‰å¯æ“ä½œæ€§ï¼ˆåŠŸèƒ½æè¿°ï¼‰
   - ä¸å‘¨å›´ç‰©ä½“çš„äº¤äº’å…³ç³»

---

## å››ã€è®­ç»ƒé˜¶æ®µä¸­Spatialæ•°æ®çš„ä½¿ç”¨

æ ¹æ®Qwen3-VLçš„è®­ç»ƒæµç¨‹ï¼š

| è®­ç»ƒé˜¶æ®µ | Tokené¢„ç®— | åºåˆ—é•¿åº¦ | Spatialæ•°æ®ä½¿ç”¨ |
|---------|----------|---------|----------------|
| **Stage 0**<br>è§†è§‰-è¯­è¨€å¯¹é½ | 67B | 8K | âŒ ä¸åŒ…å«ï¼ˆä»…è®­ç»ƒMLPï¼‰ |
| **Stage 1**<br>å¤šæ¨¡æ€é¢„è®­ç»ƒ | ~1T | 8K | âœ… **å¼€å§‹å¼•å…¥**<br>â€¢ è§†è§‰æ¥åœ°ä»»åŠ¡<br>â€¢ 2D Grounding<br>â€¢ åŸºç¡€ç©ºé—´ç†è§£ |
| **Stage 2**<br>é•¿ä¸Šä¸‹æ–‡é¢„è®­ç»ƒ | ~1T | 32K | âœ… **å¤§å¹…å¢åŠ **<br>â€¢ æ›´å¤šè§†é¢‘æ•°æ®<br>â€¢ é¢å‘ä»£ç†çš„æŒ‡ä»¤è·Ÿéš<br>â€¢ 3D Groundingå¢å¼º |
| **Stage 3**<br>è¶…é•¿ä¸Šä¸‹æ–‡é€‚åº” | 100B | 256K | âœ… ç»§ç»­åŒ…å«<br>â€¢ é•¿è§†é¢‘ç©ºé—´ç†è§£<br>â€¢ å¤æ‚ç©ºé—´æ¨ç† |
| **Post-training**<br>SFT/RL | - | - | âœ… **é«˜è´¨é‡ç²¾è°ƒ**<br>â€¢ å…·èº«AIæŒ‡ä»¤<br>â€¢ ç©ºé—´æ¨ç†å¯¹è¯ |

---

## äº”ã€ä»£ç å®ç°åˆ†æ

### **Groundingè¯„ä¼°ä»£ç **ï¼ˆ`evaluate_grounding.py`ï¼‰

```python
# å…³é”®å®ç°
prompt = '<img>{}</img><ref>{}</ref><box>'  # è¾“å…¥æ ¼å¼

# é¢„æµ‹æ ¼å¼ï¼š((x1,y1),(x2,y2)) å½’ä¸€åŒ–åˆ° [0,999]
PATTERN = re.compile(r'\((.*?)\),\((.*?)\)')

# è¯„ä¼°æ ‡å‡†ï¼šIoU â‰¥ 0.5
if iou >= 0.5:
    correct += 1
```

**å…³é”®å‘ç°**ï¼š
1. **åæ ‡å½’ä¸€åŒ–**ï¼šé¢„æµ‹åæ ‡åœ¨[0,999]èŒƒå›´ï¼Œéœ€è¦è¿˜åŸåˆ°å®é™…å›¾åƒå°ºå¯¸
2. **IoUé˜ˆå€¼**ï¼š2Då®šä½ä½¿ç”¨0.5çš„ä¸¥æ ¼é˜ˆå€¼
3. **æ ¼å¼åŒ–è¾“å‡º**ï¼šæ¨¡å‹éœ€è¦ç²¾ç¡®æŒ‰ç…§ `((x1,y1),(x2,y2))` æ ¼å¼è¾“å‡º

---

## å…­ã€æå‡Spatial Understandingæ•ˆæœçš„æ–¹æ¡ˆ

### **æ–¹æ¡ˆ1ï¼šæ•°æ®å¢å¼ºç­–ç•¥** ğŸ”¥

#### 1.1 é’ˆå¯¹VSI-Benchçš„é“è·¯åœºæ™¯æ•°æ®æ„å»º

```python
class RoadSpatialDataGenerator:
    """é’ˆå¯¹åœ°å›¾å…¬å¸çš„é“è·¯åœºæ™¯ç©ºé—´æ•°æ®ç”Ÿæˆå™¨"""
    
    def generate_spatial_qa(self, street_view_video, hd_map_data):
        """
        ç”Ÿæˆé“è·¯åœºæ™¯ç©ºé—´é—®ç­”
        
        æ•°æ®æ¥æºï¼š
        - è¡—æ™¯è§†é¢‘ï¼ˆè½¦è½½æ‘„åƒå¤´ï¼‰
        - HDåœ°å›¾ï¼ˆå˜ç±³çº§ç²¾åº¦ï¼‰
        - GPSè½¨è¿¹
        - POIæ•°æ®
        """
        tasks = []
        
        # 1. é“è·¯å…ƒç´ è®¡æ•°
        tasks.append({
            "task": "road_element_count",
            "question": "è¿™æ®µé“è·¯è§†é¢‘ä¸­æœ‰å¤šå°‘ä¸ªäº¤é€šä¿¡å·ç¯ï¼Ÿ",
            "answer": self.count_traffic_lights(hd_map_data),
            "spatial_ability": "ç‰©ä½“è®¡æ•° + æ—¶åºè®°å¿†"
        })
        
        # 2. è·¯ç½‘ç›¸å¯¹è·ç¦»
        tasks.append({
            "task": "road_network_distance",
            "question": "ä»å½“å‰ä½ç½®å‡ºå‘ï¼Œå“ªä¸ªè·¯å£è·ç¦»ç›®æ ‡åœ°ç‚¹æœ€è¿‘ï¼Ÿï¼ˆæŒ‰é“è·¯è·ç¦»ï¼‰",
            "answer": self.compute_road_distance(hd_map_data),
            "spatial_ability": "è·¯ç½‘æ‹“æ‰‘ç†è§£ + æœ€çŸ­è·¯å¾„æ¨ç†"
        })
        
        # 3. å¯¼èˆªæ–¹ä½åˆ¤æ–­
        tasks.append({
            "task": "navigation_direction",
            "question": "è½¦è¾†åœ¨XXè·¯å‘åŒ—è¡Œé©¶ï¼Œåˆ°è¾¾YYè·¯å£ååº”è¯¥å·¦è½¬è¿˜æ˜¯å³è½¬ï¼Ÿ",
            "answer": self.infer_turn_direction(hd_map_data, target_poi),
            "spatial_ability": "è‡ªæˆ‘ä¸­å¿ƒ-å¼‚æˆ‘ä¸­å¿ƒè½¬æ¢ + è·¯å£ç†è§£"
        })
        
        return tasks
```

**ä¼˜åŠ¿**ï¼š
- åˆ©ç”¨HDåœ°å›¾çš„**å˜ç±³çº§ç²¾åº¦**æ ‡æ³¨
- é“è·¯åœºæ™¯æ¯”å®¤å†…åœºæ™¯**æ›´å¤æ‚**ï¼ˆè·¯ç½‘æ‹“æ‰‘çº¦æŸï¼‰
- ç›´æ¥æå‡**å¯¼èˆª/è‡ªåŠ¨é©¾é©¶**ç›¸å…³èƒ½åŠ›

#### 1.2 å¢å¼ºå…³ç³»æ ‡æ³¨çš„å¯†åº¦

```python
def augment_relational_annotations(scene_data):
    """
    å¢å¼ºç©ºé—´å…³ç³»æ ‡æ³¨å¯†åº¦
    
    ä»Qwen3-VLçš„è®­ç»ƒæ•°æ®æ„å»ºæ–¹æ³•å¯çŸ¥ï¼š
    å…³ç³»æ ‡æ³¨æ˜¯Spatial Understandingçš„æ ¸å¿ƒ
    """
    
    # åŸå§‹æ ‡æ³¨
    basic_relations = [
        "the cup to the left of the laptop"
    ]
    
    # å¢å¼ºæ ‡æ³¨ï¼ˆå¤šå±‚æ¬¡å…³ç³»ï¼‰
    enhanced_relations = [
        # 1. ä¸€é˜¶å…³ç³»ï¼ˆç›´æ¥ç›¸é‚»ï¼‰
        "the cup immediately to the left of the laptop",
        "the cup touching the left edge of the laptop",
        
        # 2. äºŒé˜¶å…³ç³»ï¼ˆé—´æ¥å…³ç³»ï¼‰
        "the cup is between the laptop and the monitor",
        "the cup is closer to the laptop than to the monitor",
        
        # 3. ä¸‰é˜¶å…³ç³»ï¼ˆå¤šç‰©ä½“å…³ç³»ï¼‰
        "the cup is to the left of the laptop and behind the keyboard",
        
        # 4. åŠŸèƒ½å…³ç³»ï¼ˆå¯æ“ä½œæ€§ + ç©ºé—´ï¼‰
        "the cup is within reachable distance from the chair",
        "the cup needs to be moved before accessing the keyboard"
    ]
    
    return enhanced_relations
```

#### 1.3 åˆæˆ3Dåœºæ™¯æ•°æ®

```python
def synthesize_3d_spatial_data(num_scenes=10000):
    """
    ä½¿ç”¨Blender/Unityåˆæˆå¤§è§„æ¨¡3Dåœºæ™¯
    
    è§£å†³Hypersimæ•°æ®é›†ä¸Šæ€§èƒ½ä½ï¼ˆ11%ï¼‰çš„é—®é¢˜
    """
    
    synthesis_config = {
        "scenes": [
            "living_room", "kitchen", "office", 
            "factory", "warehouse", "outdoor"
        ],
        "lighting_variations": ["sunny", "cloudy", "night", "indoor"],
        "camera_angles": 360,  # å…¨æ–¹ä½è§†è§’
        "object_density": "high",  # é«˜å¯†åº¦ç‰©ä½“å¸ƒå±€
        "occlusion_levels": [0.1, 0.3, 0.5, 0.7],  # ä¸åŒé®æŒ¡ç¨‹åº¦
    }
    
    # å…³é”®ï¼šç”Ÿæˆground truth 9-DoFæ ‡æ³¨
    for scene in scenes:
        annotations = {
            "objects": [],
            "3d_bboxes": [],  # ç²¾ç¡®çš„9-DoFè¾¹ç•Œæ¡†
            "spatial_relations": [],  # ç‰©ä½“é—´çš„ç©ºé—´å…³ç³»
            "affordances": [],  # å¯æ“ä½œæ€§æ ‡æ³¨
            "action_queries": []  # åŠ¨ä½œè§„åˆ’æŸ¥è¯¢
        }
        
    return synthetic_dataset
```

### **æ–¹æ¡ˆ2ï¼šæ¶æ„ä¼˜åŒ–** ğŸ—ï¸

#### 2.1 å¼•å…¥Road Network RoPE

```python
class RoadNetworkRoPE(nn.Module):
    """
    é“è·¯ç½‘ç»œæ„ŸçŸ¥çš„æ—‹è½¬ä½ç½®ç¼–ç 
    
    é’ˆå¯¹è·¯ç½‘æ‹“æ‰‘çš„ç‰¹æ®Šæ€§è®¾è®¡
    """
    
    def __init__(self, hidden_size, num_heads):
        super().__init__()
        self.road_distance_encoding = nn.Linear(1, hidden_size)
        self.topology_encoding = GraphAttention(hidden_size, num_heads)
        
    def forward(self, x, road_graph):
        """
        è¾“å…¥ï¼š
        - x: è§†è§‰ç‰¹å¾ [B, N, D]
        - road_graph: è·¯ç½‘å›¾ç»“æ„ (nodes, edges, distances)
        
        è¾“å‡ºï¼š
        - æ‹“æ‰‘æ„ŸçŸ¥çš„ä½ç½®ç¼–ç 
        """
        # 1. è®¡ç®—é“è·¯è·ç¦»ï¼ˆè€Œéæ¬§æ°è·ç¦»ï¼‰
        road_distances = self.compute_road_distance(road_graph)
        
        # 2. å›¾ç¥ç»ç½‘ç»œç¼–ç æ‹“æ‰‘ç»“æ„
        topology_features = self.topology_encoding(road_graph)
        
        # 3. èåˆåˆ°RoPE
        pos_encoding = self.rope(road_distances) + topology_features
        
        return x + pos_encoding
```

#### 2.2 3Dç©ºé—´æ³¨æ„åŠ›æœºåˆ¶

```python
class 3DSpatialAttention(nn.Module):
    """
    3Dç©ºé—´æ„ŸçŸ¥æ³¨æ„åŠ›
    
    è§£å†³3D Groundingæ€§èƒ½ç“¶é¢ˆ
    """
    
    def __init__(self, hidden_size):
        super().__init__()
        self.spatial_query = nn.Linear(hidden_size, hidden_size)
        self.spatial_key = nn.Linear(hidden_size + 9, hidden_size)  # +9 for 9-DoF
        
    def forward(self, visual_features, bbox_3d):
        """
        æ•´åˆ3Dè¾¹ç•Œæ¡†ä¿¡æ¯åˆ°æ³¨æ„åŠ›è®¡ç®—
        
        bbox_3d: [B, N, 9]  (x, y, z, x_size, y_size, z_size, roll, pitch, yaw)
        """
        # å°†3Då‡ ä½•ä¿¡æ¯ç¼–ç åˆ°attention keys
        spatial_keys = self.spatial_key(
            torch.cat([visual_features, bbox_3d], dim=-1)
        )
        
        # è®¡ç®—ç©ºé—´æ„ŸçŸ¥æ³¨æ„åŠ›
        attention_weights = torch.softmax(
            torch.matmul(queries, spatial_keys.transpose(-1, -2)), dim=-1
        )
        
        return torch.matmul(attention_weights, values)
```

### **æ–¹æ¡ˆ3ï¼šè®­ç»ƒç­–ç•¥ä¼˜åŒ–** ğŸ“ˆ

#### 3.1 Curriculum Learningï¼ˆè¯¾ç¨‹å­¦ä¹ ï¼‰

```python
class SpatialCurriculumTrainer:
    """
    ä»ç®€å•åˆ°å¤æ‚çš„ç©ºé—´ç†è§£è®­ç»ƒ
    """
    
    def __init__(self):
        self.stages = [
            # Stage 1: å•ç‰©ä½“å®šä½ï¼ˆç®€å•ï¼‰
            {
                "task": "single_object_grounding",
                "difficulty": "easy",
                "steps": 5000,
                "data": "RefCOCO (single object)"
            },
            
            # Stage 2: åŒç‰©ä½“å…³ç³»ï¼ˆä¸­ç­‰ï¼‰
            {
                "task": "pairwise_relation",
                "difficulty": "medium",
                "steps": 10000,
                "data": "EmbSpatialBench (pairwise)"
            },
            
            # Stage 3: å¤šç‰©ä½“å¤æ‚å…³ç³»ï¼ˆå›°éš¾ï¼‰
            {
                "task": "multi_object_reasoning",
                "difficulty": "hard",
                "steps": 15000,
                "data": "RoboSpatialHome (navigation)"
            },
            
            # Stage 4: 3Dç©ºé—´æ¨ç†ï¼ˆéå¸¸å›°éš¾ï¼‰
            {
                "task": "3d_grounding",
                "difficulty": "very_hard",
                "steps": 20000,
                "data": "ARKitScenes + Hypersim"
            }
        ]
```

#### 3.2 Contrastive Learning for Spatial Relations

```python
class SpatialContrastiveLoss(nn.Module):
    """
    å¯¹æ¯”å­¦ä¹ å¢å¼ºç©ºé—´å…³ç³»ç†è§£
    """
    
    def forward(self, anchor, positive, negative):
        """
        anchor: "the cup to the left of the laptop"
        positive: æ­£ç¡®çš„ç©ºé—´é…ç½®
        negative: é”™è¯¯çš„ç©ºé—´é…ç½®ï¼ˆå¦‚"the cup to the right of the laptop"ï¼‰
        """
        
        # è®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦
        pos_sim = F.cosine_similarity(anchor, positive)
        neg_sim = F.cosine_similarity(anchor, negative)
        
        # å¯¹æ¯”æŸå¤±ï¼šæ‹‰è¿‘æ­£æ ·æœ¬ï¼Œæ¨è¿œè´Ÿæ ·æœ¬
        loss = -torch.log(
            torch.exp(pos_sim / temperature) /
            (torch.exp(pos_sim / temperature) + torch.exp(neg_sim / temperature))
        )
        
        return loss
```

#### 3.3 å¼ºåŒ–å­¦ä¹ å¾®è°ƒï¼ˆé’ˆå¯¹åŠ¨ä½œè§„åˆ’ä»»åŠ¡ï¼‰

```python
class SpatialReasoningRLTrainer:
    """
    ä½¿ç”¨RLå¢å¼ºåŠ¨ä½œè§„åˆ’èƒ½åŠ›
    
    é’ˆå¯¹EmbSpatialBenchçš„action-conditioned queries
    """
    
    def train(self, env, model):
        """
        ç¯å¢ƒï¼šæ¨¡æ‹Ÿå…·èº«AIåœºæ™¯
        å¥–åŠ±ï¼š
        - æˆåŠŸå®Œæˆä»»åŠ¡ï¼š+10
        - æ­£ç¡®çš„ä¸­é—´æ­¥éª¤ï¼š+1
        - ç¢°æ’/é”™è¯¯åŠ¨ä½œï¼š-5
        """
        
        for episode in range(num_episodes):
            state = env.reset()  # åˆå§‹åœºæ™¯
            question = "ä¸ºäº†å¤Ÿåˆ°æ˜¾ç¤ºå™¨åé¢çš„ä¹¦ï¼Œåº”è¯¥å…ˆç§»åŠ¨ä»€ä¹ˆï¼Ÿ"
            
            # æ¨¡å‹é¢„æµ‹åŠ¨ä½œåºåˆ—
            actions = model.generate_action_plan(state, question)
            
            # åœ¨ç¯å¢ƒä¸­æ‰§è¡Œ
            success, trajectory = env.execute(actions)
            
            # è®¡ç®—å¥–åŠ±
            reward = self.compute_reward(success, trajectory)
            
            # PPOæ›´æ–°
            model.update(reward)
```

### **æ–¹æ¡ˆ4ï¼šæ•°æ®è´¨é‡æå‡** ğŸ¯

#### 4.1 Human-in-the-Loopæ ‡æ³¨

```python
class HITLSpatialAnnotation:
    """
    äººå·¥ä»‹å…¥çš„å¾ªç¯æ ‡æ³¨ç³»ç»Ÿ
    
    é’ˆå¯¹å¤æ‚çš„ç©ºé—´å…³ç³»å’Œ3Dæ ‡æ³¨
    """
    
    def annotate_with_verification(self, scene):
        """
        1. è‡ªåŠ¨ç”Ÿæˆå€™é€‰æ ‡æ³¨
        2. äººå·¥éªŒè¯å’Œä¿®æ­£
        3. è¿­ä»£æ”¹è¿›æ ‡æ³¨è´¨é‡
        """
        
        # è‡ªåŠ¨æ ‡æ³¨
        auto_annotations = self.auto_annotate(scene)
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence_scores = self.estimate_confidence(auto_annotations)
        
        # ä½ç½®ä¿¡åº¦æ ·æœ¬äººå·¥æ ‡æ³¨
        for annotation in auto_annotations:
            if annotation.confidence < 0.8:
                human_verified = self.human_verify(annotation)
                annotation = human_verified
        
        return verified_annotations
```

#### 4.2 å¤šæ¨¡æ€ä¸€è‡´æ€§éªŒè¯

```python
def verify_3d_consistency(image, bbox_3d, depth_map=None):
    """
    éªŒè¯3Dæ ‡æ³¨çš„ä¸€è‡´æ€§
    
    è§£å†³Qwen3-VLåœ¨Hypersimä¸Šæ€§èƒ½ä½çš„é—®é¢˜
    """
    
    checks = {
        # 1. æŠ•å½±ä¸€è‡´æ€§ï¼š3Dæ¡†æŠ•å½±åˆ°2Dåº”è¯¥åŒ…å«ç‰©ä½“
        "projection": check_projection_consistency(bbox_3d, image),
        
        # 2. æ·±åº¦ä¸€è‡´æ€§ï¼šå¦‚æœæœ‰æ·±åº¦å›¾ï¼Œæ£€æŸ¥æ·±åº¦å€¼åˆç†æ€§
        "depth": check_depth_consistency(bbox_3d, depth_map) if depth_map else None,
        
        # 3. ç‰©ç†åˆç†æ€§ï¼šç‰©ä½“ä¸åº”è¯¥æ‚¬ç©ºæˆ–ç©¿é€
        "physics": check_physical_plausibility(bbox_3d),
        
        # 4. å°ºå¯¸åˆç†æ€§ï¼šç‰©ä½“å°ºå¯¸åº”è¯¥åœ¨åˆç†èŒƒå›´
        "size": check_size_reasonableness(bbox_3d)
    }
    
    return all(checks.values())
```

---

## ä¸ƒã€é’ˆå¯¹æ€§èƒ½ç“¶é¢ˆçš„å…·ä½“ä¼˜åŒ–

### **ç“¶é¢ˆ1ï¼šHypersimæ€§èƒ½ä½ï¼ˆ11%ï¼‰**

**åŸå› åˆ†æ**ï¼š
- Hypersimæ˜¯åˆæˆåœºæ™¯ï¼Œå…‰ç…§å’Œçº¹ç†ä¸çœŸå®åœºæ™¯å·®å¼‚å¤§
- ç›¸æœºå‚æ•°å¤šæ ·ï¼Œåæ ‡ç³»ç»Ÿä¸ç»Ÿä¸€

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. Domain Adaptation
class DomainAdaptationFor3D:
    def __init__(self):
        self.real_encoder = RealSceneEncoder()
        self.synthetic_encoder = SyntheticSceneEncoder()
        self.domain_discriminator = DomainDiscriminator()
    
    def train(self, real_data, synthetic_data):
        # å¯¹æŠ—è®­ç»ƒï¼šè®©åˆ¤åˆ«å™¨æ— æ³•åŒºåˆ†çœŸå®å’Œåˆæˆåœºæ™¯
        # ç›®æ ‡ï¼šå­¦ä¹ åŸŸä¸å˜ç‰¹å¾
        pass

# 2. æ•°æ®å¢å¼º
def augment_for_hypersim(image, bbox_3d):
    """é’ˆå¯¹åˆæˆåœºæ™¯çš„ç‰¹æ®Šå¢å¼º"""
    augmentations = [
        RandomLighting(),      # å…‰ç…§å˜åŒ–
        RandomTexture(),       # çº¹ç†æ‰°åŠ¨
        CameraJitter(),        # ç›¸æœºå‚æ•°æ‰°åŠ¨
        OcclusionSimulation()  # é®æŒ¡æ¨¡æ‹Ÿ
    ]
    return apply_augmentations(image, bbox_3d, augmentations)
```

### **ç“¶é¢ˆ2ï¼šVSI-Benchæ€§èƒ½ï¼ˆ60%ï¼‰**

**åŸå› åˆ†æ**ï¼š
- VSI-Benchéœ€è¦è§†é¢‘ä¸­çš„**æ—¶åºç©ºé—´æ¨ç†**
- éœ€è¦æ„å»º"è®¤çŸ¥åœ°å›¾"è¿›è¡Œå…¨å±€ç©ºé—´ç†è§£

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# æ˜¾å¼è®¤çŸ¥åœ°å›¾ç”Ÿæˆ
class CognitiveMapGenerator:
    """
    æ ¹æ®VSI-Benchè®ºæ–‡çš„å‘ç°ï¼š
    æ˜¾å¼ç”Ÿæˆè®¤çŸ¥åœ°å›¾å¯æå‡10%æ€§èƒ½
    """
    
    def generate_map(self, video_frames):
        """
        ä»è§†é¢‘å¸§ç”Ÿæˆ10x10è®¤çŸ¥åœ°å›¾
        """
        # 1. æå–æ¯å¸§çš„ç‰©ä½“ä½ç½®
        object_positions = []
        for frame in video_frames:
            positions = self.detect_objects(frame)
            object_positions.append(positions)
        
        # 2. èåˆæ—¶åºä¿¡æ¯
        global_map = self.temporal_fusion(object_positions)
        
        # 3. å½’ä¸€åŒ–åˆ°10x10ç½‘æ ¼
        normalized_map = self.normalize_to_grid(global_map, grid_size=10)
        
        return normalized_map
    
    def answer_with_map(self, question, cognitive_map):
        """
        ä½¿ç”¨è®¤çŸ¥åœ°å›¾å›ç­”ç©ºé—´é—®é¢˜
        """
        # æ ¹æ®VSI-Benchçš„å®éªŒï¼šä½¿ç”¨è®¤çŸ¥åœ°å›¾æå‡è·ç¦»æ¨ç†
        if "distance" in question:
            return self.compute_distance_from_map(cognitive_map, question)
        elif "direction" in question:
            return self.compute_direction_from_map(cognitive_map, question)
```

### **ç“¶é¢ˆ3ï¼šERQAæ€§èƒ½ï¼ˆ52.5%ï¼‰**

**åŸå› åˆ†æ**ï¼š
- ERQAéœ€è¦**å¤šå›¾åƒå…³ç³»æ¨ç†**
- éœ€è¦è·¨å›¾ç‰‡çš„ç©ºé—´ä¸€è‡´æ€§ç†è§£

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
class MultiImageSpatialReasoner:
    """
    å¢å¼ºå¤šå›¾åƒç©ºé—´æ¨ç†èƒ½åŠ›
    """
    
    def fuse_spatial_context(self, images):
        """
        èåˆå¤šå¼ å›¾ç‰‡çš„ç©ºé—´ä¸Šä¸‹æ–‡
        """
        # 1. æå–æ¯å¼ å›¾ç‰‡çš„åœºæ™¯å›¾
        scene_graphs = [self.extract_scene_graph(img) for img in images]
        
        # 2. è·¨å›¾ç‰‡å¯¹é½ç‰©ä½“
        aligned_objects = self.cross_image_alignment(scene_graphs)
        
        # 3. æ„å»ºå…¨å±€ç©ºé—´å…³ç³»å›¾
        global_graph = self.build_global_graph(aligned_objects)
        
        return global_graph
    
    def reason_across_images(self, question, global_graph):
        """
        åœ¨å…¨å±€å›¾ä¸Šè¿›è¡Œæ¨ç†
        """
        # å›¾ç¥ç»ç½‘ç»œæ¨ç†
        reasoning_result = self.gnn_reasoning(global_graph, question)
        return reasoning_result
```

---

## å…«ã€å®Œæ•´æå‡æ–¹æ¡ˆå®æ–½è·¯çº¿å›¾

```
é˜¶æ®µ1ï¼ˆ1-2ä¸ªæœˆï¼‰ï¼šæ•°æ®å‡†å¤‡
â”œâ”€â”€ æ”¶é›†é“è·¯åœºæ™¯æ•°æ®ï¼ˆè¡—æ™¯è§†é¢‘ + HDåœ°å›¾ï¼‰
â”œâ”€â”€ ç”ŸæˆSpatial Understandingæ ‡æ³¨
â”‚   â”œâ”€â”€ å…³ç³»æ ‡æ³¨ï¼ˆæ¨¡æ¿ + LLMç”Ÿæˆï¼‰
â”‚   â”œâ”€â”€ å¯æ“ä½œæ€§æ ‡ç­¾
â”‚   â””â”€â”€ åŠ¨ä½œæ¡ä»¶æŸ¥è¯¢
â””â”€â”€ åˆæˆ3Dåœºæ™¯æ•°æ®ï¼ˆBlender/Unityï¼‰

é˜¶æ®µ2ï¼ˆ2-3ä¸ªæœˆï¼‰ï¼šæ¨¡å‹è®­ç»ƒ
â”œâ”€â”€ Curriculum Learning
â”‚   â”œâ”€â”€ Stage 1: å•ç‰©ä½“å®šä½ï¼ˆ5K stepsï¼‰
â”‚   â”œâ”€â”€ Stage 2: åŒç‰©ä½“å…³ç³»ï¼ˆ10K stepsï¼‰
â”‚   â”œâ”€â”€ Stage 3: å¤šç‰©ä½“æ¨ç†ï¼ˆ15K stepsï¼‰
â”‚   â””â”€â”€ Stage 4: 3Dæ¨ç†ï¼ˆ20K stepsï¼‰
â”œâ”€â”€ å¯¹æ¯”å­¦ä¹ ï¼ˆSpatial Relationsï¼‰
â””â”€â”€ Domain Adaptationï¼ˆçœŸå® â†” åˆæˆï¼‰

é˜¶æ®µ3ï¼ˆ1ä¸ªæœˆï¼‰ï¼šæ¨¡å‹ä¼˜åŒ–
â”œâ”€â”€ æ¶æ„æ”¹è¿›
â”‚   â”œâ”€â”€ Road Network RoPE
â”‚   â”œâ”€â”€ 3D Spatial Attention
â”‚   â””â”€â”€ è®¤çŸ¥åœ°å›¾ç”Ÿæˆæ¨¡å—
â””â”€â”€ è®­ç»ƒç­–ç•¥ä¼˜åŒ–
    â”œâ”€â”€ æ··åˆç²¾åº¦è®­ç»ƒ
    â””â”€â”€ æ¢¯åº¦ç´¯ç§¯

é˜¶æ®µ4ï¼ˆ1ä¸ªæœˆï¼‰ï¼šè¯„ä¼°ä¸è¿­ä»£
â”œâ”€â”€ åœ¨æ‰€æœ‰åŸºå‡†ä¸Šè¯„ä¼°
â”œâ”€â”€ åˆ†æé”™è¯¯æ¡ˆä¾‹
â””â”€â”€ é’ˆå¯¹æ€§æ•°æ®è¡¥å……

é¢„æœŸæå‡ï¼š
âœ… EmbSpatialBench: 84.3% â†’ 90%+
âœ… VSI-Bench: 60.0% â†’ 75%+
âœ… Hypersim: 11.0% â†’ 30%+
âœ… ERQA: 52.5% â†’ 65%+
```

---

## ä¹ã€å…³é”®ä»£ç å®¡æŸ¥ç»“è®º

æ ¹æ®`Qwen-VL/eval_mm/evaluate_grounding.py`çš„åˆ†æï¼š

1. **è¾“å…¥æ ¼å¼**ï¼š`<img>{image}</img><ref>{text}</ref><box>`
2. **è¾“å‡ºæ ¼å¼**ï¼š`((x1,y1),(x2,y2))` å½’ä¸€åŒ–åˆ°[0,999]
3. **è¯„ä¼°æ ‡å‡†**ï¼šIoU â‰¥ 0.5
4. **åæ ‡è¿˜åŸ**ï¼šé¢„æµ‹åæ ‡é™¤ä»¥999åä¹˜ä»¥å®é™…å›¾åƒå°ºå¯¸

è¿™è¯´æ˜æ¨¡å‹åœ¨è®­ç»ƒæ—¶éœ€è¦ï¼š
- ä¸¥æ ¼çš„æ ¼å¼åŒ–è¾“å‡ºè®­ç»ƒ
- å½’ä¸€åŒ–åæ ‡ç³»ç»Ÿçš„ç†è§£
- ç²¾ç¡®çš„è¾¹ç•Œæ¡†å›å½’èƒ½åŠ›









# Qwen3-VL Embodied/Spatial Understanding å®Œæ•´åˆ†ææŠ¥å‘Š

## ä¸€ã€é¢„è®­ç»ƒè¿‡ç¨‹ä¸­çš„Spatial Understandingæ•°æ®ä¸æ–¹æ³•

### 1.1 è®­ç»ƒæ•°æ®æ¥æºï¼ˆæ¥è‡ªPDF Section 3.2.5ï¼‰

æ ¹æ®Qwen3-VL.pdfçš„è¯¦ç»†åˆ†æï¼Œä»¥ä¸‹æ˜¯ä¸“é—¨ä¸ºå¢å¼ºSpatial Understandingèƒ½åŠ›çš„æ•°æ®ï¼š

#### **Spatial Understandingè®­ç»ƒæ•°æ®**

| æ•°æ®ç±»å‹ | å…·ä½“å†…å®¹ | æ•°æ®æ¥æº | æ ‡æ³¨æ ¼å¼ |
|---------|---------|---------|---------|
| **å…³ç³»æ ‡æ³¨** | "the cup to the left of the laptop" | ç²¾é€‰çœŸå®åœºæ™¯ + åˆæˆå¸ƒå±€ | è‡ªç„¶è¯­è¨€ç›¸å¯¹ä½ç½®æè¿° |
| **å¯æ“ä½œæ€§æ ‡ç­¾** | "graspable", "pressable", "sittable" | äººå·¥æ ‡æ³¨ + è‡ªåŠ¨ç”Ÿæˆ | å±æ€§æ ‡ç­¾åˆ—è¡¨ |
| **åŠ¨ä½œæ¡ä»¶æŸ¥è¯¢** | "What should I move first to reach the book behind the monitor?" | LLMç”Ÿæˆ + æ¨¡æ¿æ–¹æ³• | é—®ç­”å¯¹æ ¼å¼ |

**å…³é”®è®¾è®¡åŸåˆ™**ï¼š
```python
# æ‰€æœ‰ç©ºé—´å¼•ç”¨éƒ½ä½¿ç”¨ç›¸å¯¹è¡¨è¿°ï¼Œè€Œéç»å¯¹åæ ‡
spatial_reference_examples = {
    "æ­£ç¡®": "the cup to the left of the laptop",
    "é”™è¯¯": "the cup at position (100, 200)"
}

# æ•°æ®ç”Ÿæˆæ–¹æ³•
def generate_spatial_data():
    # 1. æ¨¡æ¿æ–¹æ³•ï¼šç¡®ä¿åŸºç¡€è¦†ç›–
    template_queries = [
        "What is {relation} the {object}?",
        "Describe the position of {object1} relative to {object2}"
    ]
    
    # 2. LLMå¢å¼ºï¼šå¢åŠ å¤šæ ·æ€§å’Œå¤æ‚æ€§
    llm_generated_queries = [
        "If I want to reach the book behind the monitor, what should I move first?",
        "Which object is closest to the window and can be sat on?"
    ]
    
    return template_queries + llm_generated_queries
```

#### **3D Groundingè®­ç»ƒæ•°æ®**

| æ•°æ®ç»„æˆ | è¯¦ç»†è¯´æ˜ | å¤„ç†æ–¹æ³• |
|---------|---------|---------|
| **å•è§†è§’å›¾åƒ** | å®¤å†…/å®¤å¤–åœºæ™¯ | æ¥è‡ªå…¬å¼€æ•°æ®é›† |
| **9-DoF 3Dè¾¹ç•Œæ¡†** | (x, y, z, x_size, y_size, z_size, roll, pitch, yaw) | **ç»Ÿä¸€åˆ°è™šæ‹Ÿç›¸æœºåæ ‡ç³»**ï¼ˆOmni3Dæ–¹æ³•ï¼‰ |
| **è‡ªç„¶è¯­è¨€å¼•ç”¨** | è¶…è¶Šç®€å•ç±»åˆ«åçš„ä¸°å¯Œæè¿° | åˆæˆå¤§è§„æ¨¡æè¿°æ€§æ ‡æ³¨ |

**æ•°æ®å¤„ç†æµç¨‹**ï¼š
```python
class 3DGroundingDataProcessor:
    """3D Groundingæ•°æ®å¤„ç†æµç¨‹"""
    
    def process_3d_annotations(self, raw_data):
        """
        å¤„ç†æ¥è‡ªå¤šä¸ªä¼ æ„Ÿå™¨çš„3Dæ ‡æ³¨
        """
        # æ­¥éª¤1: åæ ‡ç»Ÿä¸€åŒ–
        unified_coords = self.unify_to_virtual_camera(
            raw_data.bbox_3d,
            raw_data.camera_intrinsics
        )
        
        # æ­¥éª¤2: è´¨é‡è¿‡æ»¤
        filtered_data = self.filter_occluded_and_inaccurate(
            unified_coords,
            occlusion_threshold=0.7,
            accuracy_threshold=0.9
        )
        
        # æ­¥éª¤3: æ–‡æœ¬æè¿°åˆæˆ
        rich_descriptions = self.synthesize_descriptions(
            filtered_data,
            include_attributes=True,      # è¯¦ç»†å±æ€§
            include_layout=True,          # å¸ƒå±€å®‰æ’
            include_spatial_position=True, # ç©ºé—´ä½ç½®
            include_affordances=True,     # è§†è§‰å¯æ“ä½œæ€§
            include_interactions=True     # ä¸å‘¨å›´ç‰©ä½“çš„äº¤äº’
        )
        
        return {
            "image": raw_data.image,
            "bbox_3d": filtered_data,
            "query": rich_descriptions
        }
    
    def synthesize_descriptions(self, bbox_data, **kwargs):
        """
        åˆæˆä¸°å¯Œçš„æè¿°æ€§æ ‡æ³¨
        
        ç¤ºä¾‹è¾“å‡ºï¼š
        "the red wooden chair with detailed grain texture, 
         positioned next to the desk in the corner of the room,
         with a graspable backrest and sittable seat,
         partially occluded by the nearby bookshelf"
        """
        description_components = []
        
        if kwargs['include_attributes']:
            description_components.append(
                f"the {bbox_data.color} {bbox_data.material} {bbox_data.category} "
                f"with {bbox_data.texture_detail}"
            )
        
        if kwargs['include_layout']:
            description_components.append(
                f"positioned {bbox_data.relative_position} in the {bbox_data.room_area}"
            )
        
        if kwargs['include_affordances']:
            description_components.append(
                f"with {', '.join(bbox_data.affordances)}"
            )
        
        return ", ".join(description_components)
```

### 1.2 è®­ç»ƒé˜¶æ®µçš„æ•°æ®ä½¿ç”¨ï¼ˆæ¥è‡ªPDF Table 1ï¼‰

| è®­ç»ƒé˜¶æ®µ | Tokené¢„ç®— | åºåˆ—é•¿åº¦ | Spatialæ•°æ®ä½¿ç”¨æƒ…å†µ | å…³é”®ç‰¹ç‚¹ |
|---------|----------|---------|-------------------|---------|
| **Stage 0**<br>Vision-Language Alignment | 67B | 8K | âŒ **ä¸åŒ…å«** | ä»…è®­ç»ƒMLP Merger |
| **Stage 1**<br>Multimodal Pre-Training | ~1T | 8K | âœ… **å¼€å§‹å¼•å…¥**<br>â€¢ è§†è§‰groundingä»»åŠ¡<br>â€¢ 2D Grounding<br>â€¢ åŸºç¡€ç©ºé—´ç†è§£ | å…¨å‚æ•°è®­ç»ƒ<br>å°‘é‡è§†é¢‘æ•°æ® |
| **Stage 2**<br>Long-Context Pre-Training | ~1T | 32K | âœ… **å¤§å¹…å¢åŠ **<br>â€¢ æ›´å¤šè§†é¢‘æ•°æ®<br>â€¢ **é¢å‘ä»£ç†çš„æŒ‡ä»¤è·Ÿéš**<br>â€¢ 3D Groundingå¢å¼º | åºåˆ—é•¿åº¦4å€å¢åŠ <br>å¼ºè°ƒagentä»»åŠ¡ |
| **Stage 3**<br>Ultra-Long-Context | 100B | 256K | âœ… ç»§ç»­åŒ…å«<br>â€¢ é•¿è§†é¢‘ç©ºé—´ç†è§£<br>â€¢ å¤æ‚ç©ºé—´æ¨ç† | è¶…é•¿ä¸Šä¸‹æ–‡é€‚åº” |

**å…³é”®å‘ç°**ï¼š
- **Stage 1æ˜¯Spatial Understandingçš„èµ·ç‚¹**ï¼šé¦–æ¬¡å¼•å…¥groundingå’Œç©ºé—´ç†è§£æ•°æ®
- **Stage 2æ˜¯å…³é”®å¢å¼ºé˜¶æ®µ**ï¼šå¤§å¹…å¢åŠ agentå¯¼å‘æ•°æ®ï¼Œè¿™ç›´æ¥å¯¹åº”EmbSpatialBench/RoboSpatialHomeç­‰å…·èº«AIè¯„ä¼°

### 1.3 Post-Trainingä¸­çš„Spatialæ•°æ®ï¼ˆæ¥è‡ªPDF Section 4ï¼‰

#### **SFTé˜¶æ®µï¼ˆç›‘ç£å¾®è°ƒï¼‰**

```python
# SFTæ•°æ®ç»„æˆï¼ˆæ€»è®¡1,200,000æ ·æœ¬ï¼‰
sft_data_composition = {
    "text_only": "1/3",
    "image_text + video_text": "2/3",
    
    "spatial_related_domains": [
        "ç©ºé—´æ¨ç†ï¼ˆembodied intelligenceï¼‰",
        "å›¾åƒgroundingæ¨ç†ï¼ˆfine-grained visual understandingï¼‰",
        "è§†é¢‘ä¸­çš„æ—¶ç©ºgroundingï¼ˆrobust object trackingï¼‰"
    ]
}

# æ•°æ®è´¨é‡æ§åˆ¶
class SFTDataFilter:
    """ä¸¤é˜¶æ®µè¿‡æ»¤ç³»ç»Ÿ"""
    
    def query_filtering(self, queries):
        """æŸ¥è¯¢è¿‡æ»¤"""
        # 1. è¯†åˆ«ä¸å¯éªŒè¯çš„æŸ¥è¯¢
        verifiable_queries = self.filter_unverifiable(queries)
        
        # 2. æœ€å°åŒ–ä¿®æ”¹æ¨¡ç³ŠæŒ‡ä»¤
        clarified_queries = self.clarify_ambiguous(verifiable_queries)
        
        # 3. æ¶ˆé™¤ç¼ºä¹å®è´¨å†…å®¹çš„æŸ¥è¯¢
        substantial_queries = self.filter_trivial(clarified_queries)
        
        # 4. è¯„ä¼°å¤æ‚æ€§å’Œä¸Šä¸‹æ–‡ç›¸å…³æ€§
        final_queries = self.evaluate_complexity(substantial_queries)
        
        return final_queries
    
    def response_filtering(self, responses):
        """å“åº”è¿‡æ»¤"""
        # åŸºäºè§„åˆ™çš„è¿‡æ»¤
        rule_filtered = self.rule_based_filter(
            responses,
            check_repetition=True,
            check_completeness=True,
            check_format=True
        )
        
        # åŸºäºæ¨¡å‹çš„è¿‡æ»¤ï¼ˆä½¿ç”¨Qwen2.5-VLå¥–åŠ±æ¨¡å‹ï¼‰
        model_filtered = self.model_based_filter(
            rule_filtered,
            dimensions=[
                "correctness",
                "completeness", 
                "clarity",
                "helpfulness"
            ],
            # ç‰¹åˆ«å¼ºè°ƒï¼šéªŒè¯å‡†ç¡®çš„è§†è§‰ä¿¡æ¯è§£é‡Š
            emphasize_vision_grounding=True
        )
        
        return model_filtered
```

#### **Long-CoT Cold Start Dataï¼ˆé•¿é“¾å¼æ€ç»´æ•°æ®ï¼‰**

```python
# é’ˆå¯¹Spatial Understandingçš„CoTæ•°æ®
long_cot_spatial_data = {
    "VLæ ·æœ¬ä¸æ–‡æœ¬æ ·æœ¬æ¯”ä¾‹": "1:1",
    
    "å¤šæ¨¡æ€æˆåˆ†": [
        "VQAï¼ˆè§†è§‰é—®ç­”ï¼‰",
        "OCRï¼ˆå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼‰",
        "2D/3D grounding",  # â† Spatial Understandingæ ¸å¿ƒ
        "è§†é¢‘åˆ†æ",
        "STEMå’Œagenticå·¥ä½œæµä»»åŠ¡"  # â† å…·èº«AIä»»åŠ¡
    ],
    
    "å…³é”®è¿‡æ»¤æ­¥éª¤": {
        "éš¾åº¦ç­–åˆ’": "é€‰æ‹©åŸºçº¿æ¨¡å‹é€šè¿‡ç‡ä½çš„å®ä¾‹",
        
        # å…³é”®ï¼å¤šæ¨¡æ€å¿…è¦æ€§è¿‡æ»¤
        "multimodal_necessity_filter": """
        ä¸¢å¼ƒQwen3-30B-nothinkæ¨¡å‹æ— è§†è§‰è¾“å…¥ä»èƒ½æ­£ç¡®è§£å†³çš„æ ·æœ¬
        ç¡®ä¿å‰©ä½™å®ä¾‹ç¡®å®éœ€è¦å¤šæ¨¡æ€ç†è§£
        """,
        
        "å“åº”è´¨é‡æ§åˆ¶": "ç§»é™¤ä¸æ­£ç¡®ç»“æœå’Œä¸è‰¯æ¨¡å¼"
    }
}
```

#### **å¼ºåŒ–å­¦ä¹ é˜¶æ®µ**

```python
# Reasoning RLï¼ˆæ¨ç†å¼ºåŒ–å­¦ä¹ ï¼‰
reasoning_rl_tasks = {
    "spatial_related_tasks": [
        "è§†è§‰grounding",
        "è§†è§‰è°œé¢˜",
        "ç©ºé—´æ¨ç†é—®é¢˜"
    ],
    
    "æ•°æ®å‡†å¤‡": {
        "æ€»é‡": "~30K RLæŸ¥è¯¢",
        "é‡‡æ ·ç­–ç•¥": "æ¯ä¸ªæŸ¥è¯¢é‡‡æ ·16ä¸ªå“åº”",
        "è´¨é‡æ§åˆ¶": "ä¸¢å¼ƒæ‰€æœ‰å“åº”éƒ½ä¸æ­£ç¡®çš„æŸ¥è¯¢"
    },
    
    "å¥–åŠ±ç³»ç»Ÿ": {
        "éªŒè¯æ–¹å¼": "ç¡®å®šæ€§éªŒè¯ï¼ˆè§„åˆ™æˆ–ä»£ç æ‰§è¡Œå™¨ï¼‰",
        "ç®—æ³•": "SAPOï¼ˆSmooth and Adaptive Policy-gradient Optimizationï¼‰"
    }
}

# General RLï¼ˆé€šç”¨å¼ºåŒ–å­¦ä¹ ï¼‰
general_rl_tasks = {
    "spatial_related_tasks": [
        "grounding",
        "æ—¶é’Ÿè¯†åˆ«ï¼ˆç©ºé—´ç†è§£ï¼‰",
        "ç‰©ä½“è®¡æ•°ï¼ˆç©ºé—´åˆ†å¸ƒç†è§£ï¼‰"
    ],
    
    "ä¸¤ä¸ªæ€§èƒ½ç»´åº¦": {
        "æŒ‡ä»¤è·Ÿéš": "å¤„ç†å†…å®¹ã€æ ¼å¼ã€é•¿åº¦ã€ç»“æ„åŒ–è¾“å‡ºçš„å¤æ‚çº¦æŸ",
        "åå¥½å¯¹é½": "ä¸äººç±»åå¥½å¯¹é½ï¼ˆæœ‰ç”¨æ€§ã€å‡†ç¡®æ€§ã€é£æ ¼ï¼‰"
    },
    
    "æ··åˆå¥–åŠ±ç³»ç»Ÿ": {
        "åŸºäºè§„åˆ™çš„å¥–åŠ±": "å¯¹å¯éªŒè¯ä»»åŠ¡æä¾›æ¸…æ™°åé¦ˆï¼Œæœ‰æ•ˆç¼“è§£reward hacking",
        "åŸºäºæ¨¡å‹çš„å¥–åŠ±": "ä½¿ç”¨Qwen2.5-VL-72Bä½œä¸ºjudgeï¼Œè¯„ä¼°nuancedä»»åŠ¡"
    }
}
```

---

## äºŒã€ä»£ç å®ç°è¯æ®

### 2.1 æ ¸å¿ƒæ¶æ„ï¼š3D RoPEï¼ˆæ¥è‡ªrope2d.pyï¼‰

```python
# Qwen3-VL/qwen-vl-finetune/qwenvl/data/rope2d.py

def get_rope_index_3(
    spatial_merge_size: Optional[int] = 2,
    input_ids: Optional[torch.LongTensor] = None,
    image_grid_thw: Optional[torch.LongTensor] = None,
    video_grid_thw: Optional[torch.LongTensor] = None,
    second_per_grid_ts: Optional[torch.Tensor] = None,
    attention_mask: Optional[torch.Tensor] = None,
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Qwen3-VLä½¿ç”¨timestampsè€Œéç»å¯¹æ—¶é—´ä½ç½®ID
    
    å…³é”®åˆ›æ–°ï¼š
    - 3Dä½ç½®ç¼–ç ï¼š(temporal, height, width)
    - æ”¯æŒå›¾åƒå’Œè§†é¢‘çš„ç»Ÿä¸€å¤„ç†
    - æ—¶é—´æˆ³å¯¹é½ï¼šç²¾ç¡®çš„äº‹ä»¶å®šä½
    """
    
    # ä¸ºè§†é¢‘å’Œå›¾åƒè®¡ç®—3Dä½ç½®ç´¢å¼•
    # temporal: æ—¶é—´ç»´åº¦
    # height: é«˜åº¦ç»´åº¦  
    # width: å®½åº¦ç»´åº¦
    
    t_index = torch.arange(llm_grid_t).view(-1, 1).expand(-1, llm_grid_h * llm_grid_w).flatten()
    h_index = torch.arange(llm_grid_h).view(1, -1, 1).expand(llm_grid_t, -1, llm_grid_w).flatten()
    w_index = torch.arange(llm_grid_w).view(1, 1, -1).expand(llm_grid_t, llm_grid_h, -1).flatten()
    
    # å †å 3Dä½ç½®ç¼–ç 
    llm_pos_ids_list.append(torch.stack([t_index, h_index, w_index]) + text_len + st_idx)
```

**è¿™æ®µä»£ç çš„æ„ä¹‰**ï¼š
- **3Dç©ºé—´å»ºæ¨¡**ï¼šé€šè¿‡(t, h, w)ä¸‰ç»´ä½ç½®ç¼–ç ï¼Œæ¨¡å‹èƒ½å¤Ÿç†è§£è§†é¢‘/å›¾åƒä¸­çš„ç©ºé—´ç»“æ„
- **æ—¶åºç©ºé—´èåˆ**ï¼šæ”¯æŒVSI-Benchç­‰éœ€è¦æ—¶åºç©ºé—´æ¨ç†çš„ä»»åŠ¡
- **ç»Ÿä¸€å¤„ç†**ï¼šå›¾åƒå’Œè§†é¢‘ä½¿ç”¨ç›¸åŒçš„ä½ç½®ç¼–ç æ¡†æ¶

### 2.2 Groundingè¯„ä¼°å®ç°ï¼ˆæ¥è‡ªODinW-13/dataset_utils.pyï¼‰

```python
# Qwen3-VL/evaluation/ODinW-13/dataset_utils.py

def generate_odinw_jobs(data_dir: str, args):
    """ç”ŸæˆODinWç‰©ä½“æ£€æµ‹ä»»åŠ¡"""
    
    # å…³é”®ï¼šæ™ºèƒ½åˆ†è¾¨ç‡è°ƒæ•´
    def smart_resize(height, width, factor=28, 
                     min_pixels=56*56, 
                     max_pixels=14*14*4*1280):
        """
        è°ƒæ•´å›¾åƒå¤§å°ä»¥æ»¡è¶³ï¼š
        1. é«˜åº¦å’Œå®½åº¦éƒ½èƒ½è¢«factoræ•´é™¤
        2. æ€»åƒç´ åœ¨[min_pixels, max_pixels]èŒƒå›´å†…
        3. ä¿æŒå®½é«˜æ¯”
        """
        # è¿™ç¡®ä¿äº†ç©ºé—´ä¿¡æ¯çš„ç²¾ç¡®ä¿ç•™
        pass
    
    # æ„å»ºgrounding prompt
    prompt = f"Locate every instance that belongs to the following categories: '{obj_names}'. Report bbox coordinates in JSON format."
    
    # æ¶ˆæ¯æ ¼å¼
    messages = [{
        "role": "user",
        "content": [
            {"type": "image", "image": f"file://{img_path}"},
            {"type": "text", "text": prompt}
        ]
    }]
```

**å…³é”®å‘ç°**ï¼š
- **JSONæ ¼å¼è¾“å‡º**ï¼šæ¨¡å‹éœ€è¦è¾“å‡ºç»“æ„åŒ–çš„è¾¹ç•Œæ¡†åæ ‡
- **æ™ºèƒ½åˆ†è¾¨ç‡**ï¼šä¿æŒç©ºé—´ä¿¡æ¯ç²¾åº¦çš„åŒæ—¶æ§åˆ¶è®¡ç®—æˆæœ¬
- **ç»Ÿä¸€promptæ ¼å¼**ï¼šæ‰€æœ‰groundingä»»åŠ¡ä½¿ç”¨ä¸€è‡´çš„æç¤ºæ ¼å¼

### 2.3 Spatial Understanding Cookbookï¼ˆæ¥è‡ªspatial_understanding.ipynbï¼‰

```python
# Qwen3-VL/cookbooks/spatial_understanding.ipynb

# ä»»åŠ¡1: ç©ºé—´å…³ç³»ç†è§£
prompt_1 = """
Which object, in relation to your current position, 
holds the farthest placement in the image?
Answer options:
A.chair B.plant C.window D.tv stand.
"""

# ä»»åŠ¡2: å¯æ“ä½œæ€§æ„ŸçŸ¥ï¼ˆAffordanceï¼‰
prompt_2 = """
Locate the free space on the white table on the right in this image. 
Output the point coordinates in JSON format.
"""
# è¾“å‡ºæ ¼å¼ï¼š{"point_2d": [x, y], "label": "object name/description"}

# ä»»åŠ¡3: åŠ¨ä½œè§„åˆ’
prompt_3 = """
What color arrow should the robot follow to move the apple 
in between the green can and the orange? 
Choices: A. Red. B. Blue. C. Green. D. Orange.
"""

# ä»»åŠ¡4: è§†é¢‘å¯¼èˆªï¼ˆå…·èº«AIï¼‰
prompt_4 = """
You are a robot beginning at the bed facing the tv. 
You want to navigate to the toilet. 
You will perform the following actions:
1. Go forward until the TV 
2. [please fill in: turn back/turn left/turn right]
3. Go forward until the shower 
4. [please fill in]
5. Go forward until the toilet.
"""
```

**è¿™äº›ç¤ºä¾‹ç›´æ¥å¯¹åº”è¯„ä¼°æ•°æ®é›†**ï¼š
- Prompt 1 â†’ RefSpatialBenchï¼ˆç›¸å¯¹ä½ç½®æ¨ç†ï¼‰
- Prompt 2 â†’ EmbSpatialBenchï¼ˆå¯æ“ä½œæ€§ç†è§£ï¼‰
- Prompt 3 â†’ EmbSpatialBenchï¼ˆåŠ¨ä½œè§„åˆ’ï¼‰
- Prompt 4 â†’ RoboSpatialHomeï¼ˆå¯¼èˆªè§„åˆ’ï¼‰

---

## ä¸‰ã€é’ˆå¯¹5ä¸ªæ ¸å¿ƒæ•°æ®é›†çš„æå‡æ–¹æ¡ˆ

### 3.1 EmbSpatialBenchæå‡æ–¹æ¡ˆï¼ˆå½“å‰84.3% â†’ ç›®æ ‡90%+ï¼‰

**ç“¶é¢ˆåˆ†æ**ï¼š
- éœ€è¦ç†è§£ç‰©ä½“å…³ç³»ã€å¯æ“ä½œæ€§ã€åŠ¨ä½œè§„åˆ’çš„ç»¼åˆèƒ½åŠ›
- å½“å‰æ€§èƒ½å·²ç»å¾ˆé«˜ï¼Œæå‡ç©ºé—´åœ¨äºè¾¹ç¼˜æ¡ˆä¾‹

**æå‡æ–¹æ¡ˆ**ï¼š

#### **æ–¹æ¡ˆAï¼šå¢å¼ºå…³ç³»æ ‡æ³¨å¯†åº¦**

```python
class EnhancedRelationalAnnotation:
    """å¢å¼ºå…³ç³»æ ‡æ³¨ç³»ç»Ÿ"""
    
    def generate_multi_level_relations(self, scene):
        """
        ç”Ÿæˆå¤šå±‚æ¬¡å…³ç³»æ ‡æ³¨
        """
        annotations = []
        
        # Level 1: ä¸€é˜¶å…³ç³»ï¼ˆç›´æ¥ç›¸é‚»ï¼‰
        for obj1, obj2 in scene.adjacent_pairs:
            annotations.extend([
                f"{obj1} is immediately to the left of {obj2}",
                f"{obj1} is touching the left edge of {obj2}",
                f"{obj1} is within arm's reach of {obj2}"
            ])
        
        # Level 2: äºŒé˜¶å…³ç³»ï¼ˆé—´æ¥å…³ç³»ï¼‰
        for obj1, obj2, obj3 in scene.triplets:
            annotations.extend([
                f"{obj1} is between {obj2} and {obj3}",
                f"{obj1} is closer to {obj2} than to {obj3}",
                f"to reach {obj3} from {obj2}, you must pass {obj1}"
            ])
        
        # Level 3: åŠŸèƒ½å…³ç³»ï¼ˆå¯æ“ä½œæ€§ + ç©ºé—´ï¼‰
        for obj in scene.objects:
            reachable_from = scene.get_reachable_positions(obj)
            annotations.extend([
                f"{obj} is within reachable distance from {pos}" 
                for pos in reachable_from
            ])
            
            blocking_relations = scene.get_blocking_relations(obj)
            annotations.extend([
                f"{obj} needs to be moved before accessing {blocked_obj}"
                for blocked_obj in blocking_relations
            ])
        
        return annotations
```

#### **æ–¹æ¡ˆBï¼šåˆ©ç”¨åœ°å›¾æ•°æ®æ„å»ºé“è·¯åœºæ™¯Embodiedä»»åŠ¡**

```python
class RoadEmbodiedDataGenerator:
    """
    åˆ©ç”¨åœ°å›¾å…¬å¸ä¼˜åŠ¿æ„å»ºé“è·¯åœºæ™¯å…·èº«AIæ•°æ®
    """
    
    def __init__(self, hd_map_db, street_view_db, poi_db):
        self.hd_map = hd_map_db
        self.street_view = street_view_db
        self.poi = poi_db
    
    def generate_navigation_task(self):
        """
        ç”Ÿæˆå¯¼èˆªä»»åŠ¡ï¼ˆå¯¹åº”RoboSpatialHomeçš„å®¤å†…å¯¼èˆªï¼‰
        """
        # 1. é‡‡æ ·èµ·ç»ˆç‚¹
        start_gps = self.sample_location()
        end_gps = self.sample_location(distance_from=start_gps, min_dist=500, max_dist=2000)
        
        # 2. è§„åˆ’è·¯å¾„
        route = self.hd_map.plan_route(start_gps, end_gps)
        decision_points = self.identify_decision_points(route)
        
        # 3. ç”Ÿæˆé—®é¢˜
        question = f"""
        You are at {start_gps.address} facing {start_gps.heading}. 
        You want to navigate to {end_gps.poi_name}.
        You will perform the following actions:
        """
        
        for i, point in enumerate(decision_points):
            question += f"\n{i+1}. Go forward until {point.landmark}"
            if i < len(decision_points) - 1:
                question += f"\n{i+2}. [please fill in: turn left/turn right/go straight]"
        
        # 4. Ground truth
        answer = [point.action for point in decision_points[:-1]]
        
        # 5. åŒ¹é…è¡—æ™¯è§†é¢‘
        video_frames = self.street_view.get_trajectory_video(route)
        
        return {
            "question": question,
            "answer": answer,
            "video": video_frames,
            "metadata": {
                "route_length": route.length,
                "num_turns": len(decision_points),
                "complexity": self.compute_complexity(route)
            }
        }
    
    def generate_affordance_task(self):
        """
        ç”Ÿæˆå¯æ“ä½œæ€§ä»»åŠ¡ï¼ˆå¯¹åº”EmbSpatialBenchçš„affordanceï¼‰
        """
        # é“è·¯åœºæ™¯çš„å¯æ“ä½œæ€§ç¤ºä¾‹
        scene_image = self.street_view.sample_image()
        
        affordance_queries = [
            {
                "question": "Locate a safe parking spot on the right side of the road. Output coordinates in JSON format.",
                "affordance_type": "parkable",
                "ground_truth": self.hd_map.get_parking_zones(scene_image.gps)
            },
            {
                "question": "Identify the crosswalk where pedestrians can safely cross. Output bbox coordinates.",
                "affordance_type": "crossable",
                "ground_truth": self.hd_map.get_crosswalks(scene_image.gps)
            },
            {
                "question": "Find the lane that allows left turns at the upcoming intersection.",
                "affordance_type": "turnable",
                "ground_truth": self.hd_map.get_turn_lanes(scene_image.gps)
            }
        ]
        
        return affordance_queries
    
    def generate_action_planning_task(self):
        """
        ç”ŸæˆåŠ¨ä½œè§„åˆ’ä»»åŠ¡
        """
        # å¤æ‚è·¯å£åœºæ™¯
        intersection_image = self.street_view.get_intersection_view()
        
        question = f"""
        You are approaching the intersection shown in the image.
        Your destination is the shopping mall on the northeast corner.
        There are three possible routes marked in red, blue, and green arrows.
        Which arrow should you follow to:
        1. Minimize the number of turns
        2. Avoid the construction zone (marked in orange)
        3. Reach the destination fastest
        Choices: A. Red B. Blue C. Green
        """
        
        # Ground truthé€šè¿‡è·¯å¾„è§„åˆ’ç®—æ³•è®¡ç®—
        optimal_route = self.hd_map.compute_optimal_route(
            intersection_image.gps,
            destination="shopping mall",
            constraints=["avoid_construction", "minimize_turns"]
        )
        
        return {
            "question": question,
            "answer": optimal_route.color,
            "image": intersection_image,
            "reasoning": optimal_route.explanation
        }
```

**é¢„æœŸæ•ˆæœ**ï¼š
- é“è·¯åœºæ™¯æ¯”å®¤å†…åœºæ™¯**æ›´å¤æ‚**ï¼ˆè·¯ç½‘æ‹“æ‰‘ã€äº¤é€šè§„åˆ™ã€åŠ¨æ€éšœç¢ç‰©ï¼‰
- åœ¨é“è·¯åœºæ™¯ä¸Šè®­ç»ƒåï¼Œå®¤å†…åœºæ™¯æ€§èƒ½ä¼šè¿›ä¸€æ­¥æå‡ï¼ˆè¿ç§»å­¦ä¹ ï¼‰
- **æ•°æ®è§„æ¨¡ä¼˜åŠ¿**ï¼šåœ°å›¾å…¬å¸å¯ç”Ÿæˆç™¾ä¸‡çº§é“è·¯åœºæ™¯æ•°æ®

---

### 3.2 RefSpatialBenchæå‡æ–¹æ¡ˆï¼ˆå½“å‰69.9% â†’ ç›®æ ‡80%+ï¼‰

**ç“¶é¢ˆåˆ†æ**ï¼š
- ç›¸å¯¹ä½ç½®æ¨ç†å’Œå‚è€ƒè¡¨è¾¾ç†è§£æ˜¯æ ¸å¿ƒ
- 69.9%è¯´æ˜è¿˜æœ‰è¾ƒå¤§æå‡ç©ºé—´

**æå‡æ–¹æ¡ˆ**ï¼š

#### **æ–¹æ¡ˆAï¼šå¯¹æ¯”å­¦ä¹ å¢å¼ºç©ºé—´å…³ç³»**

```python
class SpatialContrastiveLearning:
    """
    å¯¹æ¯”å­¦ä¹ å¢å¼ºç©ºé—´å…³ç³»ç†è§£
    """
    
    def __init__(self, temperature=0.07):
        self.temperature = temperature
    
    def generate_contrastive_pairs(self, scene):
        """
        ç”Ÿæˆå¯¹æ¯”å­¦ä¹ æ ·æœ¬å¯¹
        """
        positive_pairs = []
        negative_pairs = []
        
        for obj1, obj2 in scene.object_pairs:
            # Anchor: æ­£ç¡®çš„ç©ºé—´æè¿°
            anchor = {
                "image": scene.image,
                "text": f"the {obj1} to the left of the {obj2}"
            }
            
            # Positive: åŒä¹‰è¡¨è¾¾
            positive = {
                "image": scene.image,
                "text": f"the {obj1} on the left side of the {obj2}"
            }
            
            # Hard Negative: é”™è¯¯çš„ç©ºé—´å…³ç³»
            hard_negative = {
                "image": scene.image,
                "text": f"the {obj1} to the right of the {obj2}"  # æ–¹å‘ç›¸å
            }
            
            # Easy Negative: ä¸ç›¸å…³çš„ç‰©ä½“
            easy_negative = {
                "image": scene.image,
                "text": f"the {obj3} to the left of the {obj4}"  # ä¸åŒç‰©ä½“å¯¹
            }
            
            positive_pairs.append((anchor, positive))
            negative_pairs.append((anchor, hard_negative))
            negative_pairs.append((anchor, easy_negative))
        
        return positive_pairs, negative_pairs
    
    def contrastive_loss(self, anchor_emb, positive_emb, negative_embs):
        """
        InfoNCEæŸå¤±
        """
        # è®¡ç®—ç›¸ä¼¼åº¦
        pos_sim = F.cosine_similarity(anchor_emb, positive_emb)
        neg_sims = [F.cosine_similarity(anchor_emb, neg_emb) for neg_emb in negative_embs]
        
        # å¯¹æ¯”æŸå¤±
        numerator = torch.exp(pos_sim / self.temperature)
        denominator = numerator + sum([torch.exp(neg_sim / self.temperature) for neg_sim in neg_sims])
        
        loss = -torch.log(numerator / denominator)
        return loss
```

#### **æ–¹æ¡ˆBï¼šåˆ©ç”¨HDåœ°å›¾æ„å»ºç²¾ç¡®ç©ºé—´å…³ç³»æ•°æ®**

```python
class HDMapSpatialRelationGenerator:
    """
    åˆ©ç”¨HDåœ°å›¾çš„å˜ç±³çº§ç²¾åº¦æ„å»ºç©ºé—´å…³ç³»æ•°æ®
    """
    
    def generate_precise_spatial_relations(self, street_view_image, hd_map_data):
        """
        ç”Ÿæˆç²¾ç¡®çš„ç©ºé—´å…³ç³»æ ‡æ³¨
        """
        # HDåœ°å›¾æä¾›çš„ç²¾ç¡®ä¿¡æ¯
        road_elements = hd_map_data.get_elements_in_view(street_view_image.gps)
        
        spatial_relations = []
        
        for elem1, elem2 in combinations(road_elements, 2):
            # è®¡ç®—ç²¾ç¡®çš„ç©ºé—´å…³ç³»
            distance = hd_map_data.compute_distance(elem1, elem2)  # å˜ç±³çº§ç²¾åº¦
            direction = hd_map_data.compute_direction(elem1, elem2)  # ç²¾ç¡®è§’åº¦
            
            # ç”Ÿæˆå¤šç§è¡¨è¾¾æ–¹å¼
            relations = [
                # è·ç¦»å…³ç³»
                f"the {elem1.type} is {distance:.1f} meters from the {elem2.type}",
                
                # æ–¹å‘å…³ç³»
                f"the {elem1.type} is {direction.cardinal} of the {elem2.type}",
                f"the {elem1.type} is at {direction.angle}Â° relative to the {elem2.type}",
                
                # ç›¸å¯¹ä½ç½®
                f"the {elem1.type} is on the {direction.side} side of the {elem2.type}",
                
                # æ‹“æ‰‘å…³ç³»
                f"the {elem1.type} is {self.get_topology_relation(elem1, elem2)} the {elem2.type}"
            ]
            
            spatial_relations.extend(relations)
        
        # ç”ŸæˆRefSpatialBenché£æ ¼çš„é—®é¢˜
        questions = []
        for relation in spatial_relations:
            questions.append({
                "image": street_view_image,
                "query": f"Describe the position of {elem1.type} relative to {elem2.type}",
                "answer": relation,
                "ground_truth_distance": distance,
                "ground_truth_direction": direction
            })
        
        return questions
```

**ä¼˜åŠ¿**ï¼š
- **ç²¾åº¦ä¼˜åŠ¿**ï¼šHDåœ°å›¾æä¾›å˜ç±³çº§ç²¾åº¦ï¼Œè¿œè¶…3Dé‡å»ºæ ‡æ³¨
- **è§„æ¨¡ä¼˜åŠ¿**ï¼šå¯è‡ªåŠ¨ç”Ÿæˆæµ·é‡æ•°æ®
- **å¤šæ ·æ€§**ï¼šé“è·¯åœºæ™¯çš„ç©ºé—´å…³ç³»æ¯”å®¤å†…æ›´å¤æ‚å¤šæ ·

---

### 3.3 RoboSpatialHomeæå‡æ–¹æ¡ˆï¼ˆå½“å‰73.9% â†’ ç›®æ ‡85%+ï¼‰

**ç“¶é¢ˆåˆ†æ**ï¼š
- å®¶åº­åœºæ™¯å¯¼èˆªå’Œç‰©ä½“äº¤äº’
- éœ€è¦å…¨å±€ç©ºé—´ç†è§£å’Œè·¯å¾„è§„åˆ’èƒ½åŠ›

**æå‡æ–¹æ¡ˆ**ï¼š

#### **æ–¹æ¡ˆAï¼šè®¤çŸ¥åœ°å›¾æ˜¾å¼ç”Ÿæˆ**

```python
class CognitiveMapGenerator:
    """
    è®¤çŸ¥åœ°å›¾ç”Ÿæˆå™¨
    æ ¹æ®VSI-Benchè®ºæ–‡ï¼šæ˜¾å¼è®¤çŸ¥åœ°å›¾å¯æå‡10%æ€§èƒ½
    """
    
    def __init__(self, grid_size=10):
        self.grid_size = grid_size
    
    def generate_cognitive_map_from_video(self, video_frames, annotations):
        """
        ä»è§†é¢‘å¸§ç”Ÿæˆ10x10è®¤çŸ¥åœ°å›¾
        """
        # æ­¥éª¤1: æå–æ¯å¸§çš„ç‰©ä½“ä½ç½®
        object_trajectories = defaultdict(list)
        
        for frame_idx, frame in enumerate(video_frames):
            objects = self.detect_objects(frame)
            for obj in objects:
                object_trajectories[obj.id].append({
                    "frame": frame_idx,
                    "position_2d": obj.bbox_center,
                    "depth": obj.estimated_depth
                })
        
        # æ­¥éª¤2: 3Dä½ç½®ä¼°è®¡
        object_3d_positions = {}
        for obj_id, trajectory in object_trajectories.items():
            # ä½¿ç”¨SLAMæˆ–SfMä¼°è®¡3Dä½ç½®
            position_3d = self.estimate_3d_position(trajectory)
            object_3d_positions[obj_id] = position_3d
        
        # æ­¥éª¤3: æŠ•å½±åˆ°10x10ç½‘æ ¼
        cognitive_map = np.zeros((self.grid_size, self.grid_size), dtype=object)
        
        # è®¡ç®—åœºæ™¯è¾¹ç•Œ
        all_positions = list(object_3d_positions.values())
        min_x, max_x = min(p.x for p in all_positions), max(p.x for p in all_positions)
        min_z, max_z = min(p.z for p in all_positions), max(p.z for p in all_positions)
        
        # å½’ä¸€åŒ–åˆ°ç½‘æ ¼
        for obj_id, pos_3d in object_3d_positions.items():
            grid_x = int((pos_3d.x - min_x) / (max_x - min_x) * (self.grid_size - 1))
            grid_z = int((pos_3d.z - min_z) / (max_z - min_z) * (self.grid_size - 1))
            
            if cognitive_map[grid_z, grid_x] is None:
                cognitive_map[grid_z, grid_x] = []
            cognitive_map[grid_z, grid_x].append(obj_id)
        
        return cognitive_map, object_3d_positions
    
    def answer_with_cognitive_map(self, question, cognitive_map, object_positions):
        """
        ä½¿ç”¨è®¤çŸ¥åœ°å›¾å›ç­”ç©ºé—´é—®é¢˜
        """
        if "navigate" in question.lower():
            # å¯¼èˆªä»»åŠ¡
            start_obj = self.extract_start_object(question)
            end_obj = self.extract_end_object(question)
            
            start_grid = self.find_object_in_map(start_obj, cognitive_map)
            end_grid = self.find_object_in_map(end_obj, cognitive_map)
            
            # A*è·¯å¾„è§„åˆ’
            path = self.a_star_search(start_grid, end_grid, cognitive_map)
            
            # è½¬æ¢ä¸ºå¯¼èˆªæŒ‡ä»¤
            instructions = self.path_to_instructions(path, cognitive_map)
            return instructions
        
        elif "distance" in question.lower():
            # è·ç¦»æŸ¥è¯¢
            obj1 = self.extract_object(question, index=0)
            obj2 = self.extract_object(question, index=1)
            
            pos1 = object_positions[obj1]
            pos2 = object_positions[obj2]
            
            distance = np.linalg.norm([pos1.x - pos2.x, pos1.z - pos2.z])
            return f"{distance:.2f} meters"
        
        elif "direction" in question.lower():
            # æ–¹å‘æŸ¥è¯¢
            obj1 = self.extract_object(question, index=0)
            obj2 = self.extract_object(question, index=1)
            
            grid1 = self.find_object_in_map(obj1, cognitive_map)
            grid2 = self.find_object_in_map(obj2, cognitive_map)
            
            direction = self.compute_direction(grid1, grid2)
            return direction
```

#### **æ–¹æ¡ˆBï¼šåˆ©ç”¨åœ°å›¾æ•°æ®æ„å»ºå®¤å†…å¯¼èˆªæ•°æ®**

```python
class IndoorNavigationDataGenerator:
    """
    åˆ©ç”¨åœ°å›¾å…¬å¸çš„å®¤å†…åœ°å›¾æ•°æ®æ„å»ºå¯¼èˆªä»»åŠ¡
    """
    
    def __init__(self, indoor_map_db, indoor_imagery_db):
        self.indoor_map = indoor_map_db  # å•†åœºã€æœºåœºã€åœ°é“ç«™çš„å®¤å†…åœ°å›¾
        self.indoor_imagery = indoor_imagery_db  # å®¤å†…è¡—æ™¯
    
    def generate_mall_navigation_task(self):
        """
        ç”Ÿæˆå•†åœºå¯¼èˆªä»»åŠ¡ï¼ˆç±»ä¼¼RoboSpatialHomeçš„å®¶åº­å¯¼èˆªï¼‰
        """
        # 1. é€‰æ‹©å•†åœºå’Œæ¥¼å±‚
        mall = self.indoor_map.sample_mall()
        floor = mall.sample_floor()
        
        # 2. é‡‡æ ·èµ·ç»ˆç‚¹
        start_shop = floor.sample_shop()
        end_shop = floor.sample_shop(distance_from=start_shop, min_dist=50)
        
        # 3. è§„åˆ’è·¯å¾„
        route = floor.plan_route(start_shop, end_shop)
        waypoints = route.get_waypoints()
        
        # 4. ç”Ÿæˆé—®é¢˜
        question = f"""
        You are at {start_shop.name} in {mall.name}.
        You want to navigate to {end_shop.name}.
        The route passes through the following landmarks:
        """
        
        for i, waypoint in enumerate(waypoints):
            question += f"\n{i+1}. {waypoint.landmark}"
            if i < len(waypoints) - 1:
                question += f"\n   Then [turn left/turn right/go straight]?"
        
        # 5. Ground truth
        answer = [wp.action for wp in waypoints[:-1]]
        
        # 6. åŒ¹é…å®¤å†…è¡—æ™¯
        video_frames = self.indoor_imagery.get_route_video(route)
        
        return {
            "question": question,
            "answer": answer,
            "video": video_frames,
            "scene_type": "indoor_mall",
            "complexity": len(waypoints)
        }
    
    def generate_object_interaction_task(self):
        """
        ç”Ÿæˆç‰©ä½“äº¤äº’ä»»åŠ¡
        """
        scene = self.indoor_imagery.sample_scene()
        
        interaction_tasks = [
            {
                "question": "You want to buy a coffee. Which direction should you go?",
                "answer": self.indoor_map.find_nearest_poi(scene.location, "coffee_shop"),
                "interaction_type": "navigation_to_service"
            },
            {
                "question": "You need to find an ATM. Identify the ATM location in the image.",
                "answer": self.indoor_map.get_atm_locations(scene.location),
                "interaction_type": "service_localization"
            },
            {
                "question": "Which elevator should you take to reach the 3rd floor?",
                "answer": self.indoor_map.find_elevator(scene.location, target_floor=3),
                "interaction_type": "vertical_navigation"
            }
        ]
        
        return interaction_tasks
```

**ä¼˜åŠ¿**ï¼š
- **çœŸå®åœºæ™¯**ï¼šå•†åœºã€æœºåœºç­‰å®¤å†…åœºæ™¯ä¸å®¶åº­åœºæ™¯ç±»ä¼¼ä½†æ›´å¤æ‚
- **æ•°æ®è§„æ¨¡**ï¼šåœ°å›¾å…¬å¸æœ‰å¤§é‡å®¤å†…åœ°å›¾å’Œå®¤å†…è¡—æ™¯æ•°æ®
- **æ ‡æ³¨è´¨é‡**ï¼šå®¤å†…åœ°å›¾æä¾›ç²¾ç¡®çš„POIä½ç½®å’Œè·¯å¾„ä¿¡æ¯

---

### 3.4 VSI-Benchæå‡æ–¹æ¡ˆï¼ˆå½“å‰60.0% â†’ ç›®æ ‡75%+ï¼‰

**ç“¶é¢ˆåˆ†æ**ï¼š
- VSI-Benchæ˜¯è§†é¢‘ç©ºé—´ç†è§£ï¼Œéœ€è¦æ—¶åºè®°å¿†å’Œç©ºé—´æ¨ç†çš„ç»“åˆ
- 60%çš„æ€§èƒ½è¯´æ˜è¿™æ˜¯æœ€å¤§çš„ç“¶é¢ˆ

**æå‡æ–¹æ¡ˆ**ï¼š

#### **æ–¹æ¡ˆAï¼šæ—¶åºç©ºé—´èåˆæ¶æ„**

```python
class TemporalSpatialFusionModule(nn.Module):
    """
    æ—¶åº-ç©ºé—´èåˆæ¨¡å—
    ä¸“é—¨é’ˆå¯¹VSI-Benchçš„è§†é¢‘ç©ºé—´ç†è§£
    """
    
    def __init__(self, hidden_size=4096, num_frames=64):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_frames = num_frames
        
        # æ—¶åºç¼–ç å™¨
        self.temporal_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=hidden_size, nhead=32),
            num_layers=6
        )
        
        # ç©ºé—´ç¼–ç å™¨
        self.spatial_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=hidden_size, nhead=32),
            num_layers=6
        )
        
        # æ—¶ç©ºäº¤å‰æ³¨æ„åŠ›
        self.temporal_spatial_cross_attention = nn.MultiheadAttention(
            embed_dim=hidden_size,
            num_heads=32
        )
        
        # è®¤çŸ¥åœ°å›¾ç”Ÿæˆå™¨
        self.cognitive_map_generator = CognitiveMapHead(hidden_size, grid_size=10)
    
    def forward(self, video_features, spatial_queries):
        """
        video_features: [B, T, H, W, D]  (batch, time, height, width, dim)
        spatial_queries: [B, Q, D]  (batch, num_queries, dim)
        """
        B, T, H, W, D = video_features.shape
        
        # 1. æ—¶åºå»ºæ¨¡ï¼šè·¨å¸§èšåˆ
        temporal_features = video_features.view(B, T, H*W, D)
        temporal_features = temporal_features.mean(dim=2)  # [B, T, D]
        temporal_encoded = self.temporal_encoder(temporal_features)
        
        # 2. ç©ºé—´å»ºæ¨¡ï¼šæ¯å¸§å†…çš„ç©ºé—´å…³ç³»
        spatial_features = video_features.view(B*T, H*W, D)
        spatial_encoded = self.spatial_encoder(spatial_features)
        spatial_encoded = spatial_encoded.view(B, T, H*W, D)
        
        # 3. æ—¶ç©ºèåˆ
        # ä½¿ç”¨äº¤å‰æ³¨æ„åŠ›èåˆæ—¶åºå’Œç©ºé—´ä¿¡æ¯
        fused_features, attention_weights = self.temporal_spatial_cross_attention(
            query=spatial_queries,
            key=temporal_encoded,
            value=spatial_encoded.mean(dim=2)
        )
        
        # 4. ç”Ÿæˆè®¤çŸ¥åœ°å›¾
        cognitive_map = self.cognitive_map_generator(fused_features)
        
        return fused_features, cognitive_map, attention_weights


class CognitiveMapHead(nn.Module):
    """è®¤çŸ¥åœ°å›¾ç”Ÿæˆå¤´"""
    
    def __init__(self, hidden_size, grid_size=10):
        super().__init__()
        self.grid_size = grid_size
        self.map_projection = nn.Linear(hidden_size, grid_size * grid_size)
    
    def forward(self, features):
        """
        features: [B, Q, D]
        output: [B, grid_size, grid_size]
        """
        map_logits = self.map_projection(features.mean(dim=1))
        cognitive_map = map_logits.view(-1, self.grid_size, self.grid_size)
        return cognitive_map
```

#### **æ–¹æ¡ˆBï¼šåˆ©ç”¨è¡Œè½¦è®°å½•ä»ªæ•°æ®æ„å»ºè§†é¢‘ç©ºé—´æ•°æ®**

```python
class DashcamSpatialDataGenerator:
    """
    åˆ©ç”¨è¡Œè½¦è®°å½•ä»ªæ•°æ®æ„å»ºè§†é¢‘ç©ºé—´ç†è§£æ•°æ®
    """
    
    def __init__(self, dashcam_db, hd_map_db, trajectory_db):
        self.dashcam = dashcam_db  # è¡Œè½¦è®°å½•ä»ªè§†é¢‘
        self.hd_map = hd_map_db
        self.trajectory = trajectory_db
    
    def generate_vsi_bench_style_tasks(self):
        """
        ç”ŸæˆVSI-Benché£æ ¼çš„8é¡¹ä»»åŠ¡
        """
        video = self.dashcam.sample_video(duration=60)  # 60ç§’è§†é¢‘
        trajectory = self.trajectory.get_trajectory(video.id)
        
        tasks = []
        
        # ä»»åŠ¡1: é“è·¯å…ƒç´ è®¡æ•°
        tasks.append({
            "task": "object_count",
            "question": "How many traffic lights appear in this video?",
            "answer": self.count_elements_in_video(video, "traffic_light"),
            "ground_truth": self.hd_map.count_traffic_lights(trajectory)
        })
        
        # ä»»åŠ¡2: ç›¸å¯¹è·ç¦»
        tasks.append({
            "task": "relative_distance",
            "question": "Which landmark is closest to the final destination: A. Gas Station B. Shopping Mall C. Park D. School",
            "answer": self.compute_closest_landmark(trajectory.end, ["gas_station", "mall", "park", "school"]),
            "ground_truth": self.hd_map.get_distances(trajectory.end)
        })
        
        # ä»»åŠ¡3: ç›¸å¯¹æ–¹å‘
        tasks.append({
            "task": "relative_direction",
            "question": "At the intersection at timestamp 00:30, which direction is the hospital relative to your current heading?",
            "answer": self.compute_direction(trajectory.get_position_at(30), "hospital"),
            "ground_truth": self.hd_map.compute_bearing(trajectory.get_position_at(30), "hospital")
        })
        
        # ä»»åŠ¡4: è·¯å¾„è§„åˆ’
        tasks.append({
            "task": "route_planning",
            "question": "To reach the destination, you need to: 1. Go forward to XX intersection 2. [fill in] 3. Go forward to YY intersection 4. [fill in]",
            "answer": self.extract_turn_instructions(trajectory),
            "ground_truth": trajectory.turn_actions
        })
        
        # ä»»åŠ¡5: é“è·¯å®½åº¦ä¼°è®¡
        tasks.append({
            "task": "road_width",
            "question": "What is the width of the road at timestamp 00:45 (in meters)?",
            "answer": self.estimate_road_width(video, timestamp=45),
            "ground_truth": self.hd_map.get_road_width(trajectory.get_position_at(45))
        })
        
        # ä»»åŠ¡6: è½¦è¾†é—´è·
        tasks.append({
            "task": "vehicle_distance",
            "question": "What is the distance between your vehicle and the car in front at timestamp 00:20?",
            "answer": self.estimate_vehicle_distance(video, timestamp=20),
            "ground_truth": self.get_radar_distance(video, timestamp=20)
        })
        
        # ä»»åŠ¡7: ç»å¯¹è·ç¦»
        tasks.append({
            "task": "absolute_distance",
            "question": "What is the straight-line distance between the first traffic light and the last traffic light in the video?",
            "answer": self.compute_distance_between_elements(video, "traffic_light", first=True, last=True),
            "ground_truth": self.hd_map.compute_distance(trajectory)
        })
        
        # ä»»åŠ¡8: POIå‡ºç°é¡ºåº
        tasks.append({
            "task": "appearance_order",
            "question": "What is the order of appearance of these POIs: Starbucks, Bank of China, McDonald's, Subway Station?",
            "answer": self.extract_poi_order(video, ["Starbucks", "Bank of China", "McDonald's", "Subway"]),
            "ground_truth": self.hd_map.get_poi_order(trajectory)
        })
        
        return tasks
    
    def generate_training_data(self, num_videos=10000):
        """
        æ‰¹é‡ç”Ÿæˆè®­ç»ƒæ•°æ®
        """
        dataset = []
        
        for _ in tqdm(range(num_videos)):
            video = self.dashcam.sample_video(duration=random.randint(30, 120))
            tasks = self.generate_vsi_bench_style_tasks()
            
            dataset.append({
                "video": video,
                "tasks": tasks,
                "metadata": {
                    "duration": video.duration,
                    "num_frames": len(video.frames),
                    "trajectory_length": video.trajectory.length,
                    "scene_complexity": self.compute_complexity(video)
                }
            })
        
        return dataset
```

**ä¼˜åŠ¿**ï¼š
- **çœŸå®è§†é¢‘æ•°æ®**ï¼šè¡Œè½¦è®°å½•ä»ªæä¾›çœŸå®çš„åŠ¨æ€åœºæ™¯
- **ç²¾ç¡®æ ‡æ³¨**ï¼šHDåœ°å›¾ + GPSè½¨è¿¹æä¾›ground truth
- **è§„æ¨¡ä¼˜åŠ¿**ï¼šåœ°å›¾å…¬å¸æœ‰æµ·é‡è¡Œè½¦è®°å½•ä»ªæ•°æ®
- **å¤æ‚åº¦æ›´é«˜**ï¼šé“è·¯åœºæ™¯æ¯”å®¤å†…åœºæ™¯æ›´å¤æ‚ï¼Œè®­ç»ƒåè¿ç§»åˆ°å®¤å†…åœºæ™¯æ•ˆæœæ›´å¥½

#### **æ–¹æ¡ˆCï¼šè¯¾ç¨‹å­¦ä¹ ç­–ç•¥**

```python
class VSIBenchCurriculumTrainer:
    """
    é’ˆå¯¹VSI-Benchçš„è¯¾ç¨‹å­¦ä¹ è®­ç»ƒç­–ç•¥
    """
    
    def __init__(self):
        self.stages = [
            # Stage 1: é™æ€å›¾åƒç©ºé—´ç†è§£ï¼ˆç®€å•ï¼‰
            {
                "name": "static_spatial_understanding",
                "data": "single_frame_spatial_tasks",
                "duration": 5000,
                "difficulty": "easy",
                "tasks": ["object_count", "relative_position"]
            },
            
            # Stage 2: çŸ­è§†é¢‘æ—¶åºç†è§£ï¼ˆä¸­ç­‰ï¼‰
            {
                "name": "short_video_temporal",
                "data": "5_second_videos",
                "duration": 10000,
                "difficulty": "medium",
                "tasks": ["appearance_order", "simple_navigation"]
            },
            
            # Stage 3: é•¿è§†é¢‘ç©ºé—´è®°å¿†ï¼ˆå›°éš¾ï¼‰
            {
                "name": "long_video_spatial_memory",
                "data": "30_second_videos",
                "duration": 15000,
                "difficulty": "hard",
                "tasks": ["distance_estimation", "route_planning"]
            },
            
            # Stage 4: å¤æ‚è§†é¢‘ç©ºé—´æ¨ç†ï¼ˆéå¸¸å›°éš¾ï¼‰
            {
                "name": "complex_video_spatial_reasoning",
                "data": "60_second_videos",
                "duration": 20000,
                "difficulty": "very_hard",
                "tasks": ["all_8_tasks_combined"]
            }
        ]
    
    def train(self, model, optimizer):
        """
        è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ
        """
        for stage in self.stages:
            print(f"Training stage: {stage['name']}")
            
            # åŠ è½½è¯¥é˜¶æ®µçš„æ•°æ®
            dataloader = self.load_stage_data(stage)
            
            # è®­ç»ƒ
            for step in range(stage['duration']):
                batch = next(dataloader)
                
                # å‰å‘ä¼ æ’­
                outputs = model(batch['video'], batch['question'])
                
                # è®¡ç®—æŸå¤±
                loss = self.compute_loss(outputs, batch['answer'])
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                # è®°å½•
                if step % 100 == 0:
                    print(f"Step {step}/{stage['duration']}, Loss: {loss.item():.4f}")
            
            # é˜¶æ®µè¯„ä¼°
            eval_results = self.evaluate_stage(model, stage)
            print(f"Stage {stage['name']} evaluation: {eval_results}")
```

---

### 3.5 ERQAæå‡æ–¹æ¡ˆï¼ˆå½“å‰52.5% â†’ ç›®æ ‡65%+ï¼‰

**ç“¶é¢ˆåˆ†æ**ï¼š
- ERQAæ˜¯å¤šå›¾åƒå…·èº«æ¨ç†ï¼Œæœ€å…·æŒ‘æˆ˜æ€§
- 52.5%è¯´æ˜è¿™æ˜¯æœ€å¤§çš„æ€§èƒ½ç“¶é¢ˆ

**æå‡æ–¹æ¡ˆ**ï¼š

#### **æ–¹æ¡ˆAï¼šå¤šå›¾åƒç©ºé—´å…³ç³»å›¾æ„å»º**

```python
class MultiImageSpatialGraphBuilder:
    """
    å¤šå›¾åƒç©ºé—´å…³ç³»å›¾æ„å»ºå™¨
    """
    
    def __init__(self):
        self.object_detector = ObjectDetector()
        self.feature_extractor = FeatureExtractor()
        self.graph_builder = GraphNeuralNetwork()
    
    def build_spatial_graph(self, images):
        """
        ä»å¤šå¼ å›¾ç‰‡æ„å»ºå…¨å±€ç©ºé—´å…³ç³»å›¾
        """
        # æ­¥éª¤1: æå–æ¯å¼ å›¾ç‰‡çš„åœºæ™¯å›¾
        scene_graphs = []
        for img in images:
            objects = self.object_detector(img)
            features = self.feature_extractor(img, objects)
            scene_graph = self.build_scene_graph(objects, features)
            scene_graphs.append(scene_graph)
        
        # æ­¥éª¤2: è·¨å›¾ç‰‡å¯¹é½ç‰©ä½“
        aligned_objects = self.cross_image_alignment(scene_graphs)
        
        # æ­¥éª¤3: æ„å»ºå…¨å±€ç©ºé—´å…³ç³»å›¾
        global_graph = nx.DiGraph()
        
        # æ·»åŠ èŠ‚ç‚¹ï¼ˆç‰©ä½“ï¼‰
        for obj in aligned_objects:
            global_graph.add_node(
                obj.id,
                category=obj.category,
                features=obj.features,
                image_ids=obj.image_ids,  # è¯¥ç‰©ä½“å‡ºç°åœ¨å“ªäº›å›¾ç‰‡ä¸­
                positions=obj.positions    # åœ¨å„å›¾ç‰‡ä¸­çš„ä½ç½®
            )
        
        # æ·»åŠ è¾¹ï¼ˆç©ºé—´å…³ç³»ï¼‰
        for obj1, obj2 in combinations(aligned_objects, 2):
            # è®¡ç®—ç©ºé—´å…³ç³»
            relations = self.compute_spatial_relations(obj1, obj2)
            
            for relation in relations:
                global_graph.add_edge(
                    obj1.id,
                    obj2.id,
                    relation_type=relation.type,  # "left_of", "above", "near", etc.
                    confidence=relation.confidence,
                    evidence_images=relation.evidence_images
                )
        
        return global_graph
    
    def cross_image_alignment(self, scene_graphs):
        """
        è·¨å›¾ç‰‡å¯¹é½ç‰©ä½“
        """
        aligned_objects = []
        object_clusters = []
        
        # ä½¿ç”¨ç‰¹å¾ç›¸ä¼¼åº¦èšç±»
        all_objects = [obj for sg in scene_graphs for obj in sg.objects]
        
        for obj in all_objects:
            # æŸ¥æ‰¾æ˜¯å¦å·²æœ‰åŒ¹é…çš„èšç±»
            matched_cluster = None
            for cluster in object_clusters:
                if self.is_same_object(obj, cluster):
                    matched_cluster = cluster
                    break
            
            if matched_cluster:
                matched_cluster.add_instance(obj)
            else:
                # åˆ›å»ºæ–°èšç±»
                new_cluster = ObjectCluster(obj)
                object_clusters.append(new_cluster)
        
        # åˆå¹¶èšç±»ä¸ºå¯¹é½çš„ç‰©ä½“
        for cluster in object_clusters:
            aligned_obj = cluster.merge()
            aligned_objects.append(aligned_obj)
        
        return aligned_objects
    
    def reason_on_graph(self, global_graph, question):
        """
        åœ¨å…¨å±€å›¾ä¸Šè¿›è¡Œæ¨ç†
        """
        # ä½¿ç”¨å›¾ç¥ç»ç½‘ç»œè¿›è¡Œæ¨ç†
        node_features = torch.tensor([
            global_graph.nodes[n]['features'] 
            for n in global_graph.nodes()
        ])
        
        edge_index = torch.tensor([
            [u, v] for u, v in global_graph.edges()
        ]).t()
        
        edge_attr = torch.tensor([
            self.encode_relation(global_graph.edges[u, v]['relation_type'])
            for u, v in global_graph.edges()
        ])
        
        # GNNæ¨ç†
        output = self.graph_builder(node_features, edge_index, edge_attr, question)
        
        return output
```

#### **æ–¹æ¡ˆBï¼šåˆ©ç”¨å¤šè§†è§’è¡—æ™¯æ•°æ®**

```python
class MultiViewStreetSceneGenerator:
    """
    åˆ©ç”¨å¤šè§†è§’è¡—æ™¯æ•°æ®ç”ŸæˆERQAé£æ ¼çš„ä»»åŠ¡
    """
    
    def __init__(self, street_view_db, hd_map_db):
        self.street_view = street_view_db
        self.hd_map = hd_map_db
    
    def generate_multi_image_task(self):
        """
        ç”Ÿæˆå¤šå›¾åƒå…·èº«æ¨ç†ä»»åŠ¡
        """
        # 1. é€‰æ‹©ä¸€ä¸ªè·¯å£æˆ–åŒºåŸŸ
        location = self.hd_map.sample_intersection()
        
        # 2. è·å–å¤šä¸ªè§†è§’çš„è¡—æ™¯å›¾ç‰‡
        views = [
            self.street_view.get_view(location, heading=0),    # åŒ—
            self.street_view.get_view(location, heading=90),   # ä¸œ
            self.street_view.get_view(location, heading=180),  # å—
            self.street_view.get_view(location, heading=270)   # è¥¿
        ]
        
        # 3. ç”Ÿæˆéœ€è¦è·¨å›¾ç‰‡æ¨ç†çš„é—®é¢˜
        questions = []
        
        # é—®é¢˜ç±»å‹1: è·¨è§†è§’ç‰©ä½“å®šä½
        questions.append({
            "type": "cross_view_localization",
            "images": views,
            "question": "The Starbucks visible in Image 1 (north view) is in which direction relative to the Bank visible in Image 3 (south view)?",
            "answer": self.compute_cross_view_relation(views[0], "Starbucks", views[2], "Bank"),
            "reasoning": "multi_view_spatial_reasoning"
        })
        
        # é—®é¢˜ç±»å‹2: å…¨å±€å¯¼èˆªè§„åˆ’
        questions.append({
            "type": "global_navigation",
            "images": views,
            "question": "You are at the center of these four views. To reach the shopping mall visible in Image 2, which direction should you go first?",
            "answer": self.plan_navigation(location, "shopping_mall", views),
            "reasoning": "multi_view_navigation"
        })
        
        # é—®é¢˜ç±»å‹3: é®æŒ¡æ¨ç†
        questions.append({
            "type": "occlusion_reasoning",
            "images": views,
            "question": "The building partially visible behind the tree in Image 1 is fully visible in which other image?",
            "answer": self.find_occluded_object(views),
            "reasoning": "cross_view_occlusion"
        })
        
        # é—®é¢˜ç±»å‹4: ç©ºé—´ä¸€è‡´æ€§éªŒè¯
        questions.append({
            "type": "spatial_consistency",
            "images": views,
            "question": "Based on all four views, estimate the distance between the traffic light in Image 1 and the bus stop in Image 4.",
            "answer": self.estimate_cross_view_distance(views[0], "traffic_light", views[3], "bus_stop"),
            "reasoning": "multi_view_distance_estimation"
        })
        
        return questions
    
    def generate_sequential_scene_task(self):
        """
        ç”Ÿæˆåºåˆ—åœºæ™¯ä»»åŠ¡ï¼ˆæ²¿è·¯å¾„çš„å¤šä¸ªåœºæ™¯ï¼‰
        """
        # 1. é‡‡æ ·ä¸€æ¡è·¯å¾„
        route = self.hd_map.sample_route(length=500)  # 500ç±³è·¯å¾„
        
        # 2. æ²¿è·¯å¾„é‡‡æ ·å¤šä¸ªè§‚å¯Ÿç‚¹
        observation_points = route.sample_points(num_points=5)
        
        # 3. è·å–æ¯ä¸ªè§‚å¯Ÿç‚¹çš„è¡—æ™¯
        images = [
            self.street_view.get_view(point, heading=route.get_heading(point))
            for point in observation_points
        ]
        
        # 4. ç”Ÿæˆéœ€è¦è·¨åœºæ™¯æ¨ç†çš„é—®é¢˜
        questions = []
        
        # é—®é¢˜ç±»å‹1: æ—¶åºç©ºé—´è®°å¿†
        questions.append({
            "type": "temporal_spatial_memory",
            "images": images,
            "question": "Which landmark appears first along the route: A. Gas Station B. School C. Park D. Hospital?",
            "answer": self.find_first_appearance(images, ["gas_station", "school", "park", "hospital"]),
            "reasoning": "sequential_appearance"
        })
        
        # é—®é¢˜ç±»å‹2: ç´¯ç§¯è·ç¦»ä¼°è®¡
        questions.append({
            "type": "cumulative_distance",
            "images": images,
            "question": "What is the approximate total distance traveled from Image 1 to Image 5?",
            "answer": route.length,
            "reasoning": "distance_accumulation"
        })
        
        # é—®é¢˜ç±»å‹3: æ–¹å‘å˜åŒ–è¿½è¸ª
        questions.append({
            "type": "heading_change",
            "images": images,
            "question": "How many times did you turn left along this route?",
            "answer": self.count_left_turns(route),
            "reasoning": "direction_tracking"
        })
        
        return questions
```

#### **æ–¹æ¡ˆCï¼šå›¾ç¥ç»ç½‘ç»œå¢å¼º**

```python
class SpatialReasoningGNN(nn.Module):
    """
    ç©ºé—´æ¨ç†å›¾ç¥ç»ç½‘ç»œï¼ˆç»­ï¼‰
    """
    
    def forward(self, node_features, edge_index, edge_attr, question_embedding):
        """
        node_features: [N, 2048] - ç‰©ä½“ç‰¹å¾
        edge_index: [2, E] - è¾¹ç´¢å¼•
        edge_attr: [E, 128] - è¾¹ç‰¹å¾ï¼ˆç©ºé—´å…³ç³»ï¼‰
        question_embedding: [1, D] - é—®é¢˜åµŒå…¥
        """
        # ç¼–ç èŠ‚ç‚¹å’Œè¾¹
        x = self.node_encoder(node_features)
        edge_attr = self.edge_encoder(edge_attr)
        
        # å¤šå±‚å›¾å·ç§¯
        for gnn_layer in self.gnn_layers:
            x = gnn_layer(x, edge_index, edge_attr)
            x = F.relu(x)
        
        # å…¨å±€æ± åŒ–
        graph_embedding = self.global_pool(x)
        
        # èåˆé—®é¢˜ä¿¡æ¯
        fused = graph_embedding + question_embedding
        
        # æ¨ç†
        output = self.reasoning_head(fused)
        
        return output
```

---

## å››ã€ç»¼åˆæå‡æ–¹æ¡ˆï¼šåˆ©ç”¨åœ°å›¾æ•°æ®çš„å®Œæ•´è®­ç»ƒæµç¨‹

### 4.1 æ•°æ®æ„å»ºå®Œæ•´æµç¨‹

```python
class MapCompanySpatialDataPipeline:
    """
    åœ°å›¾å…¬å¸ç©ºé—´æ•°æ®æ„å»ºå®Œæ•´æµç¨‹
    """
    
    def __init__(self):
        # æ•°æ®æº
        self.hd_map = HDMapDatabase()
        self.street_view = StreetViewDatabase()
        self.dashcam = DashcamDatabase()
        self.indoor_map = IndoorMapDatabase()
        self.trajectory = TrajectoryDatabase()
        self.poi = POIDatabase()
        
        # æ•°æ®ç”Ÿæˆå™¨
        self.generators = {
            "EmbSpatialBench": RoadEmbodiedDataGenerator(
                self.hd_map, self.street_view, self.poi
            ),
            "RefSpatialBench": HDMapSpatialRelationGenerator(
                self.hd_map, self.street_view
            ),
            "RoboSpatialHome": IndoorNavigationDataGenerator(
                self.indoor_map, self.street_view
            ),
            "VSI-Bench": DashcamSpatialDataGenerator(
                self.dashcam, self.hd_map, self.trajectory
            ),
            "ERQA": MultiViewStreetSceneGenerator(
                self.street_view, self.hd_map
            )
        }
    
    def generate_full_dataset(self, target_size=1_000_000):
        """
        ç”Ÿæˆ100ä¸‡æ¡ç©ºé—´ç†è§£è®­ç»ƒæ•°æ®
        """
        dataset = {
            "EmbSpatialBench_style": [],
            "RefSpatialBench_style": [],
            "RoboSpatialHome_style": [],
            "VSI-Bench_style": [],
            "ERQA_style": []
        }
        
        # æ•°æ®åˆ†é…æ¯”ä¾‹
        allocation = {
            "EmbSpatialBench_style": 0.25,  # 250K
            "RefSpatialBench_style": 0.25,  # 250K
            "RoboSpatialHome_style": 0.15,  # 150K
            "VSI-Bench_style": 0.25,        # 250K
            "ERQA_style": 0.10              # 100K
        }
        
        for benchmark, ratio in allocation.items():
            num_samples = int(target_size * ratio)
            generator = self.generators[benchmark.replace("_style", "")]
            
            print(f"Generating {num_samples} samples for {benchmark}...")
            
            for i in tqdm(range(num_samples)):
                sample = generator.generate_sample()
                dataset[benchmark].append(sample)
        
        return dataset
    
    def quality_control(self, dataset):
        """
        æ•°æ®è´¨é‡æ§åˆ¶
        """
        filtered_dataset = {}
        
        for benchmark, samples in dataset.items():
            print(f"Quality control for {benchmark}...")
            
            filtered_samples = []
            
            for sample in tqdm(samples):
                # 1. å¤šæ¨¡æ€å¿…è¦æ€§æ£€æŸ¥
                if not self.check_multimodal_necessity(sample):
                    continue
                
                # 2. ç©ºé—´ä¿¡æ¯éªŒè¯
                if not self.verify_spatial_consistency(sample):
                    continue
                
                # 3. æ ‡æ³¨è´¨é‡æ£€æŸ¥
                if not self.check_annotation_quality(sample):
                    continue
                
                # 4. éš¾åº¦è¯„ä¼°
                difficulty = self.estimate_difficulty(sample)
                sample['difficulty'] = difficulty
                
                filtered_samples.append(sample)
            
            filtered_dataset[benchmark] = filtered_samples
            print(f"Retained {len(filtered_samples)}/{len(samples)} samples")
        
        return filtered_dataset
    
    def check_multimodal_necessity(self, sample):
        """
        æ£€æŸ¥æ˜¯å¦çœŸçš„éœ€è¦è§†è§‰ä¿¡æ¯
        ï¼ˆå‚è€ƒQwen3-VLçš„Long-CoTè¿‡æ»¤ç­–ç•¥ï¼‰
        """
        # ä½¿ç”¨çº¯æ–‡æœ¬æ¨¡å‹æµ‹è¯•
        text_only_model = Qwen3_30B_NoThink()
        
        # ä»…ç”¨æ–‡æœ¬æç¤º
        text_only_answer = text_only_model.generate(sample['question'])
        
        # å¦‚æœçº¯æ–‡æœ¬æ¨¡å‹èƒ½æ­£ç¡®å›ç­”ï¼Œè¯´æ˜ä¸éœ€è¦è§†è§‰ä¿¡æ¯
        if text_only_answer == sample['answer']:
            return False
        
        return True
    
    def verify_spatial_consistency(self, sample):
        """
        éªŒè¯ç©ºé—´ä¿¡æ¯çš„ä¸€è‡´æ€§
        """
        if 'bbox_3d' in sample:
            # æ£€æŸ¥3Dè¾¹ç•Œæ¡†çš„ç‰©ç†åˆç†æ€§
            if not self.check_physical_plausibility(sample['bbox_3d']):
                return False
        
        if 'spatial_relations' in sample:
            # æ£€æŸ¥ç©ºé—´å…³ç³»çš„ä¸€è‡´æ€§
            if not self.check_relation_consistency(sample['spatial_relations']):
                return False
        
        if 'video' in sample:
            # æ£€æŸ¥è§†é¢‘ä¸­çš„æ—¶åºä¸€è‡´æ€§
            if not self.check_temporal_consistency(sample['video']):
                return False
        
        return True
```

### 4.2 è®­ç»ƒç­–ç•¥å®Œæ•´æµç¨‹

```python
class SpatialUnderstandingTrainer:
    """
    ç©ºé—´ç†è§£èƒ½åŠ›è®­ç»ƒå™¨
    """
    
    def __init__(self, base_model="Qwen3-VL-235B-A22B"):
        self.model = load_model(base_model)
        self.optimizer = AdamW(self.model.parameters(), lr=1e-5)
        
        # è®­ç»ƒé˜¶æ®µ
        self.training_stages = [
            # Stage 1: ç»§ç»­é¢„è®­ç»ƒï¼ˆContinued Pre-trainingï¼‰
            {
                "name": "continued_pretraining",
                "data": "map_spatial_data",
                "epochs": 1,
                "batch_size": 64,
                "learning_rate": 1e-5,
                "sequence_length": 32768,
                "focus": "åŸºç¡€ç©ºé—´ç†è§£èƒ½åŠ›"
            },
            
            # Stage 2: ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰
            {
                "name": "supervised_finetuning",
                "data": "high_quality_spatial_qa",
                "epochs": 3,
                "batch_size": 32,
                "learning_rate": 5e-6,
                "sequence_length": 32768,
                "focus": "æŒ‡ä»¤è·Ÿéšå’Œæ ¼å¼åŒ–è¾“å‡º"
            },
            
            # Stage 3: å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰
            {
                "name": "reinforcement_learning",
                "data": "spatial_reasoning_tasks",
                "episodes": 10000,
                "batch_size": 16,
                "learning_rate": 1e-6,
                "algorithm": "SAPO",
                "focus": "æ¨ç†èƒ½åŠ›å’Œå‡†ç¡®æ€§"
            }
        ]
    
    def train_stage_1_continued_pretraining(self, dataset):
        """
        Stage 1: ç»§ç»­é¢„è®­ç»ƒ
        """
        print("="*80)
        print("Stage 1: Continued Pre-training on Map Spatial Data")
        print("="*80)
        
        # æ•°æ®åŠ è½½
        dataloader = self.create_dataloader(
            dataset,
            batch_size=64,
            shuffle=True,
            sequence_length=32768
        )
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(1):
            total_loss = 0
            
            for batch_idx, batch in enumerate(tqdm(dataloader)):
                # å‰å‘ä¼ æ’­
                outputs = self.model(
                    input_ids=batch['input_ids'],
                    attention_mask=batch['attention_mask'],
                    image_grid_thw=batch['image_grid_thw'],
                    video_grid_thw=batch['video_grid_thw'],
                    labels=batch['labels']
                )
                
                loss = outputs.loss
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                self.optimizer.step()
                
                total_loss += loss.item()
                
                # æ—¥å¿—
                if batch_idx % 100 == 0:
                    avg_loss = total_loss / (batch_idx + 1)
                    print(f"Epoch {epoch}, Batch {batch_idx}, Loss: {avg_loss:.4f}")
        
        print("Stage 1 completed!")
    
    def train_stage_2_supervised_finetuning(self, dataset):
        """
        Stage 2: ç›‘ç£å¾®è°ƒ
        """
        print("="*80)
        print("Stage 2: Supervised Fine-tuning")
        print("="*80)
        
        # æ•°æ®è¿‡æ»¤ï¼ˆå‚è€ƒQwen3-VLçš„ä¸¤é˜¶æ®µè¿‡æ»¤ï¼‰
        filtered_dataset = self.apply_two_stage_filtering(dataset)
        
        # æ•°æ®åŠ è½½
        dataloader = self.create_dataloader(
            filtered_dataset,
            batch_size=32,
            shuffle=True,
            sequence_length=32768
        )
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(3):
            for batch_idx, batch in enumerate(tqdm(dataloader)):
                # å‰å‘ä¼ æ’­
                outputs = self.model(
                    input_ids=batch['input_ids'],
                    attention_mask=batch['attention_mask'],
                    image_grid_thw=batch['image_grid_thw'],
                    video_grid_thw=batch['video_grid_thw'],
                    labels=batch['labels']
                )
                
                loss = outputs.loss
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()
                
                # æ—¥å¿—
                if batch_idx % 50 == 0:
                    print(f"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.4f}")
        
        print("Stage 2 completed!")
    
    def train_stage_3_reinforcement_learning(self, dataset):
        """
        Stage 3: å¼ºåŒ–å­¦ä¹ 
        """
        print("="*80)
        print("Stage 3: Reinforcement Learning")
        print("="*80)
        
        # åˆå§‹åŒ–RLç¯å¢ƒ
        rl_env = SpatialReasoningEnvironment(dataset)
        
        # SAPOç®—æ³•
        sapo_trainer = SAPOTrainer(
            model=self.model,
            learning_rate=1e-6,
            temperature=0.7
        )
        
        # è®­ç»ƒå¾ªç¯
        for episode in range(10000):
            # é‡‡æ ·ä»»åŠ¡
            task = rl_env.sample_task()
            
            # ç”Ÿæˆå¤šä¸ªå€™é€‰å“åº”
            responses = self.model.generate(
                task['input'],
                num_return_sequences=16,
                do_sample=True,
                temperature=0.7
            )
            
            # è¯„ä¼°å“åº”
            rewards = []
            for response in responses:
                reward = rl_env.compute_reward(response, task['ground_truth'])
                rewards.append(reward)
            
            # SAPOæ›´æ–°
            loss = sapo_trainer.update(responses, rewards)
            
            # æ—¥å¿—
            if episode % 100 == 0:
                avg_reward = np.mean(rewards)
                print(f"Episode {episode}, Avg Reward: {avg_reward:.4f}, Loss: {loss:.4f}")
        
        print("Stage 3 completed!")
    
    def apply_two_stage_filtering(self, dataset):
        """
        ä¸¤é˜¶æ®µè¿‡æ»¤ï¼ˆå‚è€ƒQwen3-VLçš„SFTæ•°æ®è¿‡æ»¤ï¼‰
        """
        # Stage 1: Query Filtering
        query_filtered = []
        for sample in dataset:
            # 1. è¯†åˆ«ä¸å¯éªŒè¯çš„æŸ¥è¯¢
            if not self.is_verifiable(sample['question']):
                continue
            
            # 2. æ¾„æ¸…æ¨¡ç³ŠæŒ‡ä»¤
            if self.is_ambiguous(sample['question']):
                sample['question'] = self.clarify_question(sample['question'])
            
            # 3. è¿‡æ»¤ç¼ºä¹å®è´¨å†…å®¹çš„æŸ¥è¯¢
            if not self.has_substance(sample['question']):
                continue
            
            # 4. è¯„ä¼°å¤æ‚æ€§
            complexity = self.evaluate_complexity(sample)
            if complexity < 0.3:  # è¿‡æ»¤è¿‡äºç®€å•çš„æ ·æœ¬
                continue
            
            query_filtered.append(sample)
        
        # Stage 2: Response Filtering
        response_filtered = []
        for sample in query_filtered:
            # 1. åŸºäºè§„åˆ™çš„è¿‡æ»¤
            if self.has_repetition(sample['answer']):
                continue
            if not self.is_complete(sample['answer']):
                continue
            if not self.is_well_formatted(sample['answer']):
                continue
            
            # 2. åŸºäºæ¨¡å‹çš„è¿‡æ»¤ï¼ˆä½¿ç”¨å¥–åŠ±æ¨¡å‹ï¼‰
            reward_score = self.reward_model.score(
                sample['question'],
                sample['answer'],
                dimensions=['correctness', 'completeness', 'clarity', 'helpfulness']
            )
            
            if reward_score < 0.7:
                continue
            
            response_filtered.append(sample)
        
        print(f"Filtering: {len(dataset)} -> {len(query_filtered)} -> {len(response_filtered)}")
        return response_filtered
```

### 4.3 è¯„ä¼°ä¸è¿­ä»£æµç¨‹

```python
class SpatialUnderstandingEvaluator:
    """
    ç©ºé—´ç†è§£èƒ½åŠ›è¯„ä¼°å™¨
    """
    
    def __init__(self, model):
        self.model = model
        # è¯„ä¼°æ•°æ®é›†
        self.benchmarks = {
            "EmbSpatialBench": EmbSpatialBenchDataset(),
            "RefSpatialBench": RefSpatialBenchDataset(),
            "RoboSpatialHome": RoboSpatialHomeDataset(),
            "VSI-Bench": VSIBenchDataset(),
            "ERQA": ERQADataset()
        }
    
    def evaluate_all_benchmarks(self):
        """
        åœ¨æ‰€æœ‰åŸºå‡†ä¸Šè¯„ä¼°
        """
        results = {}
        
        for benchmark_name, dataset in self.benchmarks.items():
            print(f"\nEvaluating on {benchmark_name}...")
            
            accuracy = self.evaluate_benchmark(dataset)
            results[benchmark_name] = accuracy
            
            print(f"{benchmark_name}: {accuracy:.2f}%")
        
        # è®¡ç®—å¹³å‡åˆ†
        avg_score = np.mean(list(results.values()))
        results['average'] = avg_score
        
        print(f"\nAverage Score: {avg_score:.2f}%")
        
        return results
    
    def evaluate_benchmark(self, dataset):
        """
        è¯„ä¼°å•ä¸ªåŸºå‡†
        """
        correct = 0
        total = 0
        
        for sample in tqdm(dataset):
            # ç”Ÿæˆé¢„æµ‹
            prediction = self.model.generate(
                sample['input'],
                max_new_tokens=512
            )
            
            # è¯„ä¼°
            is_correct = self.check_answer(prediction, sample['ground_truth'])
            
            if is_correct:
                correct += 1
            total += 1
        
        accuracy = (correct / total) * 100
        return accuracy
    
    def error_analysis(self, dataset):
        """
        é”™è¯¯åˆ†æ
        """
        errors = {
            "spatial_reasoning": [],
            "visual_perception": [],
            "language_understanding": [],
            "temporal_processing": []
        }
        
        for sample in dataset:
            prediction = self.model.generate(sample['input'])
            
            if not self.check_answer(prediction, sample['ground_truth']):
                # åˆ†ç±»é”™è¯¯ç±»å‹
                error_type = self.classify_error(sample, prediction)
                errors[error_type].append({
                    "sample": sample,
                    "prediction": prediction,
                    "ground_truth": sample['ground_truth']
                })
        
        # ç»Ÿè®¡
        print("\nError Analysis:")
        for error_type, error_list in errors.items():
            percentage = (len(error_list) / len(dataset)) * 100
            print(f"{error_type}: {len(error_list)} ({percentage:.1f}%)")
        
        return errors
    
    def identify_improvement_areas(self, errors):
        """
        è¯†åˆ«éœ€è¦æ”¹è¿›çš„é¢†åŸŸ
        """
        improvement_areas = []
        
        # åˆ†æç©ºé—´æ¨ç†é”™è¯¯
        if len(errors['spatial_reasoning']) > len(errors['visual_perception']):
            improvement_areas.append({
                "area": "spatial_reasoning",
                "priority": "high",
                "suggestions": [
                    "å¢åŠ å…³ç³»æ ‡æ³¨å¯†åº¦",
                    "å¼•å…¥å¯¹æ¯”å­¦ä¹ ",
                    "å¢å¼ºå›¾ç¥ç»ç½‘ç»œ"
                ]
            })
        
        # åˆ†æè§†è§‰æ„ŸçŸ¥é”™è¯¯
        if len(errors['visual_perception']) > 0.1 * len(dataset):
            improvement_areas.append({
                "area": "visual_perception",
                "priority": "medium",
                "suggestions": [
                    "æé«˜å›¾åƒåˆ†è¾¨ç‡",
                    "å¢å¼ºç‰©ä½“æ£€æµ‹èƒ½åŠ›",
                    "æ”¹è¿›ç‰¹å¾æå–"
                ]
            })
        
        # åˆ†ææ—¶åºå¤„ç†é”™è¯¯
        if len(errors['temporal_processing']) > 0:
            improvement_areas.append({
                "area": "temporal_processing",
                "priority": "high",
                "suggestions": [
                    "å¢å¼ºæ—¶åºå»ºæ¨¡",
                    "å¼•å…¥è®¤çŸ¥åœ°å›¾",
                    "æ”¹è¿›è§†é¢‘ç†è§£"
                ]
            })
        
        return improvement_areas
```

---

## äº”ã€é¢„æœŸæ•ˆæœä¸èµ„æºéœ€æ±‚

### 5.1 é¢„æœŸæ€§èƒ½æå‡

| æ•°æ®é›† | å½“å‰æ€§èƒ½ | ç›®æ ‡æ€§èƒ½ | æå‡å¹…åº¦ | å…³é”®æ–¹æ³• |
|-------|---------|---------|---------|---------|
| **EmbSpatialBench** | 84.3% | 90%+ | +5.7% | é“è·¯åœºæ™¯å…·èº«æ•°æ® + å¤šå±‚æ¬¡å…³ç³»æ ‡æ³¨ |
| **RefSpatialBench** | 69.9% | 80%+ | +10.1% | HDåœ°å›¾ç²¾ç¡®æ ‡æ³¨ + å¯¹æ¯”å­¦ä¹  |
| **RoboSpatialHome** | 73.9% | 85%+ | +11.1% | è®¤çŸ¥åœ°å›¾ + å®¤å†…å¯¼èˆªæ•°æ® |
| **VSI-Bench** | 60.0% | 75%+ | +15.0% | è¡Œè½¦è®°å½•ä»ªæ•°æ® + æ—¶åºç©ºé—´èåˆ |
| **ERQA** | 52.5% | 65%+ | +12.5% | å¤šè§†è§’è¡—æ™¯ + å›¾ç¥ç»ç½‘ç»œ |
| **å¹³å‡** | 68.1% | 79.0% | +10.9% | ç»¼åˆæ–¹æ¡ˆ |

### 5.2 æ•°æ®è§„æ¨¡éœ€æ±‚

```python
data_requirements = {
    "è®­ç»ƒæ•°æ®æ€»é‡": "1,000,000 æ ·æœ¬",
    
    "æ•°æ®åˆ†é…": {
        "EmbSpatialBenché£æ ¼": {
            "æ•°é‡": "250,000",
            "æ¥æº": "é“è·¯åœºæ™¯ + è¡—æ™¯è§†é¢‘",
            "æ ‡æ³¨æˆæœ¬": "ä½ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰"
        },
        "RefSpatialBenché£æ ¼": {
            "æ•°é‡": "250,000",
            "æ¥æº": "HDåœ°å›¾ + è¡—æ™¯å›¾ç‰‡",
            "æ ‡æ³¨æˆæœ¬": "æä½ï¼ˆHDåœ°å›¾ç›´æ¥æä¾›ï¼‰"
        },
        "RoboSpatialHomeé£æ ¼": {
            "æ•°é‡": "150,000",
            "æ¥æº": "å®¤å†…åœ°å›¾ + å®¤å†…è¡—æ™¯",
            "æ ‡æ³¨æˆæœ¬": "ä¸­ï¼ˆéƒ¨åˆ†äººå·¥æ ‡æ³¨ï¼‰"
        },
        "VSI-Benché£æ ¼": {
            "æ•°é‡": "250,000",
            "æ¥æº": "è¡Œè½¦è®°å½•ä»ªè§†é¢‘",
            "æ ‡æ³¨æˆæœ¬": "ä½ï¼ˆGPSè½¨è¿¹æä¾›ground truthï¼‰"
        },
        "ERQAé£æ ¼": {
            "æ•°é‡": "100,000",
            "æ¥æº": "å¤šè§†è§’è¡—æ™¯",
            "æ ‡æ³¨æˆæœ¬": "ä¸­ï¼ˆéœ€è¦è·¨è§†è§’å¯¹é½ï¼‰"
        }
    },
    
    "æ•°æ®ä¼˜åŠ¿": {
        "ç²¾åº¦": "HDåœ°å›¾å˜ç±³çº§ç²¾åº¦ >> 3Dé‡å»ºç²¾åº¦",
        "è§„æ¨¡": "ç™¾ä¸‡çº§ >> ç°æœ‰æ•°æ®é›†ï¼ˆåƒçº§ï¼‰",
        "æˆæœ¬": "è‡ªåŠ¨ç”Ÿæˆä¸ºä¸»ï¼Œäººå·¥æ ‡æ³¨ä¸ºè¾…",
        "è´¨é‡": "çœŸå®åœºæ™¯ + ç²¾ç¡®æ ‡æ³¨"
    }
}
```

### 5.3 è®¡ç®—èµ„æºéœ€æ±‚

```python
compute_requirements = {
    "Stage 1: Continued Pre-training": {
        "GPU": "64x A100 80GB",
        "è®­ç»ƒæ—¶é—´": "7å¤©",
        "æ•°æ®é‡": "1Mæ ·æœ¬ Ã— 1 epoch",
        "åºåˆ—é•¿åº¦": "32K tokens",
        "æ‰¹æ¬¡å¤§å°": "64 (å…¨å±€) = 1 per GPU"
    },
    
    "Stage 2: Supervised Fine-tuning": {
        "GPU": "32x A100 80GB",
        "è®­ç»ƒæ—¶é—´": "5å¤©",
        "æ•°æ®é‡": "500Kæ ·æœ¬ Ã— 3 epochs",
        "åºåˆ—é•¿åº¦": "32K tokens",
        "æ‰¹æ¬¡å¤§å°": "32 (å…¨å±€)"
    },
    
    "Stage 3: Reinforcement Learning": {
        "GPU": "16x A100 80GB",
        "è®­ç»ƒæ—¶é—´": "3å¤©",
        "Episodes": "10,000",
        "æ¯episodeé‡‡æ ·": "16ä¸ªå“åº”",
        "æ‰¹æ¬¡å¤§å°": "16 (å…¨å±€)"
    },
    
    "æ€»è®¡": {
        "GPUæ—¶": "çº¦ 10,000 A100-hours",
        "è®­ç»ƒå‘¨æœŸ": "15å¤©",
        "æˆæœ¬ä¼°ç®—": "$50,000 - $100,000"
    }
}
```

### 5.4 å®æ–½æ—¶é—´çº¿

```python
implementation_timeline = {
    "Phase 1: æ•°æ®å‡†å¤‡ (4å‘¨)": {
        "Week 1-2": "æ•°æ®æ”¶é›†å’Œé¢„å¤„ç†",
        "Week 3": "æ•°æ®ç”Ÿæˆå’Œæ ‡æ³¨",
        "Week 4": "è´¨é‡æ§åˆ¶å’ŒéªŒè¯"
    },
    
    "Phase 2: æ¨¡å‹è®­ç»ƒ (3å‘¨)": {
        "Week 5-6": "Continued Pre-training + SFT",
        "Week 7": "Reinforcement Learning"
    },
    
    "Phase 3: è¯„ä¼°ä¸è¿­ä»£ (2å‘¨)": {
        "Week 8": "å…¨é¢è¯„ä¼°å’Œé”™è¯¯åˆ†æ",
        "Week 9": "é’ˆå¯¹æ€§æ”¹è¿›å’Œé‡æ–°è®­ç»ƒ"
    },
    
    "æ€»è®¡": "9å‘¨ï¼ˆçº¦2ä¸ªæœˆï¼‰"
}
```

---

## å…­ã€å…³é”®æŠ€æœ¯åˆ›æ–°ç‚¹æ€»ç»“

### 6.1 æ•°æ®ç«¯åˆ›æ–°

1. **é“è·¯åœºæ™¯Embodiedæ•°æ®**
   - åˆ©ç”¨HDåœ°å›¾ + è¡—æ™¯è§†é¢‘æ„å»ºé“è·¯å¯¼èˆªä»»åŠ¡
   - æ¯”å®¤å†…åœºæ™¯æ›´å¤æ‚ï¼Œè®­ç»ƒåè¿ç§»æ•ˆæœæ›´å¥½
   - æ•°æ®è§„æ¨¡ä¼˜åŠ¿ï¼šç™¾ä¸‡çº§ vs åƒçº§

2. **å˜ç±³çº§ç²¾åº¦ç©ºé—´æ ‡æ³¨**
   - HDåœ°å›¾æä¾›å˜ç±³çº§ç²¾åº¦ï¼Œè¿œè¶…3Dé‡å»º
   - è‡ªåŠ¨ç”Ÿæˆï¼Œæˆæœ¬æä½
   - å¤šæ ·æ€§é«˜ï¼šé“è·¯åœºæ™¯ç©ºé—´å…³ç³»æ›´å¤æ‚

3. **å¤šè§†è§’è¡—æ™¯æ•°æ®**
   - è§£å†³ERQAçš„å¤šå›¾åƒæ¨ç†ç“¶é¢ˆ
   - è‡ªç„¶çš„è·¨è§†è§’å¯¹é½
   - çœŸå®çš„é®æŒ¡å’Œè§†è§’å˜åŒ–

4. **è¡Œè½¦è®°å½•ä»ªè§†é¢‘æ•°æ®**
   - è§£å†³VSI-Benchçš„è§†é¢‘ç©ºé—´ç†è§£ç“¶é¢ˆ
   - GPSè½¨è¿¹æä¾›ç²¾ç¡®ground truth
   - åŠ¨æ€åœºæ™¯ï¼Œæ—¶åºç©ºé—´èåˆ

### 6.2 æ¶æ„ç«¯åˆ›æ–°

1. **3D RoPEå¢å¼º**
   - å·²æœ‰åŸºç¡€ï¼šQwen3-VLçš„Interleaved-MRoPE
   - å¢å¼ºæ–¹å‘ï¼šRoad Network RoPEï¼ˆæ‹“æ‰‘æ„ŸçŸ¥ï¼‰
   - æ•ˆæœï¼šæ›´å¥½çš„ç©ºé—´ä½ç½®ç¼–ç 

2. **è®¤çŸ¥åœ°å›¾æ˜¾å¼ç”Ÿæˆ**
   - æ ¹æ®VSI-Benchè®ºæ–‡ï¼š+10%æ€§èƒ½
   - 10Ã—10ç½‘æ ¼è¡¨ç¤ºå…¨å±€ç©ºé—´
   - æ”¯æŒè·ç¦»å’Œæ–¹å‘æŸ¥è¯¢

3. **å›¾ç¥ç»ç½‘ç»œé›†æˆ**
   - è§£å†³ERQAçš„å¤šå›¾åƒæ¨ç†
   - è·¨å›¾ç‰‡ç‰©ä½“å¯¹é½å’Œå…³ç³»å»ºæ¨¡
   - å…¨å±€ç©ºé—´å…³ç³»å›¾

4. **æ—¶åºç©ºé—´èåˆæ¨¡å—**
   - è§£å†³VSI-Benchçš„æ—¶åºç©ºé—´ç†è§£
   - æ—¶åºç¼–ç å™¨ + ç©ºé—´ç¼–ç å™¨ + äº¤å‰æ³¨æ„åŠ›
   - ç«¯åˆ°ç«¯è®­ç»ƒ

### 6.3 è®­ç»ƒç­–ç•¥åˆ›æ–°

1. **è¯¾ç¨‹å­¦ä¹ **
   - ä»ç®€å•åˆ°å¤æ‚ï¼šé™æ€â†’çŸ­è§†é¢‘â†’é•¿è§†é¢‘â†’å¤æ‚æ¨ç†
   - æ¯ä¸ªé˜¶æ®µé’ˆå¯¹æ€§è®­ç»ƒ
   - é€æ­¥æå‡éš¾åº¦

2. **å¯¹æ¯”å­¦ä¹ **
   - å¢å¼ºç©ºé—´å…³ç³»ç†è§£
   - Hard negative mining
   - InfoNCEæŸå¤±

3. **å¤šä»»åŠ¡å¼ºåŒ–å­¦ä¹ **
   - è·¨5ä¸ªæ•°æ®é›†çš„ç»Ÿä¸€RLæ¡†æ¶
   - æ··åˆå¥–åŠ±ç³»ç»Ÿï¼ˆè§„åˆ™+æ¨¡å‹ï¼‰
   - SAPOç®—æ³•

4. **ä¸¤é˜¶æ®µæ•°æ®è¿‡æ»¤**
   - Queryè¿‡æ»¤ + Responseè¿‡æ»¤
   - å¤šæ¨¡æ€å¿…è¦æ€§æ£€æŸ¥
   - å¥–åŠ±æ¨¡å‹è¯„åˆ†

---

## ä¸ƒã€é£é™©ä¸ç¼“è§£æªæ–½

### 7.1 æ½œåœ¨é£é™©

| é£é™©ç±»å‹ | å…·ä½“é£é™© | å½±å“ç¨‹åº¦ | ç¼“è§£æªæ–½ |
|---------|---------|---------|---------|
| **æ•°æ®è´¨é‡** | è‡ªåŠ¨ç”Ÿæˆæ•°æ®å¯èƒ½æœ‰å™ªå£° | ä¸­ | ä¸¤é˜¶æ®µè¿‡æ»¤ + äººå·¥æŠ½æ£€ |
| **åŸŸè¿ç§»** | é“è·¯åœºæ™¯â†’å®¤å†…åœºæ™¯è¿ç§»æ•ˆæœä¸ç¡®å®š | ä¸­ | æ··åˆè®­ç»ƒ + åŸŸé€‚åº” |
| **è®¡ç®—æˆæœ¬** | è®­ç»ƒæˆæœ¬å¯èƒ½è¶…é¢„ç®— | é«˜ | åˆ†é˜¶æ®µè®­ç»ƒ + æ¨¡å‹å‹ç¼© |
| **è¿‡æ‹Ÿåˆ** | åœ¨ç‰¹å®šåœºæ™¯è¿‡æ‹Ÿåˆ | ä¸­ | æ•°æ®å¢å¼º + æ­£åˆ™åŒ– |
| **è¯„ä¼°åå·®** | è®­ç»ƒæ•°æ®ä¸è¯„ä¼°æ•°æ®åˆ†å¸ƒä¸åŒ¹é… | ä½ | ä¿ç•™éªŒè¯é›† + äº¤å‰éªŒè¯ |

### 7.2 ç¼“è§£æªæ–½è¯¦ç»†è¯´æ˜

```python
risk_mitigation_strategies = {
    "æ•°æ®è´¨é‡é£é™©": {
        "æªæ–½1": "ä¸¤é˜¶æ®µè¿‡æ»¤ï¼ˆQuery + Responseï¼‰",
        "æªæ–½2": "äººå·¥æŠ½æ£€10%æ ·æœ¬",
        "æªæ–½3": "å¤šæ¨¡æ€å¿…è¦æ€§éªŒè¯",
        "æªæ–½4": "ç©ºé—´ä¸€è‡´æ€§æ£€æŸ¥"
    },
    
    "åŸŸè¿ç§»é£é™©": {
        "æªæ–½1": "æ··åˆè®­ç»ƒï¼ˆé“è·¯70% + å®¤å†…30%ï¼‰",
        "æªæ–½2": "åŸŸé€‚åº”æŠ€æœ¯ï¼ˆå¯¹æŠ—è®­ç»ƒï¼‰",
        "æªæ–½3": "æ¸è¿›å¼è¿ç§»ï¼ˆå…ˆé“è·¯åå®¤å†…ï¼‰",
        "æªæ–½4": "è¯„ä¼°å¤šä¸ªåŸŸçš„æ€§èƒ½"
    },
    
    "è®¡ç®—æˆæœ¬é£é™©": {
        "æªæ–½1": "åˆ†é˜¶æ®µè®­ç»ƒï¼ˆå¯ä¸­æ–­æ¢å¤ï¼‰",
        "æªæ–½2": "ä½¿ç”¨å°æ¨¡å‹éªŒè¯ï¼ˆQwen3-VL-8Bï¼‰",
        "æªæ–½3": "æ··åˆç²¾åº¦è®­ç»ƒï¼ˆFP16/BF16ï¼‰",
        "æªæ–½4": "æ¢¯åº¦ç´¯ç§¯å‡å°‘GPUéœ€æ±‚"
    },
    
    "è¿‡æ‹Ÿåˆé£é™©": {
        "æªæ–½1": "æ•°æ®å¢å¼ºï¼ˆæ—‹è½¬ã€è£å‰ªã€é¢œè‰²æŠ–åŠ¨ï¼‰",
        "æªæ–½2": "Dropoutå’Œæƒé‡è¡°å‡",
        "æªæ–½3": "Early stopping",
        "æªæ–½4": "éªŒè¯é›†ç›‘æ§"
    }
}
```

## å…«ã€æ€»ç»“ä¸å»ºè®®

### 8.1 æ ¸å¿ƒä¼˜åŠ¿

2. **åœºæ™¯ä¼˜åŠ¿**
   - é“è·¯åœºæ™¯æ¯”å®¤å†…åœºæ™¯æ›´å¤æ‚
   - è·¯ç½‘æ‹“æ‰‘çº¦æŸã€äº¤é€šè§„åˆ™ã€åŠ¨æ€éšœç¢ç‰©
   - è®­ç»ƒåè¿ç§»åˆ°å®¤å†…åœºæ™¯æ•ˆæœæ›´å¥½
   - ç›´æ¥èµ‹èƒ½å¯¼èˆªå’Œè‡ªåŠ¨é©¾é©¶ä¸šåŠ¡

3. **æŠ€æœ¯ä¼˜åŠ¿**
   - åŸºäºQwen3-VLçš„å…ˆè¿›æ¶æ„ï¼ˆ3D RoPEã€DeepStackï¼‰
   - ç»“åˆæœ€æ–°ç ”ç©¶æˆæœï¼ˆè®¤çŸ¥åœ°å›¾ã€å›¾ç¥ç»ç½‘ç»œï¼‰
   - å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼ˆé¢„è®­ç»ƒâ†’SFTâ†’RLï¼‰
   - ç³»ç»ŸåŒ–çš„è¯„ä¼°å’Œè¿­ä»£æœºåˆ¶

4. **å•†ä¸šä¼˜åŠ¿**
   - æ•°æ®é£è½®æ•ˆåº”ï¼šè¯„ä¼°â†’æ”¹è¿›â†’äº§å“æå‡â†’æ›´å¤šæ•°æ®
   - æŠ€æœ¯æŠ¤åŸæ²³ï¼šé«˜è´¨é‡æ ‡æ³¨ + å¤§è§„æ¨¡æ•°æ®
   - å¤šåœºæ™¯åº”ç”¨ï¼šå¯¼èˆªã€è‡ªåŠ¨é©¾é©¶ã€æœºå™¨äººã€AR/VR

### 8.2 å®æ–½å»ºè®®

#### **çŸ­æœŸå»ºè®®ï¼ˆ1-3ä¸ªæœˆï¼‰**

```python
short_term_plan = {
    "Month 1: æ•°æ®å‡†å¤‡å’ŒéªŒè¯": {
        "Week 1-2": {
            "ä»»åŠ¡": "æ•°æ®æ”¶é›†å’Œé¢„å¤„ç†",
            "äº§å‡º": [
                "æ”¶é›†10ä¸‡æ¡é“è·¯åœºæ™¯æ•°æ®",
                "æ”¶é›†5ä¸‡æ¡å®¤å†…åœºæ™¯æ•°æ®",
                "å»ºç«‹æ•°æ®å¤„ç†pipeline"
            ],
            "è´Ÿè´£äºº": "æ•°æ®å·¥ç¨‹å›¢é˜Ÿ"
        },
        "Week 3-4": {
            "ä»»åŠ¡": "å°è§„æ¨¡éªŒè¯å®éªŒ",
            "äº§å‡º": [
                "åœ¨Qwen3-VL-8Bä¸ŠéªŒè¯æ•°æ®è´¨é‡",
                "è¯„ä¼°æ•°æ®å¯¹æ€§èƒ½çš„å½±å“",
                "ç¡®å®šæœ€ç»ˆæ•°æ®é…æ¯”"
            ],
            "è´Ÿè´£äºº": "ç®—æ³•å›¢é˜Ÿ"
        }
    },
    
    "Month 2: æ¨¡å‹è®­ç»ƒ": {
        "Week 5-6": {
            "ä»»åŠ¡": "Continued Pre-training + SFT",
            "äº§å‡º": [
                "åœ¨100ä¸‡æ•°æ®ä¸Šè®­ç»ƒQwen3-VL-32B",
                "ä¸­é—´checkpointè¯„ä¼°",
                "è°ƒæ•´è¶…å‚æ•°"
            ],
            "è´Ÿè´£äºº": "è®­ç»ƒå›¢é˜Ÿ"
        },
        "Week 7-8": {
            "ä»»åŠ¡": "å¼ºåŒ–å­¦ä¹ å’Œä¼˜åŒ–",
            "äº§å‡º": [
                "RLè®­ç»ƒ10K episodes",
                "å¤šä¸ªcheckpointå¯¹æ¯”",
                "é€‰æ‹©æœ€ä½³æ¨¡å‹"
            ],
            "è´Ÿè´£äºº": "è®­ç»ƒå›¢é˜Ÿ"
        }
    },
    
    "Month 3: è¯„ä¼°å’Œè¿­ä»£": {
        "Week 9-10": {
            "ä»»åŠ¡": "å…¨é¢è¯„ä¼°",
            "äº§å‡º": [
                "5ä¸ªæ•°æ®é›†å®Œæ•´è¯„ä¼°",
                "é”™è¯¯åˆ†ææŠ¥å‘Š",
                "æ”¹è¿›æ–¹å‘è¯†åˆ«"
            ],
            "è´Ÿè´£äºº": "è¯„ä¼°å›¢é˜Ÿ"
        },
        "Week 11-12": {
            "ä»»åŠ¡": "é’ˆå¯¹æ€§æ”¹è¿›",
            "äº§å‡º": [
                "é’ˆå¯¹ç“¶é¢ˆé‡æ–°è®­ç»ƒ",
                "æœ€ç»ˆæ¨¡å‹å‘å¸ƒ",
                "æŠ€æœ¯æŠ¥å‘Šæ’°å†™"
            ],
            "è´Ÿè´£äºº": "å…¨å›¢é˜Ÿ"
        }
    }
}
```

#### **ä¸­æœŸå»ºè®®ï¼ˆ3-6ä¸ªæœˆï¼‰**

```python
mid_term_plan = {
    "æ•°æ®æ‰©å±•": {
        "ç›®æ ‡": "æ‰©å±•åˆ°500ä¸‡è®­ç»ƒæ ·æœ¬",
        "æ–¹æ³•": [
            "å¢åŠ æ›´å¤šåŸå¸‚çš„è¡—æ™¯æ•°æ®",
            "å¼•å…¥æ›´å¤šå®¤å†…åœºæ™¯ï¼ˆæœºåœºã€åœ°é“ç«™ã€å•†åœºï¼‰",
            "åˆæˆæ•°æ®ç”Ÿæˆï¼ˆBlender/Unityï¼‰",
            "ä¼—åŒ…æ ‡æ³¨è¡¥å……"
        ]
    },
    
    "æ¨¡å‹ä¼˜åŒ–": {
        "ç›®æ ‡": "åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šè¾¾åˆ°SOTA",
        "æ–¹æ³•": [
            "æ¶æ„æœç´¢ï¼ˆNASï¼‰",
            "è’¸é¦åˆ°å°æ¨¡å‹ï¼ˆQwen3-VL-8Bï¼‰",
            "é‡åŒ–å’ŒåŠ é€Ÿï¼ˆINT8/FP8ï¼‰",
            "å¤šæ¨¡æ€èåˆä¼˜åŒ–"
        ]
    },
    
    "åº”ç”¨è½åœ°": {
        "ç›®æ ‡": "é›†æˆåˆ°äº§å“ä¸­",
        "æ–¹æ³•": [
            "å¯¼èˆªåŠ©æ‰‹å¢å¼ºï¼ˆç©ºé—´ç†è§£é—®ç­”ï¼‰",
            "ARå¯¼èˆªï¼ˆå®æ—¶ç©ºé—´å®šä½ï¼‰",
            "è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ï¼ˆ3Dç‰©ä½“æ£€æµ‹ï¼‰",
            "æœºå™¨äººå¯¼èˆªï¼ˆè·¯å¾„è§„åˆ’ï¼‰"
        ]
    }
}
```

#### **é•¿æœŸå»ºè®®ï¼ˆ6-12ä¸ªæœˆï¼‰**

```python
long_term_plan = {
    "æŠ€æœ¯æ¼”è¿›": {
        "æ–¹å‘1": "ç«¯åˆ°ç«¯å…·èº«AIç³»ç»Ÿ",
        "æè¿°": "ä»æ„ŸçŸ¥â†’ç†è§£â†’è§„åˆ’â†’æ‰§è¡Œçš„å®Œæ•´é—­ç¯",
        "å…³é”®æŠ€æœ¯": [
            "ä¸–ç•Œæ¨¡å‹ï¼ˆWorld Modelï¼‰",
            "å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¼˜åŒ–",
            "ä»¿çœŸç¯å¢ƒè®­ç»ƒ",
            "çœŸå®ç¯å¢ƒéƒ¨ç½²"
        ]
    },
    
    "æ•°æ®ç”Ÿæ€": {
        "æ–¹å‘2": "æ„å»ºç©ºé—´æ™ºèƒ½æ•°æ®å¹³å°",
        "æè¿°": "å¼€æ”¾æ•°æ®æ ‡æ³¨å’Œå…±äº«å¹³å°",
        "å…³é”®ç»„ä»¶": [
            "æ•°æ®æ ‡æ³¨å·¥å…·",
            "è´¨é‡æ§åˆ¶ç³»ç»Ÿ",
            "æ•°æ®äº¤æ˜“å¸‚åœº",
            "ç¤¾åŒºè´¡çŒ®æ¿€åŠ±"
        ]
    },
    
    "äº§å“çŸ©é˜µ": {
        "æ–¹å‘3": "å¤šåœºæ™¯ç©ºé—´æ™ºèƒ½äº§å“",
        "æè¿°": "è¦†ç›–å¯¼èˆªã€é©¾é©¶ã€æœºå™¨äººã€AR/VR",
        "äº§å“çº¿": [
            "æ™ºèƒ½å¯¼èˆªåŠ©æ‰‹ï¼ˆCç«¯ï¼‰",
            "è‡ªåŠ¨é©¾é©¶æ„ŸçŸ¥ç³»ç»Ÿï¼ˆBç«¯ï¼‰",
            "æœºå™¨äººç©ºé—´ç†è§£SDKï¼ˆBç«¯ï¼‰",
            "ARç©ºé—´å®šä½æœåŠ¡ï¼ˆBç«¯ï¼‰"
        ]
    }
}
```

### 8.3 å…³é”®æˆåŠŸå› ç´ 

```python
success_factors = {
    "1. æ•°æ®è´¨é‡": {
        "é‡è¦æ€§": "â­â­â­â­â­",
        "å…³é”®ç‚¹": [
            "HDåœ°å›¾ç²¾åº¦ä¿è¯",
            "å¤šæ¨¡æ€å¿…è¦æ€§éªŒè¯",
            "ä¸¤é˜¶æ®µè´¨é‡è¿‡æ»¤",
            "æŒç»­çš„äººå·¥æŠ½æ£€"
        ],
        "é£é™©": "æ•°æ®å™ªå£°å¯¼è‡´æ€§èƒ½ä¸‹é™",
        "ç¼“è§£": "ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶æµç¨‹"
    },
    
    "2. è®­ç»ƒç­–ç•¥": {
        "é‡è¦æ€§": "â­â­â­â­â­",
        "å…³é”®ç‚¹": [
            "è¯¾ç¨‹å­¦ä¹ ä»æ˜“åˆ°éš¾",
            "å¯¹æ¯”å­¦ä¹ å¢å¼ºå…³ç³»ç†è§£",
            "å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–æ¨ç†",
            "å¤šä»»åŠ¡è”åˆè®­ç»ƒ"
        ],
        "é£é™©": "è®­ç»ƒä¸ç¨³å®šæˆ–è¿‡æ‹Ÿåˆ",
        "ç¼“è§£": "å……åˆ†çš„éªŒè¯å’Œearly stopping"
    },
    
    "3. è®¡ç®—èµ„æº": {
        "é‡è¦æ€§": "â­â­â­â­",
        "å…³é”®ç‚¹": [
            "å……è¶³çš„GPUèµ„æºï¼ˆ64x A100ï¼‰",
            "é«˜æ•ˆçš„åˆ†å¸ƒå¼è®­ç»ƒ",
            "æ··åˆç²¾åº¦åŠ é€Ÿ",
            "æ¢¯åº¦ç´¯ç§¯ä¼˜åŒ–"
        ],
        "é£é™©": "æˆæœ¬è¶…é¢„ç®—",
        "ç¼“è§£": "åˆ†é˜¶æ®µè®­ç»ƒï¼Œå…ˆå°æ¨¡å‹éªŒè¯"
    },
    
    "4. å›¢é˜Ÿåä½œ": {
        "é‡è¦æ€§": "â­â­â­â­",
        "å…³é”®ç‚¹": [
            "æ•°æ®å›¢é˜Ÿï¼ˆæ•°æ®æ”¶é›†å’Œå¤„ç†ï¼‰",
            "ç®—æ³•å›¢é˜Ÿï¼ˆæ¨¡å‹è®¾è®¡å’Œè®­ç»ƒï¼‰",
            "è¯„ä¼°å›¢é˜Ÿï¼ˆæ€§èƒ½è¯„ä¼°å’Œåˆ†æï¼‰",
            "äº§å“å›¢é˜Ÿï¼ˆåº”ç”¨è½åœ°ï¼‰"
        ],
        "é£é™©": "æ²Ÿé€šä¸ç•…å¯¼è‡´å»¶æœŸ",
        "ç¼“è§£": "å®šæœŸåŒæ­¥ä¼šè®®å’Œæ˜ç¡®åˆ†å·¥"
    },
    
    "5. è¿­ä»£é€Ÿåº¦": {
        "é‡è¦æ€§": "â­â­â­â­",
        "å…³é”®ç‚¹": [
            "å¿«é€Ÿå®éªŒéªŒè¯",
            "è‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹",
            "é”™è¯¯åˆ†æå’Œæ”¹è¿›",
            "æŒç»­ä¼˜åŒ–è¿­ä»£"
        ],
        "é£é™©": "è¿­ä»£å‘¨æœŸè¿‡é•¿",
        "ç¼“è§£": "å»ºç«‹è‡ªåŠ¨åŒ–pipeline"
    }
}
```

### 8.4 é¢„æœŸROIåˆ†æ

```python
roi_analysis = {
    "æŠ•å…¥æˆæœ¬": {
        "æ•°æ®æˆæœ¬": {
            "æ•°æ®æ”¶é›†": "å·²æœ‰ï¼ˆåœ°å›¾å…¬å¸ç°æœ‰èµ„äº§ï¼‰",
            "æ•°æ®æ ‡æ³¨": "$50,000ï¼ˆéƒ¨åˆ†äººå·¥æ ‡æ³¨ï¼‰",
            "æ•°æ®å­˜å‚¨": "$10,000ï¼ˆäº‘å­˜å‚¨ï¼‰",
            "å°è®¡": "$60,000"
        },
        "è®¡ç®—æˆæœ¬": {
            "GPUè®­ç»ƒ": "$80,000ï¼ˆ10,000 A100-hoursï¼‰",
            "å®éªŒéªŒè¯": "$20,000ï¼ˆå°æ¨¡å‹å®éªŒï¼‰",
            "å°è®¡": "$100,000"
        },
        "äººåŠ›æˆæœ¬": {
            "æ•°æ®å·¥ç¨‹å¸ˆ": "$30,000ï¼ˆ2äººÃ—3ä¸ªæœˆï¼‰",
            "ç®—æ³•å·¥ç¨‹å¸ˆ": "$45,000ï¼ˆ3äººÃ—3ä¸ªæœˆï¼‰",
            "è¯„ä¼°å·¥ç¨‹å¸ˆ": "$15,000ï¼ˆ1äººÃ—3ä¸ªæœˆï¼‰",
            "å°è®¡": "$90,000"
        },
        "æ€»æŠ•å…¥": "$250,000"
    },
    
    "é¢„æœŸæ”¶ç›Š": {
        "æŠ€æœ¯æ”¶ç›Š": {
            "æ€§èƒ½æå‡": "å¹³å‡+10.9%ï¼ˆ68.1%â†’79.0%ï¼‰",
            "æŠ€æœ¯é¢†å…ˆ": "åœ¨Spatial Understandingé¢†åŸŸè¾¾åˆ°SOTA",
            "è®ºæ–‡å‘è¡¨": "é¡¶ä¼šè®ºæ–‡ï¼ˆCVPR/ICCV/NeurIPSï¼‰",
            "å¼€æºå½±å“": "GitHub starså’Œç¤¾åŒºè®¤å¯"
        },
        "å•†ä¸šæ”¶ç›Š": {
            "å¯¼èˆªäº§å“": {
                "ç”¨æˆ·ä½“éªŒæå‡": "ç©ºé—´é—®ç­”å‡†ç¡®ç‡æå‡15%",
                "ç”¨æˆ·ç•™å­˜": "é¢„è®¡æå‡5%",
                "å¹´æ”¶ç›Šå¢é•¿": "$500,000"
            },
            "è‡ªåŠ¨é©¾é©¶": {
                "æ„ŸçŸ¥èƒ½åŠ›æå‡": "3Dç‰©ä½“æ£€æµ‹mAPæå‡10%",
                "å®‰å…¨æ€§æå‡": "å‡å°‘è¯¯æ£€å’Œæ¼æ£€",
                "å¹´æ”¶ç›Šå¢é•¿": "$1,000,000"
            },
            "Bç«¯æœåŠ¡": {
                "APIè°ƒç”¨": "ç©ºé—´ç†è§£APIæœåŠ¡",
                "SDKæˆæƒ": "æœºå™¨äºº/ARå…¬å¸æˆæƒ",
                "å¹´æ”¶ç›Šå¢é•¿": "$300,000"
            },
            "å•†ä¸šæ”¶ç›Šå°è®¡": "$1,800,000/å¹´"
        },
        "æˆ˜ç•¥æ”¶ç›Š": {
            "æŠ€æœ¯æŠ¤åŸæ²³": "ç‹¬ç‰¹çš„åœ°å›¾æ•°æ®ä¼˜åŠ¿",
            "äººæ‰å¸å¼•": "é¡¶å°–AIäººæ‰åŠ å…¥",
            "å“ç‰Œæå‡": "æŠ€æœ¯é¢†å…ˆå½¢è±¡",
            "ä¼°å€¼æå‡": "å…¬å¸ä¼°å€¼å¢é•¿"
        }
    },
    
    "ROIè®¡ç®—": {
        "ç¬¬ä¸€å¹´ROI": "($1,800,000 - $250,000) / $250,000 = 620%",
        "å›æœ¬å‘¨æœŸ": "çº¦2ä¸ªæœˆ",
        "3å¹´ç´¯è®¡æ”¶ç›Š": "$5,400,000",
        "3å¹´ROI": "2,060%"
    }
}
```

### 8.5 æœ€ç»ˆå»ºè®®ä¼˜å…ˆçº§

```python
priority_recommendations = {
    "P0 - ç«‹å³æ‰§è¡Œï¼ˆæœ¬æœˆï¼‰": [
        {
            "å»ºè®®": "å¯åŠ¨å°è§„æ¨¡éªŒè¯å®éªŒ",
            "åŸå› ": "éªŒè¯æ•°æ®è´¨é‡å’Œè®­ç»ƒæ–¹æ³•çš„å¯è¡Œæ€§",
            "èµ„æº": "1x A100 + 2åå·¥ç¨‹å¸ˆ",
            "å‘¨æœŸ": "2å‘¨",
            "äº§å‡º": "éªŒè¯æŠ¥å‘Šå’Œå¯è¡Œæ€§åˆ†æ"
        },
        {
            "å»ºè®®": "æ”¶é›†å’Œå¤„ç†10ä¸‡æ¡é“è·¯åœºæ™¯æ•°æ®",
            "åŸå› ": "ä¸ºæ­£å¼è®­ç»ƒåšå‡†å¤‡",
            "èµ„æº": "æ•°æ®å›¢é˜Ÿ",
            "å‘¨æœŸ": "2å‘¨",
            "äº§å‡º": "é«˜è´¨é‡è®­ç»ƒæ•°æ®é›†"
        }
    ],
    
    "P1 - è¿‘æœŸæ‰§è¡Œï¼ˆ1-2ä¸ªæœˆï¼‰": [
        {
            "å»ºè®®": "åœ¨Qwen3-VL-32Bä¸Šè¿›è¡Œå®Œæ•´è®­ç»ƒ",
            "åŸå› ": "éªŒè¯æ–¹æ¡ˆåœ¨ä¸­ç­‰è§„æ¨¡æ¨¡å‹ä¸Šçš„æ•ˆæœ",
            "èµ„æº": "32x A100 + è®­ç»ƒå›¢é˜Ÿ",
            "å‘¨æœŸ": "3å‘¨",
            "äº§å‡º": "è®­ç»ƒå¥½çš„æ¨¡å‹å’Œè¯„ä¼°æŠ¥å‘Š"
        },
        {
            "å»ºè®®": "å»ºç«‹è‡ªåŠ¨åŒ–è¯„ä¼°pipeline",
            "åŸå› ": "åŠ é€Ÿè¿­ä»£é€Ÿåº¦",
            "èµ„æº": "1åå·¥ç¨‹å¸ˆ",
            "å‘¨æœŸ": "2å‘¨",
            "äº§å‡º": "è‡ªåŠ¨åŒ–è¯„ä¼°ç³»ç»Ÿ"
        }
    ],
    
    "P2 - ä¸­æœŸæ‰§è¡Œï¼ˆ3-6ä¸ªæœˆï¼‰": [
        {
            "å»ºè®®": "æ‰©å±•åˆ°Qwen3-VL-235Bå¤§æ¨¡å‹",
            "åŸå› ": "è¿½æ±‚æœ€ä½³æ€§èƒ½",
            "èµ„æº": "64x A100 + å…¨å›¢é˜Ÿ",
            "å‘¨æœŸ": "1ä¸ªæœˆ",
            "äº§å‡º": "SOTAæ€§èƒ½æ¨¡å‹"
        },
        {
            "å»ºè®®": "é›†æˆåˆ°å¯¼èˆªäº§å“ä¸­",
            "åŸå› ": "å®ç°å•†ä¸šä»·å€¼",
            "èµ„æº": "äº§å“å›¢é˜Ÿ",
            "å‘¨æœŸ": "2ä¸ªæœˆ",
            "äº§å‡º": "å¢å¼ºç‰ˆå¯¼èˆªäº§å“"
        }
    ],
    
    "P3 - é•¿æœŸè§„åˆ’ï¼ˆ6-12ä¸ªæœˆï¼‰": [
        {
            "å»ºè®®": "æ„å»ºç«¯åˆ°ç«¯å…·èº«AIç³»ç»Ÿ",
            "åŸå› ": "æŠ€æœ¯é¢†å…ˆå’Œé•¿æœŸç«äº‰åŠ›",
            "èµ„æº": "ç ”ç©¶å›¢é˜Ÿ",
            "å‘¨æœŸ": "6ä¸ªæœˆ",
            "äº§å‡º": "å®Œæ•´çš„å…·èº«AIè§£å†³æ–¹æ¡ˆ"
        },
        {
            "å»ºè®®": "å¼€æ”¾æ•°æ®å¹³å°å’Œç¤¾åŒº",
            "åŸå› ": "å»ºç«‹ç”Ÿæ€å’Œå½±å“åŠ›",
            "èµ„æº": "å¹³å°å›¢é˜Ÿ",
            "å‘¨æœŸ": "6ä¸ªæœˆ",
            "äº§å‡º": "å¼€æ”¾æ•°æ®å¹³å°"
        }
    ]
}
```

---

## ä¹ã€å®Œæ•´ä»£ç ç¤ºä¾‹

ä¸ºäº†ä¾¿äºå®æ–½ï¼Œæˆ‘æä¾›ä¸€ä¸ªå®Œæ•´çš„ç«¯åˆ°ç«¯ä»£ç ç¤ºä¾‹ï¼š

```python
"""
å®Œæ•´çš„Spatial Understandingè®­ç»ƒå’Œè¯„ä¼°æµç¨‹
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import AutoModelForImageTextToText, AutoProcessor
from tqdm import tqdm
import numpy as np

# ============================================================================
# 1. æ•°æ®ç”Ÿæˆ
# ============================================================================

class MapSpatialDataGenerator:
    """åœ°å›¾å…¬å¸ç©ºé—´æ•°æ®ç”Ÿæˆå™¨"""
    
    def __init__(self, hd_map_db, street_view_db):
        self.hd_map = hd_map_db
        self.street_view = street_view_db
    
    def generate_dataset(self, num_samples=100000):
        """ç”Ÿæˆå®Œæ•´æ•°æ®é›†"""
        dataset = []
        
        for i in tqdm(range(num_samples), desc="Generating data"):
            # éšæœºé€‰æ‹©ä»»åŠ¡ç±»å‹
            task_type = np.random.choice([
                'navigation',
                'spatial_relation',
                'affordance',
                'distance_estimation',
                'direction_query'
            ])
            
            if task_type == 'navigation':
                sample = self.generate_navigation_task()
            elif task_type == 'spatial_relation':
                sample = self.generate_spatial_relation_task()
            elif task_type == 'affordance':
                sample = self.generate_affordance_task()
            elif task_type == 'distance_estimation':
                sample = self.generate_distance_task()
            else:
                sample = self.generate_direction_task()
            
            dataset.append(sample)
        
        return dataset
    
    def generate_navigation_task(self):
        """ç”Ÿæˆå¯¼èˆªä»»åŠ¡"""
        # é‡‡æ ·èµ·ç»ˆç‚¹
        start = self.hd_map.sample_location()
        end = self.hd_map.sample_location(distance_from=start, min_dist=500)
        
        # è§„åˆ’è·¯å¾„
        route = self.hd_map.plan_route(start, end)
        
        # è·å–è¡—æ™¯è§†é¢‘
        video = self.street_view.get_route_video(route)
        
        # ç”Ÿæˆé—®é¢˜
        question = f"Navigate from {start.address} to {end.address}. What actions should you take?"
        
        # Ground truth
        answer = route.get_turn_instructions()
        
        return {
            'video': video,
            'question': question,
            'answer': answer,
            'task_type': 'navigation'
        }
    
    def generate_spatial_relation_task(self):
        """ç”Ÿæˆç©ºé—´å…³ç³»ä»»åŠ¡"""
        # é‡‡æ ·ä½ç½®
        location = self.hd_map.sample_location()
        
        # è·å–è¡—æ™¯å›¾ç‰‡
        image = self.street_view.get_image(location)
        
        # è·å–å‘¨å›´POI
        pois = self.hd_map.get_nearby_pois(location, radius=100)
        
        # ç”Ÿæˆé—®é¢˜
        poi1, poi2 = np.random.choice(pois, 2, replace=False)
        question = f"What is the spatial relationship between {poi1.name} and {poi2.name}?"
        
        # Ground truth
        relation = self.hd_map.compute_spatial_relation(poi1, poi2)
        answer = f"{poi1.name} is {relation.direction} of {poi2.name}, approximately {relation.distance:.1f} meters away"
        
        return {
            'image': image,
            'question': question,
            'answer': answer,
            'task_type': 'spatial_relation'
        }

# ============================================================================
# 2. æ•°æ®é›†ç±»
# ============================================================================

class SpatialUnderstandingDataset(torch.utils.data.Dataset):
    """ç©ºé—´ç†è§£æ•°æ®é›†"""
    
    def __init__(self, data, processor):
        self.data = data
        self.processor = processor
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        
        # æ„å»ºæ¶ˆæ¯
        messages = [{
            "role": "user",
            "content": [
                {"type": "image" if 'image' in sample else "video",
                 "image" if 'image' in sample else "video": sample.get('image') or sample.get('video')},
                {"type": "text", "text": sample['question']}
            ]
        }]
        
        # å¤„ç†è¾“å…¥
        inputs = self.processor.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        )
        
        # å¤„ç†æ ‡ç­¾
        answer_ids = self.processor.tokenizer.encode(
            sample['answer'],
            add_special_tokens=False
        )
        
        return {
            'input_ids': inputs['input_ids'].squeeze(0),
            'attention_mask': inputs['attention_mask'].squeeze(0),
            'labels': torch.tensor(answer_ids),
            'image_grid_thw': inputs.get('image_grid_thw'),
            'video_grid_thw': inputs.get('video_grid_thw')
        }

# ============================================================================
# 3. è®­ç»ƒå™¨
# ============================================================================

class SpatialUnderstandingTrainer:
    """ç©ºé—´ç†è§£è®­ç»ƒå™¨"""
    
    def __init__(self, model_name="Qwen/Qwen3-VL-32B-Instruct"):
        self.model = AutoModelForImageTextToText.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto"
        )
        self.processor = AutoProcessor.from_pretrained(model_name)
        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-5)
    
    def train(self, train_dataset, num_epochs=3, batch_size=4):
        """è®­ç»ƒæ¨¡å‹"""
        dataloader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            collate_fn=self.collate_fn
        )
        
        self.model.train()
        
        for epoch in range(num_epochs):
            total_loss = 0
            
            for batch_idx, batch in enumerate(tqdm(dataloader, desc=f"Epoch {epoch+1}")):
                # å‰å‘ä¼ æ’­
                outputs = self.model(
                    input_ids=batch['input_ids'].to(self.model.device),
                    attention_mask=batch['attention_mask'].to(self.model.device),
                    labels=batch['labels'].to(self.model.device)
                )
                
                loss = outputs.loss
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                self.optimizer.step()
                
                total_loss += loss.item()
                
                # æ—¥å¿—
                if batch_idx % 100 == 0:
                    avg_loss = total_loss / (batch_idx + 1)
                    print(f"Batch {batch_idx}, Loss: {avg_loss:.4f}")
            
            print(f"Epoch {epoch+1} completed, Avg Loss: {total_loss/len(dataloader):.4f}")
    
    def collate_fn(self, batch):
        """æ‰¹å¤„ç†å‡½æ•°"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„paddingé€»è¾‘
        return {
            'input_ids': torch.stack([b['input_ids'] for b in batch]),
            'attention_mask': torch.stack([b['attention_mask'] for b in batch]),
            'labels': torch.stack([b['labels'] for b in batch])
        }

# ============================================================================
# 4. è¯„ä¼°å™¨
# ============================================================================

class SpatialUnderstandingEvaluator:
    """ç©ºé—´ç†è§£è¯„ä¼°å™¨"""
    
    def __init__(self, model, processor):
        self.model = model
        self.processor = processor
    
    def evaluate(self, test_dataset):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        
        correct = 0
        total = 0
        
        with torch.no_grad():
            for sample in tqdm(test_dataset, desc="Evaluating"):
                # ç”Ÿæˆé¢„æµ‹
                prediction = self.generate_answer(sample)
                
                # æ£€æŸ¥ç­”æ¡ˆ
                is_correct = self.check_answer(prediction, sample['answer'])
                
                if is_correct:
                    correct += 1
                total += 1
        
        accuracy = (correct / total) * 100
        print(f"Accuracy: {accuracy:.2f}%")
        
        return accuracy
    
    def generate_answer(self, sample):
        """ç”Ÿæˆç­”æ¡ˆ"""
        messages = [{
            "role": "user",
            "content": [
                {"type": "image" if 'image' in sample else "video",
                 "image" if 'image' in sample else "video": sample.get('image') or sample.get('video')},
                {"type": "text", "text": sample['question']}
            ]
        }]
        
        inputs = self.processor.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt"
        ).to(self.model.device)
        
        generated_ids = self.model.generate(**inputs, max_new_tokens=512)
        
        prediction = self.processor.batch_decode(
            generated_ids,
            skip_special_tokens=True
        )[0]
        
        return prediction
    
    def check_answer(self, prediction, ground_truth):
        """æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦æ­£ç¡®"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„åŒ¹é…é€»è¾‘
        return ground_truth.lower() in prediction.lower()

# ============================================================================
# 5. ä¸»æµç¨‹
# ============================================================================

def main():
    """ä¸»æµç¨‹"""
    print("="*80)
    print("Spatial Understanding Training Pipeline")
    print("="*80)
    
    # 1. æ•°æ®ç”Ÿæˆ
    print("\n[Step 1] Generating training data...")
    data_generator = MapSpatialDataGenerator(hd_map_db=None, street_view_db=None)
    train_data = data_generator.generate_dataset(num_samples=10000)
    test_data = data_generator.generate_dataset(num_samples=1000)
    
    # 2. åˆ›å»ºæ•°æ®é›†
    print("\n[Step 2] Creating datasets...")
    processor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-32B-Instruct")
    train_dataset = SpatialUnderstandingDataset(train_data, processor)
    test_dataset = SpatialUnderstandingDataset(test_data, processor)
    
    # 3. è®­ç»ƒæ¨¡å‹
    print("\n[Step 3] Training model...")
    trainer = SpatialUnderstandingTrainer()
    trainer.train(train_dataset, num_epochs=3, batch_size=4)
    
    # 4. è¯„ä¼°æ¨¡å‹
    print("\n[Step 4] Evaluating model...")
    evaluator = SpatialUnderstandingEvaluator(trainer.model, processor)
    accuracy = evaluator.evaluate(test_dataset)
    
    print(f"\n[Final] Training completed! Accuracy: {accuracy:.2f}%")

if __name__ == "__main__":
    main()
```

---

## åã€æœ€ç»ˆæ€»ç»“

æœ¬æŠ¥å‘Šæä¾›äº†ä¸€ä¸ª**å®Œæ•´ã€ç³»ç»Ÿã€å¯æ‰§è¡Œ**çš„æ–¹æ¡ˆï¼Œç”¨äºæå‡Qwen3-VLåœ¨Embodied/Spatial Understandingæ–¹é¢çš„èƒ½åŠ›ã€‚

### æ ¸å¿ƒäº®ç‚¹ï¼š

1. **æ•°æ®ä¼˜åŠ¿æœ€å¤§åŒ–**ï¼šå……åˆ†åˆ©ç”¨åœ°å›¾å…¬å¸çš„HDåœ°å›¾ã€è¡—æ™¯ã€è¡Œè½¦è®°å½•ä»ªç­‰ç‹¬ç‰¹æ•°æ®èµ„æº
2. **æŠ€æœ¯æ–¹æ¡ˆå®Œæ•´**ï¼šä»æ•°æ®ç”Ÿæˆâ†’æ¨¡å‹è®­ç»ƒâ†’è¯„ä¼°è¿­ä»£çš„å®Œæ•´æµç¨‹
3. **é¢„æœŸæ•ˆæœæ˜ç¡®**ï¼šå¹³å‡æ€§èƒ½ä»68.1%æå‡åˆ°79.0%ï¼ˆ+10.9%ï¼‰
4. **å•†ä¸šä»·å€¼æ¸…æ™°**ï¼šç¬¬ä¸€å¹´ROIè¾¾620%ï¼Œç›´æ¥èµ‹èƒ½å¯¼èˆªå’Œè‡ªåŠ¨é©¾é©¶ä¸šåŠ¡
5. **å®æ–½è·¯å¾„æ˜ç¡®**ï¼šåˆ†é˜¶æ®µå®æ–½ï¼Œé£é™©å¯æ§ï¼Œ3ä¸ªæœˆå¯è§æˆæ•ˆ

### ç«‹å³è¡ŒåŠ¨å»ºè®®ï¼š

**æœ¬å‘¨å†…å¯åŠ¨**ï¼š
- ç»„å»ºé¡¹ç›®å›¢é˜Ÿï¼ˆæ•°æ®+ç®—æ³•+è¯„ä¼°ï¼‰
- æ”¶é›†10ä¸‡æ¡é“è·¯åœºæ™¯æ•°æ®
- å¯åŠ¨å°è§„æ¨¡éªŒè¯å®éªŒï¼ˆQwen3-VL-8Bï¼‰

**æœ¬æœˆå†…å®Œæˆ**ï¼š
- éªŒè¯æ•°æ®è´¨é‡å’Œè®­ç»ƒæ–¹æ³•
- ç¡®å®šæœ€ç»ˆæŠ€æœ¯æ–¹æ¡ˆ
- ç”³è¯·è®¡ç®—èµ„æºï¼ˆ64x A100ï¼‰

**3ä¸ªæœˆå†…äº¤ä»˜**ï¼š
- è®­ç»ƒå¥½çš„Qwen3-VL-32Bæ¨¡å‹
- 5ä¸ªæ•°æ®é›†çš„å®Œæ•´è¯„ä¼°æŠ¥å‘Š
- é›†æˆåˆ°å¯¼èˆªäº§å“çš„demo

è¿™æ˜¯ä¸€ä¸ª**é«˜å›æŠ¥ã€ä½é£é™©ã€å¯è½åœ°**çš„æŠ€æœ¯æ–¹æ¡ˆï¼Œå¼ºçƒˆå»ºè®®ç«‹å³å¯åŠ¨ï¼


