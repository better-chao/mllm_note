## 三大模块架构

1. **Vision Encoder（视觉编码器）**
   - 采用SigLIP-2架构
   - 支持动态输入分辨率
   - 默认使用SigLIP2-SO-400M变体（400M参数）
   - 小规模模型（2B/4B）使用SigLIP2-Large（300M参数）

2. **MLP-based Vision-Language Merger（视觉-语言合并器）**
   - 使用两层MLP结构
   - 将2×2的视觉特征压缩为单个视觉token
   - 与LLM的隐藏维度对齐
   - 支持DeepStack机制的专门合并器

3. **Large Language Model（大语言模型）**
   - 基于Qwen3系列
   - 提供多种规模：密集模型（2B/4B/8B/32B）和MoE模型（30B-A3B/235B-A22B）

这种三模块架构继承自Qwen2.5-VL，是一个经典的视觉-语言模型设计范式，能够有效地处理文本、图像和视频等多模态输入。

---

## 👤 User

请给出这篇文章的详细技术报告

---

## 🤖 Claude

# Qwen3-VL 技术报告详解

## 一、模型概述

### 1.1 基本信息
- **发布时间**：2025年12月1日
- **模型系列**：Qwen系列最强视觉-语言模型
- **上下文窗口**：原生支持256K tokens
- **模型规模**：
  - 密集模型：2B/4B/8B/32B
  - MoE模型：30B-A3B（激活3B）、235B-A22B（激活22B）

### 1.2 三大核心能力
1. **纯文本理解**：性能超越部分纯文本模型
2. **长文档理解**：256K token窗口支持文本和多模态交错输入
3. **多模态推理**：在MMMU、MathVista等benchmark上领先

---

## 二、架构创新

### 2.1 整体架构（三模块设计）

#### **Vision Encoder（视觉编码器）**
- 基础架构：SigLIP-2
- 参数规模：
  - 标准：SigLIP2-SO-400M（400M参数）
  - 小模型：SigLIP2-Large（300M参数，用于2B/4B）
- 技术特点：
  - 支持动态分辨率输入
  - 使用2D-RoPE位置编码
  - 根据输入尺寸插值绝对位置编码

#### **Vision-Language Merger（视觉-语言合并器）**
- 结构：两层MLP
- 功能：将2×2视觉特征压缩为单个视觉token
- 特殊设计：支持DeepStack机制的专用合并器

#### **Large Language Model（大语言模型）**
- 基座：Qwen3系列
- 旗舰模型：Qwen3-VL-235B-A22B
  - 总参数：235B
  - 激活参数：22B/token
  - 性能：在多数任务上超越纯文本模型

### 2.2 三大架构升级

#### **升级1：Interleaved MRoPE（交错多分辨率位置编码）**

**问题诊断**：
- Qwen2.5-VL使用的原始MRoPE将嵌入维度分块为时间(t)、水平(h)、垂直(w)三个子空间
- 导致频谱不平衡，影响长视频理解

**解决方案**：
- 将t、h、w三个分量交错分布在嵌入维度上
- 确保每个空间-时间轴在低频和高频段均匀表示
- 显著提升长距离位置建模能力

**技术细节**：
```
原始MRoPE: [t, t, ..., h, h, ..., w, w, ...]  # 分块排列
交错MRoPE: [t, h, w, t, h, w, t, h, w, ...]  # 交错排列
```

#### **升级2：DeepStack（深度堆叠视觉token）**

**核心思想**：
- 从ViT的多层提取视觉token
- 将不同层级的视觉特征注入LLM的对应层

**具体实现**：
1. 从视觉编码器选择3个不同层级的特征
2. 使用专用merger将多层特征投影为视觉tokens
3. 直接加到LLM前三层的隐藏状态上

**优势**：
- 保留从低级到高级的丰富视觉信息
- 增强视觉-语言对齐
- 不增加额外上下文长度

#### **升级3：Text-based Time Alignment（基于文本的时间对齐）**

**Qwen2.5-VL的问题**：
1. 使用T-RoPE将时间位置ID直接绑定到绝对时间
2. 长视频产生过大、稀疏的时间位置ID
3. 需要大量不同帧率的训练数据

**Qwen3-VL的解决方案**：
- 使用文本token表示时间戳：`<3.0 seconds>`
- 支持多种格式：秒(seconds)和时分秒(HMS)格式
- 虽然增加上下文长度，但显著提升时间感知能力

**应用场景**：
- 视频定位(Video Grounding)
- 密集视频描述(Dense Video Captioning)

---

## 三、训练方法论

### 3.1 预训练阶段（4个阶段）

#### **Stage 0: Vision-Language Alignment（视觉-语言对齐）**
- **训练范围**：仅训练Merger层
- **冻结部分**：Vision Encoder + LLM
- **数据量**：67B tokens
- **数据类型**：
  - 高质量图像-文本配对
  - 视觉知识集合
  - OCR数据
- **序列长度**：8,192 tokens

#### **Stage 1: Multimodal Pre-Training（多模态预训练）**
- **训练范围**：全参数端到端训练
- **数据量**：~1T tokens
- **数据组成**：
  - 视觉-语言(VL)数据
  - 纯文本数据（保持LLM语言能力）
  - 交错图像-文本文档
  - 视觉定位任务
  - VQA数据
  - STEM领域数据
  - 少量视频数据
- **序列长度**：8,192 tokens

#### **Stage 2: Long-Context Pre-Training（长上下文预训练）**
- **序列长度**：32,768 tokens（4倍提升）
- **数据量**：~1T tokens
- **数据调整**：
  - 增加纯文本比例（强化长文本理解）
  - 大幅增加视频数据
  - 增加agent导向的指令跟随数据
- **目标**：支持长视频和复杂多步骤任务

#### **Stage 3: Ultra-Long-Context Adaptation（超长上下文适配）**
- **序列长度**：262,144 tokens（256K）
- **数据量**：100B tokens
- **数据重点**：
  - 长视频理解
  - 长文档理解
- **能力**：处理极长序列输入

### 3.2 后训练阶段（3个阶段）

#### **阶段1：Supervised Fine-Tuning (SFT)**

**训练策略**：
- 两阶段训练：
  - 第一阶段：32K上下文长度
  - 第二阶段：256K上下文长度（关注长文档和长视频）

**模型变体**：
- **Non-thinking模型**：标准格式数据
- **Thinking模型**：Chain-of-Thought (CoT)格式数据

**数据规模与质量**：
- 约120万样本
- 数据组成：
  - 1/3 纯文本数据
  - 2/3 多模态数据（图像-文本、视频-文本）
- 支持多语言
- 包含单轮和多轮对话
- 支持工具增强的图像搜索和视觉推理

**数据过滤流程**：

1. **Query Filtering（问题过滤）**：
   - 使用Qwen2.5-VL识别不可验证的查询
   - 最小修订模糊指令
   - 消除缺乏实质内容的查询
   - 评估复杂性和相关性

2. **Response Filtering（响应过滤）**：
   - **规则过滤**：
     - 消除重复、不完整、格式错误的响应
     - 过滤偏离主题或有害内容
   - **模型过滤**：
     - 使用Qwen2.5-VL系列奖励模型
     - 多维度评估：正确性、完整性、清晰度、有用性
     - 特别验证视觉信息的准确使用

#### **阶段2：Strong-to-Weak Distillation（强到弱蒸馏）**

**核心方法**：
- 使用强大的教师模型向学生模型传递能力
- **关键特点**：使用纯文本数据微调LLM骨干网络

**蒸馏流程**：
1. **Off-policy Distillation（离策略蒸馏）**：
   - 组合教师模型生成的输出
   - 帮助学生模型获得基础推理能力
   
2. **On-policy Distillation（在策略蒸馏）**：
   - 学生模型生成响应
   - 通过最小化KL散度对齐学生和教师的logits

**效果**：
- 显著提升推理能力（文本和多模态任务）
- 高效构建轻量级模型

#### **阶段3：Reinforcement Learning (RL)**

**RL-1: Reasoning RL（推理强化学习）**

数据准备：
- 任务类型：数学、编码、逻辑推理、视觉定位、视觉谜题
- 可确定性验证（规则或代码执行器）
- 约30K RL查询
- 过滤掉通过率>90%的简单问题

奖励系统：
- 统一奖励框架
- 任务特定格式提示
- 代码切换惩罚机制

RL算法：
- **SAPO** (Smooth and Adaptive Policy Optimization)
- 在不同任务、模型规模和架构上表现一致

**RL-2: General RL（通用强化学习）**

优化目标：
1. **Instruction Following（指令跟随）**：
   - 评估对用户指令的遵循能力
   - 处理内容、格式、长度、结构化输出约束

2. **Preference Alignment（偏好对齐）**：
   - 针对开放式查询
   - 优化有用性、事实准确性、风格适当性

3. **纠正错误先验**：
   - 针对性任务：反直觉物体计数、复杂时钟识别
   - 用事实知识替代错误先验

4. **抑制不良行为**：
   - 不当语言混用
   - 过度重复
   - 格式错误

奖励机制：
- **规则奖励**：高精度验证明确的真值
- **模型奖励**：使用Qwen2.5-VL-72B或Qwen3作为评判器

### 3.3 Thinking with Images（视觉思考）

**两阶段训练范式**：

**第一阶段（冷启动）**：
- 合成约10K定位样本
- 简单的两轮VQA任务（属性检测）
- 在Qwen2.5-VL-32B上SFT
- 行为模式：think → act → analyze feedback → answer
- 应用多轮工具集成RL

**第二阶段（扩展）**：
- 从第一阶段的代理蒸馏生成约120K交互数据
- 覆盖更广泛的视觉任务
- 使用蒸馏和合成数据进行Qwen3-VL的后训练

**RL奖励信号**（3种）：
1. **Answer Accuracy Reward**：使用Qwen3-32B评估最终答案正确性
2. **Multi-Turn Reasoning Reward**：使用Qwen2.5-VL-72B评估推理过程
3. **Tool-Calling Reward**：比较实际工具调用次数与专家估计目标

**关键发现**：
- 模型倾向于退化为单次工具调用（hack前两个奖励）
- 引入工具调用奖励以促进自适应探索

### 3.4 优化技巧

#### **Square-Root Reweighting（平方根重加权）**
- **问题**：文本和多模态样本长度差异大
- **解决**：从按样本损失改为按token平方根归一化损失
- **效果**：更好平衡文本和多模态数据贡献

---

## 四、训练数据构建

### 4.1 Image Caption & Interleaved Data

#### **Image Caption Data（图像描述数据）**

**数据来源**：
- 大规模现代中英双语图像-文本对
- 来自网络来源

**精炼流程**：
1. **Recaptioning（重新描述）**：
   - 使用专门微调的Qwen2.5-VL-32B模型
   - 利用原始文本生成更全面、流畅、细粒度的描述
   - 丰富视觉元素描述（物体属性、空间布局、上下文语义）

2. **去重**：
   - 仅对重新描述的文本进行语义相似度去重
   - 保持视觉多样性

3. **聚类增强**：
   - 使用FAISS等工具对视觉嵌入聚类
   - 识别稀疏区域
   - 针对性数据增强

**结果**：高保真、规模化、多样化、描述粒度细的数据集

#### **Interleaved Text-Image Data（交错文本-图像数据）**

**数据来源**：
- 中英文网站的真实多模态文档
- 书籍规模的交错数据

**处理流程**：
1. **领域分类**：
   - 使用微调的Qwen模型进行细粒度领域识别
   - 系统性排除：广告、促销内容、标题党

2. **多模态解析**（书籍数据）：
   - 使用微调的Qwen2.5-VL-7B模型
   - 精确提取和对齐文本与嵌入的图表、照片

3. **超长上下文构建**：
   - 合并连续页面至256K tokens
   - 保持自然页面顺序和多模态一致性

**质量控制**：
- 移除纯文本或低对齐片段
- 超长书籍序列要求：
  - 最小页数阈值
  - 最小图像-文本比率

### 4.2 Knowledge（知识数据）

**目标**：提升实体感知和视觉识别能力

**实体类别**（>12类）：
- 动物、植物、地标
- 食物
- 日常物品（车辆、电子产品、服装等）

**采样策略**：
- **重要性采样**：高知名度实体采样更多
- **长尾覆盖**：低知名度实体少量采样
- 平衡数据质量、实用性和多样性

**数据精炼**：
1. 噪声和错位过滤
2. **Caption增强**：
   - 使用LLM生成丰富描述
   - 不仅识别主实体
   - 描述：视觉属性、周围环境、空间布局、交互关系

**效果**：知识丰富、上下文感知、区分性强的训练信号

### 4.3 OCR, Document Parsing & Long Document

#### **OCR数据**
- **规模**：3000万内部样本
- **流程**：粗到细pipeline
  - OCR专用模型生成伪标签
  - Qwen2.5-VL精炼
  - 无人工标注

- **多语言扩展**：
  - Qwen2.5-VL支持10种语言（除中英文外）
  - Qwen3-VL新增29种语言
  - 合成约3000万高质量多语言OCR样本
  - 策划超过100万内部真实多语言图像

#### **Document Parsing（文档解析）**

**数据来源**：
- 300万PDF from Common Crawl
- 10种文档类型（各30万样本）
- 400万内部文档

**处理流程**：
1. 内部布局模型预测阅读顺序和边界框
2. Qwen2.5-VL-72B进行区域特定识别
3. 重组为位置感知、布局对齐的解析数据

**统一标注框架**（2种表示）：
1. **QwenVL-HTML**：
   - 细粒度元素级边界框
   
2. **QwenVL-Markdown**：
   - 仅对图像和表格定位
   - 表格编码为LaTeX

**数据增强**：
- 大规模合成HTML语料
- 系统性转换为Markdown格式
- 在真实文档上生成伪标签并质量过滤

#### **Long Document Understanding（长文档理解）**

**目标**：处理数十页的多页PDF

**数据构建**：
1. **合成长文档解析序列**：
   - 合并单页文档样本
   - 多个页面图像置于开头
   - 后跟OCR或HTML解析的对应文本

2. **长文档VQA**：
   - 采样高质量多页PDF
   - 生成多样化VQA样本
   - 要求跨页面推理
   - 支持证据来自多种模态：图表、表格、图形、正文
   - 平衡问题类型分布
   - 促进鲁棒、有依据的多跳推理

### 4.4 Grounding and Counting（定位和计数）

#### **Box-based Grounding（基于框的定位）**

**开源数据集**：
- COCO、Objects365、OpenImages、RefCOCO/+/g

**自动合成pipeline**（3阶段）：
1. 使用Qwen2.5-VL从未标注图像提取候选对象
2. 使用开放词汇检测器（Grounding DINO）和Qwen2.5-VL定位和标注
3. 质量评估，系统性过滤低置信度或不准确标注

**结果**：大规模、高多样性的框定位数据集

#### **Point-based Grounding（基于点的定位）**

**数据来源**（3种）：
1. 公开pointing和counting标注（PixMo）
2. 公开目标检测和实例分割benchmark
3. 高精度pointing标注（专用合成pipeline）

#### **Counting（计数）**

**任务类型**（3种）：
1. Direct counting（直接计数）
2. Box-based counting（基于框计数）
3. Point-based counting（基于点计数）

**坐标系统**：
- 归一化坐标：缩放到[0, 1000]范围
- 与Qwen2.5-VL不同
- 优势：
  - 对分辨率和宽高比变化更鲁棒
  - 简化后处理
  - 提升下游应用可用性

### 4.5 Spatial Understanding & 3D Recognition

#### **Spatial Understanding（空间理解）**

**能力目标**：
- 解释空间关系
- 推断物体功能属性
- 动作规划和具身推理

**数据类型**：
1. **关系标注**：
   - 例："杯子在笔记本电脑左侧"

2. **功能属性标签**：
   - 例："可抓取"、"可按压"、"可坐"

3. **动作条件查询**：
   - 例："要拿到显示器后面的书，我应该先移动什么？"

**数据来源**：
- 策划的真实世界场景
- 合成生成的布局

**查询生成**：
- 模板化方法
- LLM生成（确保多样性和复杂性）

**关键特点**：
- 所有空间引用相对于其他物体或场景框架
- 而非绝对坐标
- 鼓励鲁棒的关系推理

#### **3D Grounding（3D定位）**

**数据来源**：
- 公开的室内和室外场景集合

**数据格式**：
- VQA格式
- 组成：
  1. 单视角相机图像
  2. 自然语言指代表达
  3. 9-DoF 3D边界框标注（JSON格式）
     - 物体空间位置
     - 语义标签

**数据处理**：
1. 过滤严重遮挡和不准确标签
2. 遵循Omni3D统一数据到虚拟相机坐标系
3. 合成大量描述性captions
4. 创建丰富的3D定位文本查询

**Caption增强**：
- 超越类别名称
- 包括：详细属性、布局安排、空间位置、视觉功能属性、与周围物体的交互

### 4.6 Code（代码数据）

#### **Text-Only Coding（纯文本代码）**
- 复用Qwen3和Qwen3-Coder系列代码语料
- 涵盖：
  - 软件开发
  - 算法问题求解
  - 数学推理
  - agent导向任务
- 建立代码语法、算法逻辑、通用程序生成的基础理解

#### **Multimodal Coding（多模态代码）**

**数据来源**：
- 开源数据集
- 内部合成pipeline

**任务类型**：
1. UI截图转换为响应式HTML/CSS
2. 从图像生成可编辑SVG代码
3. 解决视觉编程挑战
4. 回答多模态编码问题（如StackOverflow带图帖子）
5. 转录视觉表示为代码/标记：
   - 流程图
   - 图表
   - LaTeX方程

**效果**：模型成为视觉感知和可执行逻辑之间的桥梁

### 4.7 Video（视频数据）

#### **Temporal-Aware Video Understanding（时间感知视频理解）**

**创新1：Dense Caption Synthesis（密集描述合成）**
- **策略**：短到长caption合成
- **使用**：内部captioning模型
- **输出**：
  - 整体故事级描述
  - 时间戳交错
  - 时间连贯
- **精细化标注**：
  - 事件级时间摘要
  - 片段特定视觉细节

**创新2：Spatio-Temporal Video Grounding（时空视频定位）**
- 策划和合成大规模视频数据
- 标注级别：
  - 物体级
  - 动作级
  - 人物级
- 强化时空定位能力
- 提升细粒度视频理解

#### **Video Data Balancing and Sampling（视频数据平衡和采样）**

**来源平衡**：
- 教学内容
- 电影片段
- 以自我为中心的录制
- 等多种视频来源

**平衡方法**：
- 基于元数据的系统性策划：
  - 视频标题
  - 时长
  - 分类标签

**长度自适应采样**：
- 根据序列长度约束动态调整：
  - 帧率(fps)
  - 最大帧数
- 缓解次优采样信息损失
- 保留视觉细节
- 优化训练效率

### 4.8 STEM（科学、技术、工程、数学）

**核心理念**：分而治之策略
1. 独立开发细粒度视觉感知
2. 独立开发鲁棒语言推理
3. 协同整合实现多模态推理

#### **Visual Perception Data（视觉感知数据）**

**合成pipeline**：
- 通过代码渲染生成几何图表

**数据类型**：
1. **Point Grounding（点定位）**：
   - 100万样本
   - 目标：交点、角点、重心

2. **Perception VQA（感知VQA）**：
   - 200万问答对
   - 目标：图表的细粒度视觉理解

3. **Diagram Captions（图表描述）**：
   - 600万样本
   - 丰富标注，涵盖多个STEM学科

**Caption生成**：
- 两阶段框架：
  1. 初始生成
  2. 基于模型的严格验证
- 使用专业模型集成确保准确性和描述粒度

#### **Multi-modal Reasoning Data（多模态推理数据）**

**主要数据**：
- 超过6000万K-12和本科级习题

**数据处理**：
1. **质量过滤**：
   - 丢弃低质量样本：
     - 图像损坏
     - 内容无关
     - 答案不完整或错误

2. **重新表述**：
   - 中英文互译
   - 标准化答案格式：
     - 逐步解决方案列表
     - 数学表达式
     - 符号表示法

**Long CoT Problem-Solving Data（长CoT问题求解数据）**：
- 合成超过1200万多模态推理样本
- 使用强推理模型的原始rollouts
- 确保推理过程的连续性和丰富性

**数据验证**：
- 结合规则检查和模型验证
- 过滤模糊答案或代码切换实例
- 通过拒绝采样保留挑战性问题

#### **Linguistic Reasoning Data（语言推理数据）**
- 整合Qwen3的推理数据
- 多模态推理能力主要源于语言推理能力

### 4.9 Agent（智能体数据）

#### **GUI（图形用户界面）**

**数据覆盖**：
- 桌面环境
- 移动环境
- Web环境

**GUI Interface Perception（界面感知）**：
- 数据来源：
  - 元数据
  - 解析工具
  - 人工标注
- 任务类型：
  - 元素描述
  - 密集captioning
  - 密集定位
- 目标：鲁棒理解多样化用户界面

**Agentic Capability（代理能力）**：
- 多步骤任务轨迹
- 生成方法：
  - 自演化轨迹生产框架
  - 有针对性的人工审计
- Chain-of-Thought设计和增强：
  - 加强规划
  - 决策制定
  - 实时执行中的反思性自我纠正

#### **Function Calling（函数调用）**

**多模态函数调用轨迹合成pipeline**：
1. 指示capable模型生成：
   - 用户查询
   - 对应函数定义
2. 采样模型函数调用和理由
3. 合成函数响应
4. 重复直到查询被判定为解决
5. 每步之间过滤格式错误的轨迹

**优势**：
- 从大量图像构建大规模多模态函数调用轨迹
- 无需实现可执行函数

#### **Search（搜索）**

**重要性**：促进长尾实体的知识整合

**数据类型**：
- 多模态事实查找轨迹
- 工具：
  - 在线图像搜索
  - 文本搜索

**学习目标**：
- 对不熟悉实体执行搜索
- 从网络收集信息生成更准确响应

---

## 五、性能评估

### 5.1 综合性能（Qwen3-VL-235B-A22B）

#### **STEM & Puzzle领域**
| Benchmark | Qwen3-VL Thinking | Qwen3-VL Instruct | Gemini 2.5 Pro | GPT-5 | Claude Opus 4.1 |
|-----------|-------------------|-------------------|----------------|-------|-----------------|
| MMMU | 80.6 | 78.7 | 81.7 | 80.9 | 74.4 |
| MMMU-Pro | 69.3 | 68.1 | 68.8 | 71.2 | 62.7 |
| MathVista | 85.8 | 84.9 | 82.7 | 77.7 | 75.5 |
| MathVision | 74.6 | 66.5 | 73.3 | 66.0 | 64.3 |
| ZeroBench | 4 | 2 | 3 | 1 | 3 |
| LogicVista | 72.2 | 65.8 | 72.0 | 68.7 | 67.3 |
| VisuLogic | 34.4 | 29.9 | 31.6 | 26.9 | 27.9 |

#### **General VQA**
| Benchmark | Qwen3-VL Thinking | Qwen3-VL Instruct | Gemini 2.5 Pro | GPT-5 |
|-----------|-------------------|-------------------|----------------|-------|
| MMBench-EN | 88.8 | 89.3 | 90.1 | 88.4 |
| RealWorldQA | 81.3 | 79.2 | 78.0 | 76.0 |
| MMStar | 78.7 | 78.4 | 77.5 | 78.5 |

#### **Document Understanding**
| Benchmark | Qwen3-VL Instruct | Gemini 2.5 Pro | GPT-5 |
|-----------|-------------------|----------------|-------|
| DocVQA | 97.1 | 92.6 | 94.0 |
| OCRBench | 920 | 866 | 872 |
| OCRBench_v2(en) | 67.1 | 54.3 | 55.2 |
| CharXiv(RQ) | 62.1 | 67.9 | 62.9 |

#### **2D/3D Grounding**
| Benchmark | Qwen3-VL Instruct | Gemini 2.5 Pro | GPT-5 |
|-----------|-------------------|----------------|-------|
| RefCOCO-avg | 91.9 | 74.6 | - |
| CountBench | 93.0 | 91.0 | 91.0 |
| ODinW-13 | 48.6 | 33.7 | 34.5 |
| SUNRGBD | 39.4 | 29.7 | - |

#### **Video Understanding**
| Benchmark | Qwen3-VL Instruct | Gemini 2.5 Pro | GPT-5 |
|-----------|-------------------|----------------|-------|
| VideoMME | 79.2 | 85.1 | 80.6 |
| MLVU | 84.3 | 85.6 | 81.2 |
| LVBench | 67.7 | 73.0 | 69.0 |

### 5.2 长上下文能力

**Needle-in-a-Haystack测试**（Qwen3-VL-235B-A22B-Instruct）：
- **训练范围内**（0-30分钟，256K tokens）：
  - 准确率：**100%**
  
- **外推范围**（40-120分钟，512K-1024K tokens）：
  - 准确率：**99.5%**
  - 使用YaRN位置扩展

**评估设置**：
- 视频采样：1 FPS
- 帧分辨率：动态调整以保持恒定视觉token预算

### 5.3 多语言能力

**OCR多语言支持**（内部测试集）：
- 支持语言数：**39种**
- 达到70%+准确率：**32种语言**
- 显著扩展：从Qwen2.5-VL的10种（除中英文）到39种

### 5.4 文本任务性能（Qwen3-VL-235B-A22B-Instruct）

#### **vs 纯文本模型**
| Benchmark | Qwen3-VL | Qwen3 | DeepSeek V3 | Claude Opus 4 |
|-----------|----------|-------|-------------|---------------|
| MMLU-Pro | 81.8 | 83.0 | 81.2 | 86.6 |
| GPQA | 74.3 | 77.5 | 68.4 | 74.9 |
| AIME-25 | 74.7 | 70.3 | 46.6 | 33.9 |
| HMMT-25 | 57.4 | 55.4 | 27.5 | 15.9 |
| LiveCodeBench v6 | 54.3 | 51.8 | 45.2 | 44.6 |
| IFEval | 87.8 | 88.7 | 82.3 | 87.4 |
| Arena-Hard V2 | 77.4 | 79.2 | 45.6 | 51.5 |

**关键发现**：
- Qwen3-VL在AIME-25和HMMT-25等推理任务上**超越纯文本模型**
- 实现视觉和文本能力的真正整合

### 5.5 模型规模对比

#### **中等规模模型**（32B/30B-A3B）
- Qwen3-VL-32B在多数benchmark上**超越Gemini-2.5-Flash和GPT-5-mini**
- 在推理任务上**超越前代Qwen2.5-VL-72B**

#### **小规模模型**（2B/4B/8B）
- Qwen3-VL-8B在所有5个benchmark上取得最高性能
- MMBench-EN：79.9 (2B) → 85.3 (8B)
- MMStar：68.1 (2B) → 75.3 (8B)

### 5.6 Ablation Study（消融实验）

#### **Vision Encoder对比**
| ViT | ImageNet-1K | OmniBench | OCRB | AI2D | InfoVQA |
|-----|-------------|-----------|------|------|---------|
| SigLIP-2 | 84.2 | 36.9 | 77.2 | 74.1 | 65.3 |
| Qwen3-ViT | 84.6 | 45.5 | 78.7 | 76.2 | 67.0 |

**结论**：Qwen3-ViT在OmniBench上大幅提升（+8.6），证明其作为更强视觉骨干网络的有效性

#### **DeepStack效果**
| Method | AVG | InfoVQA | DocVQA | MMMU | MMStar |
|--------|-----|---------|--------|------|--------|
| Baseline | 74.7 | 71.9 | 89.5 | 52.9 | 55.5 |
| DeepStack | 76.0 | 74.2 | 91.1 | 54.1 | 57.7 |

**结论**：DeepStack显著提升细粒度视觉理解能力（InfoVQA +2.3, DocVQA +1.6）

---

## 六、核心技术亮点

### 6.1 架构创新
1. **Interleaved MRoPE**：平衡频谱，提升长视频理解
2. **DeepStack**：多层视觉特征融合，增强视觉-语言对齐
3. **Text-based Timestamps**：显式时间表示，精确时间定位

### 6.2 训练创新
1. **Square-Root Reweighting**：平衡文本和多模态学习
2. **4阶段预训练**：从8K逐步扩展到256K上下文
3. **3阶段后训练**：SFT + 蒸馏 + RL
4. **Thinking with Images**：视觉思考能力

### 6.3 数据创新
1. **合成数据pipeline**：大规模自动化数据生成
2. **多级质量过滤**：规则+模型双重过滤
3. **长上下文构建**：合并连续页面/帧至256K tokens
4. **多语言扩展**：39种语言OCR支持

### 6.4 能力突破
1. **文本能力不退化**：在部分任务上超越纯文本模型
2. **长上下文理解**：256K原生支持，1M外推99.5%准确率
3. **多模态推理**：在STEM、视觉谜题等任务上领先
4. **工具增强感知**：通过工具提升细粒度理解能力

---

## 七、应用场景

### 7.1 当前能力
- 长文档分析（数百页技术文档）
- 长视频理解（2小时视频）
- 数学和STEM问题求解
- 代码生成和理解
- GUI交互和agent决策
- 多语言OCR和文档解析

### 7.2 未来方向
- **Embodied AI Agents**：
  - 感知和推理多模态输入
  - 在动态环境中执行上下文感知动作
  - 与用户交互
  - 操作数字界面
  - 通过多模态决策引导机器人系统

- **统一理解-生成架构**：
  - 探索视觉生成能力
  - 进一步提升整体智能

- **工具增强推理**：
  - 实时多模态控制
  - 交互式感知

---

## 八、开源信息

- **许可证**：Apache 2.0
- **模型下载**：
  - HuggingFace: https://huggingface.co/Qwen
  - ModelScope: https://modelscope.cn/organization/qwen
- **代码仓库**：https://github.com/QwenLM/Qwen3-VL
- **在线体验**：https://chat.qwen.ai

---

## 九、总结

Qwen3-VL通过**三大架构创新**（Interleaved MRoPE、DeepStack、Text-based Timestamps）、**系统化训练流程**（4阶段预训练+3阶段后训练）和**精心构建的多模态数据**，实现了：

1. **SOTA多模态性能**：在多数benchmark上超越GPT-5、Gemini 2.5 Pro和Claude Opus 4.1
2. **文本能力保持**：在部分推理任务上超越纯文本模型
3. **超长上下文**：256K原生支持，1M外推高准确率
4. **规模化部署**：从2B到235B全系列模型

这是一个真正实现**视觉-语言能力融合**的里程碑式模型，为多模态AI的未来发展奠定了坚实基础。