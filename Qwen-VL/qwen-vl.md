# Qwen-VL è¯¦ç»†æŠ€æœ¯åˆ†ææŠ¥å‘Š

## ä¸€ã€æ¨¡å‹æ¶æ„è¯¦è§£

### 1.1 æ•´ä½“æ¶æ„è®¾è®¡

Qwen-VLé‡‡ç”¨**ä¸‰ç»„ä»¶æ¶æ„**,æ€»å‚æ•°é‡ä¸º**9.6B**:

| ç»„ä»¶ | å‚æ•°é‡ | åˆå§‹åŒ–æ¥æº |
|------|--------|-----------|
| Visual Encoder (ViT) | 1.9B | OpenCLIP ViT-bigG |
| VL Adapter | 0.08B | éšæœºåˆå§‹åŒ– |
| Large Language Model | 7.7B | Qwen-7B |

### 1.2 æ ¸å¿ƒç»„ä»¶åˆ†æ

#### **Visual Encoder (è§†è§‰ç¼–ç å™¨)**
- æ¶æ„:Vision Transformer (ViT)
- åˆå§‹åŒ–:OpenCLIPçš„ViT-bigGé¢„è®­ç»ƒæƒé‡
- è¾“å…¥å¤„ç†:
  - Stage 1:224Ã—224åˆ†è¾¨ç‡
  - Stage 2-3:448Ã—448åˆ†è¾¨ç‡(æå‡ç»†ç²’åº¦ç†è§£)
  - Patch stride:14
  - è¾“å‡ºåºåˆ—é•¿åº¦:(448/14)Â² = 1024

#### **Position-aware Vision-Language Adapter (ä½ç½®æ„ŸçŸ¥é€‚é…å™¨)**
è¿™æ˜¯Qwen-VLçš„**å…³é”®åˆ›æ–°**:

```
è¾“å…¥:ViTè¾“å‡ºçš„1024é•¿åº¦ç‰¹å¾åºåˆ—
æ ¸å¿ƒæœºåˆ¶:å•å±‚Cross-Attention
- Query:256ä¸ªå¯å­¦ä¹ çš„embeddingå‘é‡
- Key/Value:æ¥è‡ªViTçš„å›¾åƒç‰¹å¾
- ä½ç½®ç¼–ç :2Dç»å¯¹ä½ç½®ç¼–ç èå…¥query-keyå¯¹
è¾“å‡º:å‹ç¼©åçš„256é•¿åº¦ç‰¹å¾åºåˆ—
```

**è®¾è®¡äº®ç‚¹**:
1. **ç‰¹å¾å‹ç¼©**:1024â†’256,æå‡æ•ˆç‡
2. **ä½ç½®ä¿ç•™**:é€šè¿‡2Dä½ç½®ç¼–ç ä¿ç•™ç©ºé—´ä¿¡æ¯,æ”¯æŒç»†ç²’åº¦ç†è§£
3. **æ¶ˆèå®éªŒ**:è®ºæ–‡æµ‹è¯•äº†64/144/256/400ä¸ªæŸ¥è¯¢å‘é‡,256ä¸ºæœ€ä¼˜

#### **Large Language Model**
- åŸºåº§:Qwen-7B(7.7Bå‚æ•°)
- ç‰¹æ®Štokenæ‰©å±•:
  - å›¾åƒæ ‡è®°:`<img>` `</img>`
  - è¾¹ç•Œæ¡†:`<box>` `</box>`
  - å¼•ç”¨æ ‡è®°:`<ref>` `</ref>`
  - å¯¹è¯æ ‡è®°:`<im_start>` `<im_end>`

### 1.3 è¾“å…¥è¾“å‡ºæ¥å£

#### **è¾¹ç•Œæ¡†è¡¨ç¤ºåˆ›æ–°**
```
å½’ä¸€åŒ–åæ ‡:[0, 1000)èŒƒå›´
æ ¼å¼:
- è¾“å…¥/è¾“å‡º:"<box>(Xtopleft,Ytopleft),(Xbottomright,Ybottomright)</box>"
- å…³è”æ–‡æœ¬:"<ref>æè¿°æ–‡æœ¬</ref><box>...</box>"
```

**ä¼˜åŠ¿**:
- æ— éœ€é¢å¤–ä½ç½®è¯æ±‡è¡¨
- ç›´æ¥tokenizationä¸ºæ–‡æœ¬
- æ”¯æŒä¸­è‹±æ–‡grounding

---

## äºŒã€è®­ç»ƒæ•°æ®è¯¦è§£

### 2.1 ä¸‰é˜¶æ®µè®­ç»ƒæ•°æ®

#### **Stage 1: é¢„è®­ç»ƒ(1.4Bæ ·æœ¬)**

æ•°æ®æ¸…æ´—ç­–ç•¥:
1. è¿‡æ»¤è¿‡å¤§å®½é«˜æ¯”å›¾åƒ
2. ç§»é™¤è¿‡å°å›¾åƒ
3. æ•°æ®é›†ç‰¹å®šçš„CLIPåˆ†æ•°è¿‡æ»¤
4. ç§»é™¤éè‹±æ–‡/ä¸­æ–‡å­—ç¬¦
5. æ¸…ç†HTMLæ ‡ç­¾å’Œä¸è§„åˆ™æ¨¡å¼

| æ•°æ®æº | åŸå§‹é‡ | æ¸…æ´—å | ä¿ç•™ç‡ | è¯­è¨€ |
|--------|--------|--------|--------|------|
| LAION-en | 2B | 280M | 14% | è‹±æ–‡ |
| LAION-COCO | 600M | 300M | 50% | è‹±æ–‡ |
| DataComp | 1.4B | 300M | 21% | è‹±æ–‡ |
| Coyo | 700M | 200M | 28% | è‹±æ–‡ |
| CC12M | 12M | 8M | 66% | è‹±æ–‡ |
| CC3M | 3M | 3M | 100% | è‹±æ–‡ |
| SBU | 1M | 0.8M | 80% | è‹±æ–‡ |
| COCO Caption | 0.6M | 0.6M | 100% | è‹±æ–‡ |
| LAION-zh | 108M | 105M | 97% | ä¸­æ–‡ |
| In-house Data | 220M | 220M | 100% | ä¸­æ–‡ |
| **æ€»è®¡** | **5B** | **1.4B** | **28%** | 77.3%è‹±æ–‡/22.7%ä¸­æ–‡ |

#### **Stage 2: å¤šä»»åŠ¡é¢„è®­ç»ƒ**

7å¤§ä»»åŠ¡å¹¶è¡Œè®­ç»ƒ:

| ä»»åŠ¡ç±»å‹ | æ ·æœ¬æ•° | æ•°æ®é›† |
|---------|--------|-------|
| **Captioning** | 19.7M | LAION-en/zh, DataComp, Coyo, CC12M/3M, SBU, COCO |
| **VQA** | 3.6M | GQA, VGQA, VQAv2, DVQA, OCR-VQA, DocVQA, TextVQA, ChartQA, AI2D |
| **Grounding** | 3.5M | GRIT (ç”Ÿæˆåè¯/çŸ­è¯­grounded captions) |
| **Ref Grounding** | 8.7M | GRIT, Visual Genome, RefCOCOç³»åˆ— |
| **Grounded Cap** | 8.7M | åŒRef Grounding |
| **OCR** | 24.8M | **SynthDoG-en/zh**, Common Crawl PDF/HTML |
| **çº¯æ–‡æœ¬** | 7.8M | In-houseæ•°æ®(é˜²æ­¢ç¾éš¾æ€§é—å¿˜) |

**OCRæ•°æ®ç”Ÿæˆ**:
- èƒŒæ™¯:COCO train2017/unlabeled2017
- å­—ä½“:41ç§è‹±æ–‡ + 11ç§ä¸­æ–‡
- å·¥å…·:SynthDoG(Kim et al., 2022)
- åæ ‡æ ‡æ³¨:å››è¾¹å½¢åæ ‡

#### **Stage 3: æŒ‡ä»¤å¾®è°ƒ(350Kæ ·æœ¬)**

æ•°æ®æ„æˆ:
1. **LLMè‡ªæŒ‡ä»¤ç”Ÿæˆ**:caption/å¯¹è¯æ•°æ®
2. **äººå·¥æ ‡æ³¨**:å®šä½+å¤šå›¾ç†è§£å¯¹è¯
3. **æ¨¡å‹ç”Ÿæˆ**:ç»“åˆç­–ç•¥æ‹¼æ¥
4. **çº¯æ–‡æœ¬å¯¹è¯**:ä¿æŒé€šç”¨å¯¹è¯èƒ½åŠ›

### 2.2 æ•°æ®æ ¼å¼

**Multi-taské¢„è®­ç»ƒæ ¼å¼**:
```
Image Captioning:
<img>path</img>Generate the caption in English: [caption]<eos>

VQA:
<img>path</img> [question] Answer: [answer]<eos>

Caption with Grounding:
<img>path</img>Generate the caption in English with grounding:
<ref>object</ref><box>(x1,y1),(x2,y2)</box>...<eos>

OCR:
<img>path</img>OCR with grounding: 
<ref>text</ref><quad>(x1,y1),(x2,y2),(x3,y3),(x4,y4)</quad>...<eos>
```

**SFTå¯¹è¯æ ¼å¼(ChatML)**:
```
<im_start>user
Picture 1: <img>path</img>é—®é¢˜å†…å®¹<im_end>
<im_start>assistant
å›ç­”å†…å®¹<im_end>
```

---

## ä¸‰ã€è®­ç»ƒLossä¸ä¼˜åŒ–ç­–ç•¥

### 3.1 è®­ç»ƒç›®æ ‡

**ç»Ÿä¸€Loss**:æ ‡å‡†è¯­è¨€å»ºæ¨¡äº¤å‰ç†µæŸå¤±
```
Loss = CrossEntropy(text_tokens)
```

**å…³é”®è®¾è®¡**:
- ä»…å¯¹æ–‡æœ¬tokenè®¡ç®—loss
- å›¾åƒç‰¹å¾åºåˆ—ä¸å‚ä¸lossè®¡ç®—
- SFTé˜¶æ®µ:ä»…ç›‘ç£å›ç­”å’Œç‰¹æ®Štoken,ä¸ç›‘ç£è§’è‰²åå’Œé—®é¢˜

### 3.2 è®­ç»ƒè¶…å‚æ•°

#### **Stage 1: é¢„è®­ç»ƒ**
```python
Optimizer: AdamW (Î²1=0.9, Î²2=0.98, eps=1e-6)
Learning Rate: 
  - Peak: 2e-4
  - Min: 1e-6
  - Schedule: Cosine decay
  - Warmup: 500 steps
  - ViT layer-wise decay: 0.95

Batch Size: 30720
Training Steps: 50,000 (~1.5Bæ ·æœ¬)
Weight Decay: 0.05
Gradient Clip: 1.0
Precision: BFloat16
Trainable: ViT + VL Adapter (LLMå†»ç»“)
```

#### **Stage 2: å¤šä»»åŠ¡é¢„è®­ç»ƒ**
```python
Learning Rate:
  - Peak: 5e-5
  - Min: 1e-5
  - Warmup: 400 steps

Batch Size: 4096
Training Steps: 19,000
Model Parallelism: 2 (ViT + LLM)
Trainable: å…¨æ¨¡å‹è§£å†»
Resolution: 448Ã—448
```

#### **Stage 3: SFT**
```python
Learning Rate:
  - Peak: 1e-5
  - Min: 1e-6
  - Warmup: 3,000 steps

Batch Size: 128
Training Steps: 8,000
Trainable: LLM + VL Adapter (ViTå†»ç»“)
Data: 350Kå¯¹è¯æ ·æœ¬
```

### 3.3 æ”¶æ•›æ›²çº¿åˆ†æ

è®ºæ–‡Figure 6æ˜¾ç¤º:
- **Lossç¨³å®šä¸‹é™**:ä»3.0é™è‡³1.8
- **Captionèƒ½åŠ›æå‡**:Flickr CIDErä»62â†’76
- **Zero-shot VQAæ¶Œç°**:VQAv2ä»48%â†’56%(æ— VQAè®­ç»ƒæ•°æ®!)

---

## å››ã€è¯„ä¼°ä»»åŠ¡ä¸æŒ‡æ ‡ä½“ç³»

### 4.1 æ ‡å‡†Benchmarkè¯„ä¼°

#### **å›¾åƒæè¿°(Image Captioning)**

| Benchmark | Metric | Qwen-VL | SOTAå¯¹æ¯” |
|-----------|--------|---------|----------|
| Nocaps (val) | CIDErâ†‘ | 121.4 | InstructBLIP:121.9 |
| **Flickr30K** | CIDErâ†‘ | **85.8** | Flamingo-80B:67.2 |

#### **é€šç”¨VQA**

| Benchmark | Metric | Qwen-VL | Qwen-VL-Chat | Specialist SOTA |
|-----------|--------|---------|--------------|-----------------|
| **VQAv2** | Accâ†‘ | **79.5** | 78.2 | PALI-X-55B:86.1 |
| **OKVQA** | Accâ†‘ | **58.6** | 56.6 | PALI-X-55B:66.1 |
| **GQA** | EMâ†‘ | **59.3** | 57.5 | CFR:72.1 |
| ScienceQA-Img | Accâ†‘ | 67.1 | **68.2** | LLaVA+GPT4:92.5 |
| VizWiz | VQA Scoreâ†‘ | 35.2 | **38.9** | PALI-X-55B:70.9 |

#### **æ–‡æœ¬å¯¼å‘VQA**

| Benchmark | Metric | Qwen-VL | å¯¹æ¯”æ¨¡å‹æœ€ä½³ |
|-----------|--------|---------|-------------|
| **TextVQA** | VQA Scoreâ†‘ | **63.8** | InstructBLIP:50.7 |
| DocVQA | ANLSâ†‘ | 65.1 | Pix2Struct-Large:76.6 |
| **ChartQA** | Relaxed EMâ†‘ | **65.7** | Pix2Struct-Large:58.6 |
| **AI2D** | EMâ†‘ | **62.3** | Pix2Struct-Large:42.1 |
| **OCR-VQA** | EMâ†‘ | **75.7** | Pix2Struct-Large:71.3 |

**å…³é”®å‘ç°**:Qwen-VLåœ¨448åˆ†è¾¨ç‡ä¸‹è¶…è¶Š1024åˆ†è¾¨ç‡çš„Pix2Struct

#### **Referring Expression Comprehension**

åœ¨RefCOCO/RefCOCO+/RefCOCOg/GRITä¸Š**å…¨é¢SOTA**(generalistæ¨¡å‹):

| Dataset | Metric | Qwen-VL-7B | Shikra-13B |
|---------|--------|-----------|------------|
| RefCOCO val | Accâ†‘ | **89.36** | 87.83 |
| RefCOCO+ testB | Accâ†‘ | **77.21** | 74.41 |
| RefCOCOg test | Accâ†‘ | **85.48** | 83.16 |
| GRIT refexp | Accâ†‘ | **78.22** | 69.03 |

### 4.2 Few-shotå­¦ä¹ 

åœ¨OKVQA/VizWiz/TextVQA/Flickr30Kä¸Š:
- **Qwen-VL-9B** â‰ˆ **Flamingo-80B** (8.9xå‚æ•°ä¼˜åŠ¿)
- 4-shotæ€§èƒ½æ˜¾è‘—æå‡

### 4.3 æŒ‡ä»¤éµå¾ªè¯„ä¼°

#### **TouchStone**
| Model | è‹±æ–‡å¾—åˆ† | ä¸­æ–‡å¾—åˆ† |
|-------|---------|---------|
| mPLUG-Owl | 605.4 | - |
| Qwen-VL-Chat | **645.2** | **401.2** |
| Qwen-VL-Chat-1.1 | **711.6** | **481.7** |

#### **SEED-Bench**
| Model | Image | Video | All |
|-------|-------|-------|-----|
| InstructBLIP | 58.8 | 38.1 | 53.4 |
| Qwen-VL | **62.3** | **39.1** | **56.3** |
| Qwen-VL-Chat | **65.4** | 37.8 | **58.2** |

#### **MME**
| Model | Perception | Cognition |
|-------|-----------|-----------|
| InstructBLIP | 1212.82 | 291.79 |
| Qwen-VL-Chat | **1487.58** | **360.71** |

---

## äº”ã€ä»£ç å®ç°åˆ†æ

### 5.1 æ ¸å¿ƒä»£ç ç»“æ„

æ ¹æ®finetune.py:312-315:
```python
if training_args.fix_vit and hasattr(model,'transformer') and hasattr(model.transformer,'visual'):
    model.transformer.visual.requires_grad_(False)
    if hasattr(model.transformer.visual,'attn_pool'):
        model.transformer.visual.attn_pool.requires_grad_(True)
```

**è®­ç»ƒç­–ç•¥**:
- **Stage 1**: ViT + VL Adapterå¯è®­ç»ƒ,LLMå†»ç»“
- **Stage 2**: å…¨éƒ¨è§£å†»
- **Stage 3(SFT)**: LLM + VL Adapterå¯è®­ç»ƒ,ViTå†»ç»“(**VL Adapterä¸­çš„attn_poolé™¤å¤–**)

### 5.2 æ•°æ®å¤„ç†(finetune.py:124-175)

**é¢„å¤„ç†æµç¨‹**:
```python
1. è§’è‰²æ˜ å°„:
   roles = {"user": "<|im_start|>user", "assistant": "<|im_start|>assistant"}

2. Token IDä½¿ç”¨:
   - im_start = tokenizer.im_start_id
   - im_end = tokenizer.im_end_id
   
3. Lossæ©ç ç­–ç•¥:
   - System prompt: IGNORE_TOKEN_ID
   - Userè¾“å…¥: IGNORE_TOKEN_ID
   - Assistantå›ç­”: æ­£å¸¸tokenè®¡ç®—loss
   - ç‰¹æ®Štoken(<im_end>ç­‰): è®¡ç®—loss
```

### 5.3 LoRA/Q-LoRAæ”¯æŒ

#### **LoRAé…ç½®**(finetune.py:54-63)
```python
lora_r: 64
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ["c_attn", "attn.c_proj", "w1", "w2"]
```

#### **æ˜¾å­˜å¯¹æ¯”**(å•A100-80G,batch_size=1):

| æ–¹æ³• | 384 tokens | 2048 tokens |
|------|-----------|-------------|
| LoRA (Base) | 37.1G / 2.3s | 38.7G / 6.1s |
| LoRA (Chat) | 23.3G / 2.2s | 27.3G / 5.9s |
| Q-LoRA | 17.0G / 4.2s | 19.3G / 7.9s |

**å…³é”®å·®å¼‚**:
- **LoRA (Base)**:éœ€è®­ç»ƒembedding + output layer(æ–°å¢special tokens)
- **LoRA (Chat)**:æ— éœ€è®­ç»ƒè¿™äº›å±‚(å·²åŒ…å«ChatML tokens)
- **Q-LoRA**:4-bité‡åŒ–,æœ€çœæ˜¾å­˜ä½†é€Ÿåº¦è¾ƒæ…¢

### 5.4 è¯„ä¼°è„šæœ¬

æ ¹æ®README:
- æ‰€æœ‰benchmarkè¯„ä¼°è„šæœ¬ä½äº`eval_mm/`
- æ”¯æŒçš„è¯„ä¼°:
  - Image Captioning: eval_mm/evaluate_caption.py
  - VQA: eval_mm/evaluate_vqa.py
  - Grounding: eval_mm/evaluate_grounding.py
  - Multiple Choice: eval_mm/evaluate_multiple_choice.py
  - MME: eval_mm/mme/eval.py
  - SEED-Bench: eval_mm/seed_bench/eval.py

---

## å…­ã€æ ¸å¿ƒåˆ›æ–°ç‚¹æ€»ç»“

### 6.1 æ¶æ„åˆ›æ–°
1. **Position-aware Adapter**:2Dä½ç½®ç¼–ç  + Cross-Attentionå‹ç¼©
2. **ç»Ÿä¸€æ¥å£**:è¾¹ç•Œæ¡†ä½œä¸ºæ–‡æœ¬åºåˆ—,æ— éœ€ä¸“ç”¨è¯æ±‡è¡¨
3. **å¤šåˆ†è¾¨ç‡è®­ç»ƒ**:224â†’448æå‡ç»†ç²’åº¦èƒ½åŠ›

### 6.2 è®­ç»ƒåˆ›æ–°
1. **ä¸‰é˜¶æ®µè®­ç»ƒ**:é¢„è®­ç»ƒâ†’å¤šä»»åŠ¡â†’SFT,é€æ­¥å¢å¼ºèƒ½åŠ›
2. **OCRæ•°æ®åˆæˆ**:SynthDoGç”Ÿæˆ24.8Mé«˜è´¨é‡OCRæ ·æœ¬
3. **é˜²ç¾éš¾æ€§é—å¿˜**:Stage 2/3æ··åˆçº¯æ–‡æœ¬æ•°æ®

### 6.3 èƒ½åŠ›çªç ´
1. **é¦–ä¸ªæ”¯æŒä¸­æ–‡groundingçš„é€šç”¨æ¨¡å‹**
2. **Zero-shot grounding**:æœªè§ä¸­æ–‡groundingæ•°æ®ä¹Ÿèƒ½æ³›åŒ–
3. **å¤šå›¾äº¤é”™å¯¹è¯**:æ”¯æŒå›¾åƒæ¯”è¾ƒå’Œå¤šå›¾ç†è§£
4. **ç»†ç²’åº¦è¯†åˆ«**:448åˆ†è¾¨ç‡è¶…è¶Š1024åˆ†è¾¨ç‡æ¨¡å‹

---

## ä¸ƒã€æ€§èƒ½Benchmarkå¯¹æ¯”

### ç»¼åˆèƒ½åŠ›é›·è¾¾å›¾(è®ºæ–‡Figure 1)

åœ¨9å¤§ä»»åŠ¡ä¸Šä¸åŒè§„æ¨¡generalistæ¨¡å‹å¯¹æ¯”,Qwen-VLåœ¨**7/9ä»»åŠ¡ä¸Šé¢†å…ˆ**:
- âœ… é¢†å…ˆ:VQAv2, Flickr30K, RefCOCO, OKVQA, GQA, TextVQA, VizWiz
- â‰ˆ æŒå¹³:Nocaps
- âŒ è½å:AI2D(ä½†ä»æ˜¯generalistæ¨¡å‹æœ€ä½³)

### ä¸ä¸“ç”¨SOTAå¯¹æ¯”

| ä»»åŠ¡ | Qwen-VL | Specialist SOTA | å·®è· |
|------|---------|-----------------|------|
| Caption | 85.8 | 127.0 (PALI-17B) | -41.2 |
| VQAv2 | 79.5 | 86.1 (PALI-X-55B) | -6.6 |
| Grounding | 89.36 | 92.64 (UNINEXT-H) | -3.28 |
| OCR-VQA | 75.7 | 75.0 (PALI-X-55B) | **+0.7** |

**ç»“è®º**:åœ¨OCRä»»åŠ¡ä¸Šå·²**è¶…è¶Šä¸“ç”¨æ¨¡å‹**

---

## å…«ã€ä»£ç åº“åŠŸèƒ½å®Œæ•´æ€§

### 8.1 æ”¯æŒçš„åŠŸèƒ½

âœ… **æ¨ç†**:
- ğŸ¤— Transformersé›†æˆ
- ğŸ¤– ModelScopeé›†æˆ
- é‡åŒ–æ¨ç†(Int4)
- Web UI Demo(web_demo_mm.py)
- OpenAI APIå…¼å®¹(openai_api.py)

âœ… **å¾®è°ƒ**:
- Full-parameter finetuning
- LoRA (finetune/finetune_lora_ds.sh)
- Q-LoRA (finetune/finetune_qlora_ds.sh)
- DeepSpeed ZeRO-2/3æ”¯æŒ

âœ… **è¯„ä¼°**:
- å®Œæ•´benchmarkè¯„ä¼°è„šæœ¬(eval_mm/)
- TouchStoneè¯„ä¼°(touchstone/)
- MME/SEED-Benchè¯„ä¼°

### 8.2 é‡åŒ–æ€§èƒ½(Int4)

| Metric | BF16 | Int4 | å·®å¼‚ |
|--------|------|------|------|
| TouchStone-EN | 645.2 | 651.4 | +6.2 |
| TouchStone-CN | 401.2 | 386.6 | -14.6 |
| æ¨ç†é€Ÿåº¦(8192 tokens) | 24.32 tok/s | **34.34 tok/s** | +41% |
| æ˜¾å­˜(ç”Ÿæˆ8192 tokens) | 28.01GB | **17.23GB** | -38% |

**ç»“è®º**:Int4å‡ ä¹æ— æŸæ€§èƒ½,æ˜¾è‘—æå‡æ•ˆç‡

---

## ä¹ã€å±€é™æ€§ä¸æœªæ¥æ–¹å‘

### 9.1 å½“å‰å±€é™
1. **åˆ†è¾¨ç‡ä¸Šé™**:448Ã—448ç›¸æ¯”GPT-4Vçš„é«˜åˆ†è¾¨ç‡ä»æœ‰å·®è·
2. **ä¸“ç”¨ä»»åŠ¡å·®è·**:åœ¨æŸäº›ä»»åŠ¡ä¸Šä¸ä¸“ç”¨æ¨¡å‹ä»æœ‰6-40åˆ†å·®è·
3. **ä¸­æ–‡èƒ½åŠ›**:è™½é¢†å…ˆå¼€æºæ¨¡å‹,ä½†éƒ¨åˆ†ä»»åŠ¡å¼±äºGPT-4V/Gemini

### 9.2 è®ºæ–‡æå‡ºçš„æœªæ¥æ–¹å‘
1. âœ¨ å¤šæ¨¡æ€æ‰©å±•:é›†æˆè¯­éŸ³ã€è§†é¢‘
2. ğŸ“ˆ è§„æ¨¡å‡çº§:æ›´å¤§æ¨¡å‹ã€æ›´å¤šæ•°æ®ã€æ›´é«˜åˆ†è¾¨ç‡
3. ğŸ¨ ç”Ÿæˆèƒ½åŠ›:é«˜ä¿çœŸå›¾åƒç”Ÿæˆã€æµç•…è¯­éŸ³ç”Ÿæˆ

---

## åã€æ€»ç»“

Qwen-VLæ˜¯ä¸€ä¸ª**è®¾è®¡ç²¾å·§ã€è®­ç»ƒå……åˆ†ã€èƒ½åŠ›å…¨é¢**çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹:

**æ ¸å¿ƒä¼˜åŠ¿**:
1. ğŸ† åŒè§„æ¨¡generalistæ¨¡å‹æ€§èƒ½é¢†å…ˆ
2. ğŸŒ åŸç”Ÿä¸­è‹±åŒè¯­æ”¯æŒ
3. ğŸ¯ ç»†ç²’åº¦ç†è§£(grounding + OCR)
4. ğŸ’¡ æ¶æ„ç®€æ´é«˜æ•ˆ(9.6Bå‚æ•°)
5. ğŸ› ï¸ å®Œæ•´å·¥å…·é“¾(æ¨ç†/å¾®è°ƒ/è¯„ä¼°)

**é€‚ç”¨åœºæ™¯**:
- âœ… å¤šè¯­è¨€å›¾åƒç†è§£
- âœ… æ–‡æ¡£/å›¾è¡¨/OCRä»»åŠ¡
- âœ… ç»†ç²’åº¦å®šä½(grounding)
- âœ… å¤šå›¾å¯¹è¯äº¤äº’
- âœ… èµ„æºå—é™éƒ¨ç½²(Int4é‡åŒ–)

**ä»£ç è´¨é‡**:
- å®Œæ•´å¯å¤ç°çš„è®­ç»ƒæµç¨‹
- ä¸°å¯Œçš„å¾®è°ƒé€‰é¡¹(Full/LoRA/Q-LoRA)
- è¯¦ç»†çš„è¯„ä¼°è„šæœ¬
- æ¸…æ™°çš„æ•°æ®æ ¼å¼æ–‡æ¡£

è¿™æ˜¯ä¸€ä¸ª**ç”Ÿäº§çº§**çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹,é€‚åˆä½œä¸ºç ”ç©¶åŸºçº¿æˆ–å•†ä¸šåº”ç”¨çš„èµ·ç‚¹ã€‚

---

## ğŸ‘¤ User

æ—¢ç„¶ä»–æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¨¡å‹,èƒ½å¦ä»‹ç»ä¸€ä¸‹è§†è§‰éƒ¨åˆ†æ€ä¹ˆå¤„ç†,æ–‡æœ¬è¾“å…¥æ€ä¹ˆå¤„ç†,ä¹‹ååˆå¦‚ä½•å¾—åˆ°è¾“å‡º,ä¸‰ä¸ªé˜¶æ®µçš„æŸå¤±å‡½æ•°åˆ†å¸ƒæ˜¯ä»€ä¹ˆ?

---

## ğŸ¤– Claude

éå¸¸å¥½çš„é—®é¢˜ï¼è®©æˆ‘è¯¦ç»†è§£æQwen-VLçš„**å®Œæ•´æ•°æ®æµ**å’Œ**ä¸‰é˜¶æ®µæŸå¤±å‡½æ•°**ã€‚

---

# Qwen-VL æ•°æ®æµä¸æŸå¤±å‡½æ•°è¯¦è§£

## ä¸€ã€å®Œæ•´æ•°æ®æµæ¶æ„

### 1.1 æ€»ä½“æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input Imageâ”‚         â”‚  Input Text  â”‚
â”‚ (448Ã—448Ã—3) â”‚         â”‚  "è¿™æ˜¯ä»€ä¹ˆ?" â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                       â”‚
       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         STEP 1: ç‰¹å¾æå–é˜¶æ®µ              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                       â”‚
       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ViT-bigG   â”‚         â”‚  Tokenizer   â”‚
â”‚   Encoder    â”‚         â”‚  (Qwen-7B)   â”‚
â”‚   (1.9B)     â”‚         â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                        â”‚
       â”‚                        â–¼
       â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                 â”‚ Text Tokens  â”‚
       â”‚                 â”‚ [101, 256,..] â”‚
       â”‚                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚ Image Patchesâ”‚                â”‚
â”‚ (32Ã—32Ã—1536) â”‚                â”‚
â”‚ = 1024 tokensâ”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
       â”‚                        â”‚
       â–¼                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      STEP 2: è§†è§‰ç‰¹å¾å‹ç¼©é˜¶æ®µ             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                        â”‚
       â–¼                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  VL Adapter          â”‚        â”‚
â”‚  (Cross-Attention)   â”‚        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚        â”‚
â”‚  â”‚ Query: 256ä¸ª   â”‚  â”‚        â”‚
â”‚  â”‚ å¯å­¦ä¹ å‘é‡      â”‚  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚        â”‚
â”‚  â”‚ Key/Value:     â”‚  â”‚        â”‚
â”‚  â”‚ 1024ä¸ªpatchç‰¹å¾â”‚  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚        â”‚
â”‚  â”‚ 2D Position    â”‚  â”‚        â”‚
â”‚  â”‚ Encoding       â”‚  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
       â”‚                        â”‚
       â–¼                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚ å‹ç¼©åçš„å›¾åƒ  â”‚                â”‚
â”‚ ç‰¹å¾ (256ç»´)  â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
       â”‚                        â”‚
       â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      STEP 3: ç‰¹å¾æ‹¼æ¥ä¸æ ‡è®°é˜¶æ®µ           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                        â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Sequence Construction:    â”‚
   â”‚  <img> [256ä¸ªimage tokens] â”‚
   â”‚  </img> [text tokens]      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      STEP 4: LLMç”Ÿæˆé˜¶æ®µ                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Qwen-7B LLM  â”‚
   â”‚   (7.7B params)â”‚
   â”‚                â”‚
   â”‚  Transformer   â”‚
   â”‚  Decoder Layersâ”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Output Logits â”‚
   â”‚  (vocab_size)  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Generated     â”‚
   â”‚  Text:         â”‚
   â”‚  "è¿™æ˜¯ä¸€åªæ‹‰å¸ƒ â”‚
   â”‚   æ‹‰å¤šçŠ¬ã€‚"     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## äºŒã€è§†è§‰éƒ¨åˆ†è¯¦ç»†å¤„ç†æµç¨‹

### 2.1 å›¾åƒé¢„å¤„ç†

```python
# ä¼ªä»£ç ç¤ºæ„
def preprocess_image(image_path):
    """
    è¾“å…¥: PIL Image æˆ–å›¾åƒè·¯å¾„
    è¾“å‡º: tensor [3, 448, 448]
    """
    # 1. åŠ è½½å›¾åƒ
    image = load_image(image_path)
    
    # 2. Resizeåˆ°å›ºå®šåˆ†è¾¨ç‡
    # Stage 1: 224Ã—224
    # Stage 2/3: 448Ã—448
    image = resize(image, size=(448, 448))
    
    # 3. å½’ä¸€åŒ– (ImageNetç»Ÿè®¡é‡)
    mean = [0.48145466, 0.4578275, 0.40821073]
    std = [0.26862954, 0.26130258, 0.27577711]
    image = normalize(image, mean, std)
    
    return image  # shape: [3, 448, 448]
```

### 2.2 ViTç¼–ç è¿‡ç¨‹

```python
class VisionTransformer:
    """
    è¾“å…¥: [B, 3, 448, 448]
    è¾“å‡º: [B, 1024, 1536]  # 1024ä¸ªpatch, æ¯ä¸ª1536ç»´
    """
    def __init__(self):
        self.patch_size = 14
        self.patch_embed = Conv2d(3, 1536, kernel_size=14, stride=14)
        self.pos_embed = Parameter(torch.zeros(1, 1024, 1536))
        self.transformer_blocks = ModuleList([...])  # 48å±‚Transformer
        
    def forward(self, x):
        # x: [B, 3, 448, 448]
        
        # 1. Patch Embedding
        # (448/14) Ã— (448/14) = 32 Ã— 32 = 1024 patches
        x = self.patch_embed(x)  # [B, 1536, 32, 32]
        x = x.flatten(2).transpose(1, 2)  # [B, 1024, 1536]
        
        # 2. åŠ å…¥ä½ç½®ç¼–ç 
        x = x + self.pos_embed
        
        # 3. ç»è¿‡Transformerå±‚
        for block in self.transformer_blocks:
            x = block(x)  # Self-Attention + FFN
            
        return x  # [B, 1024, 1536]
```

**å…³é”®ç»†èŠ‚**:
- **Patchæ•°é‡**: (448/14)Â² = 1024ä¸ªpatches
- **ç‰¹å¾ç»´åº¦**: 1536 (ViT-bigGçš„hidden size)
- **æ— CLS token**: ç›´æ¥ä½¿ç”¨æ‰€æœ‰patch tokens

### 2.3 VL Adapterå‹ç¼©

```python
class PositionAwareVLAdapter(nn.Module):
    """
    è¾“å…¥: [B, 1024, 1536] (ViTè¾“å‡º)
    è¾“å‡º: [B, 256, 4096]  (LLMè¾“å…¥ç»´åº¦)
    """
    def __init__(self):
        self.num_queries = 256
        # å¯å­¦ä¹ çš„query embeddings
        self.query_tokens = Parameter(torch.randn(1, 256, 4096))
        
        # Cross-Attentionå±‚
        self.cross_attn = MultiheadAttention(
            embed_dim=4096,
            num_heads=32,
            kdim=1536,  # ViTç‰¹å¾ç»´åº¦
            vdim=1536
        )
        
        # 2Dä½ç½®ç¼–ç 
        self.position_encoding_2d = build_2d_sincos_position_embedding(
            h=32, w=32, embed_dim=1536
        )
        
    def forward(self, image_features):
        # image_features: [B, 1024, 1536]
        B = image_features.size(0)
        
        # 1. åŠ å…¥2Dä½ç½®ç¼–ç åˆ°Key
        pos_enc = self.position_encoding_2d  # [1024, 1536]
        keys = image_features + pos_enc.unsqueeze(0)
        
        # 2. Query: å¯å­¦ä¹ å‘é‡
        queries = self.query_tokens.expand(B, -1, -1)  # [B, 256, 4096]
        
        # 3. Cross-Attention
        # Q: [B, 256, 4096]
        # K, V: [B, 1024, 1536] (è‡ªåŠ¨æŠ•å½±åˆ°4096ç»´)
        output, _ = self.cross_attn(
            query=queries.transpose(0, 1),      # [256, B, 4096]
            key=keys.transpose(0, 1),           # [1024, B, 1536]
            value=image_features.transpose(0, 1)  # [1024, B, 1536]
        )
        
        output = output.transpose(0, 1)  # [B, 256, 4096]
        return output
```

**æ ¸å¿ƒæœºåˆ¶**:
1. **Query**: 256ä¸ªå¯å­¦ä¹ å‘é‡(ç±»ä¼¼DETRçš„object queries)
2. **Key/Value**: æ¥è‡ªViTçš„1024ä¸ªpatchç‰¹å¾
3. **ä½ç½®ä¿¡æ¯**: é€šè¿‡2Dä½ç½®ç¼–ç æ³¨å…¥åˆ°Keyä¸­
4. **è¾“å‡º**: 256ä¸ªå‹ç¼©åçš„è§†è§‰token,ç»´åº¦å¯¹é½LLM(4096)

---

## ä¸‰ã€æ–‡æœ¬éƒ¨åˆ†å¤„ç†æµç¨‹

### 3.1 Tokenization

```python
class QwenTokenizer:
    """
    åŸºäºQwen-7Bçš„tokenizer
    è¯æ±‡è¡¨å¤§å°: ~151,851
    """
    def __init__(self):
        # ç‰¹æ®Štoken
        self.img_start_id = 151857  # <img>
        self.img_end_id = 151858    # </img>
        self.im_start_id = 151644   # <im_start>
        self.im_end_id = 151645     # <im_end>
        self.box_start_id = 151859  # <box>
        self.box_end_id = 151860    # </box>
        self.ref_start_id = 151861  # <ref>
        self.ref_end_id = 151862    # </ref>
        
    def encode(self, text):
        """
        è¾“å…¥: "è¿™æ˜¯ä»€ä¹ˆ?"
        è¾“å‡º: [101, 256, 389, 102]
        """
        return self.tokenize(text)
    
    def from_list_format(self, content_list):
        """
        è¾“å…¥: [
            {'image': 'path/to/img.jpg'},
            {'text': 'è¿™æ˜¯ä»€ä¹ˆ?'}
        ]
        è¾“å‡º: "<img>path/to/img.jpg</img>è¿™æ˜¯ä»€ä¹ˆ?"
        """
        text = ""
        for item in content_list:
            if 'image' in item:
                text += f"<img>{item['image']}</img>"
            elif 'text' in item:
                text += item['text']
        return text
```

### 3.2 æ–‡æœ¬Embedding

```python
def text_embedding(text_tokens):
    """
    è¾“å…¥: [101, 256, 389, 102]  # token IDs
    è¾“å‡º: [B, seq_len, 4096]
    """
    # 1. Token Embedding
    token_embeds = embedding_layer(text_tokens)  # [B, seq_len, 4096]
    
    # 2. ä½ç½®ç¼–ç  (RoPE)
    # Qwenä½¿ç”¨RoPE (Rotary Position Embedding)
    # åœ¨Attentionå±‚ä¸­åŠ¨æ€è®¡ç®—ï¼Œæ­¤å¤„ä¸æ˜¾å¼æ·»åŠ 
    
    return token_embeds
```

---

## å››ã€å¤šæ¨¡æ€èåˆæœºåˆ¶

### 4.1 åºåˆ—æ„é€ 

```python
def construct_multimodal_sequence(image_features, text_tokens):
    """
    è¾“å…¥:
      - image_features: [B, 256, 4096] (æ¥è‡ªVL Adapter)
      - text_tokens: [B, text_len]
    
    è¾“å‡º:
      - input_embeds: [B, 256 + 2 + text_len, 4096]
    """
    B = image_features.size(0)
    
    # 1. è·å–ç‰¹æ®Štokençš„embedding
    img_start_embed = embedding_layer(torch.tensor([151857]))  # <img>
    img_end_embed = embedding_layer(torch.tensor([151858]))    # </img>
    
    # 2. è·å–æ–‡æœ¬embedding
    text_embeds = embedding_layer(text_tokens)  # [B, text_len, 4096]
    
    # 3. æ‹¼æ¥åºåˆ—
    # æ ¼å¼: <img> [256ä¸ªimage tokens] </img> [text tokens]
    input_embeds = torch.cat([
        img_start_embed.expand(B, 1, -1),  # [B, 1, 4096]
        image_features,                     # [B, 256, 4096]
        img_end_embed.expand(B, 1, -1),    # [B, 1, 4096]
        text_embeds                         # [B, text_len, 4096]
    ], dim=1)
    
    # æ€»é•¿åº¦: 1 + 256 + 1 + text_len = 258 + text_len
    return input_embeds
```

**åºåˆ—ç¤ºä¾‹**:
```
è¾“å…¥: å›¾åƒ + "è¿™æ˜¯ä»€ä¹ˆ?"

Tokenåºåˆ—:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ <img> â”‚ [256ä¸ªè§†è§‰token]      â”‚ </img> â”‚ è¿™ æ˜¯ ä»€ ä¹ˆ ? â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  ç‰¹æ®Š   â”‚    æ¥è‡ªVL Adapter     â”‚  ç‰¹æ®Š  â”‚   æ–‡æœ¬tokens   â”‚
  token  â”‚    çš„å›¾åƒç‰¹å¾          â”‚  token â”‚                â”‚
         â””â”€â”€â”€â”€â”€â”€â”€ 258 tokens â”€â”€â”€â”€â”€â”˜

Embeddingç»´åº¦:
æ¯ä¸ªä½ç½®: [4096ç»´å‘é‡]
```

### 4.2 LLMå¤„ç†

```python
class QwenLMWithVision(nn.Module):
    def forward(self, input_embeds):
        """
        è¾“å…¥: [B, seq_len, 4096]
        è¾“å‡º: [B, seq_len, vocab_size]
        """
        # 1. ç»è¿‡Transformer Decoderå±‚
        hidden_states = input_embeds
        
        for layer in self.transformer_layers:  # 32å±‚
            # Self-Attention (å¸¦RoPEä½ç½®ç¼–ç )
            hidden_states = layer.self_attn(hidden_states)
            # Feed-Forward
            hidden_states = layer.ffn(hidden_states)
        
        # 2. è¾“å‡ºæŠ•å½±
        logits = self.lm_head(hidden_states)  # [B, seq_len, 151851]
        
        return logits
```

**å…³é”®ç‚¹**:
- **å›¾åƒtokenå‚ä¸Attention**: è§†è§‰ç‰¹å¾ä¸æ–‡æœ¬tokené€šè¿‡Self-Attentionäº¤äº’
- **ä½ç½®ç¼–ç **: RoPEä¸ºæ‰€æœ‰token(åŒ…æ‹¬å›¾åƒ)æä¾›ä½ç½®ä¿¡æ¯
- **Causal Mask**: è®­ç»ƒæ—¶ä½¿ç”¨å› æœæ©ç ,ç¡®ä¿è‡ªå›å½’ç”Ÿæˆ

---

## äº”ã€ä¸‰é˜¶æ®µæŸå¤±å‡½æ•°è¯¦è§£

### 5.1 Stage 1: é¢„è®­ç»ƒé˜¶æ®µ

#### **æŸå¤±å‡½æ•°**

```python
def stage1_loss(logits, labels, image_mask):
    """
    ä»…å¯¹æ–‡æœ¬tokenè®¡ç®—æŸå¤±
    
    å‚æ•°:
      - logits: [B, seq_len, vocab_size]
      - labels: [B, seq_len]  # ground truth tokens
      - image_mask: [B, seq_len]  # æ ‡è®°å“ªäº›ä½ç½®æ˜¯å›¾åƒtoken
    
    è¿”å›:
      - loss: æ ‡é‡
    """
    # 1. æ„é€ æŸå¤±æ©ç 
    # å›¾åƒtokenä½ç½®: IGNORE (-100)
    # æ–‡æœ¬tokenä½ç½®: æ­£å¸¸è®¡ç®—
    loss_mask = (image_mask == 0)  # False for image, True for text
    
    # 2. å°†å›¾åƒtokenä½ç½®çš„labelè®¾ä¸º-100
    masked_labels = labels.clone()
    masked_labels[~loss_mask] = -100
    
    # 3. è®¡ç®—Cross-Entropy Loss
    # shift prediction: é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
    shift_logits = logits[:, :-1, :].contiguous()
    shift_labels = masked_labels[:, 1:].contiguous()
    
    loss_fct = CrossEntropyLoss(ignore_index=-100)
    loss = loss_fct(
        shift_logits.view(-1, shift_logits.size(-1)),
        shift_labels.view(-1)
    )
    
    return loss
```

#### **æŸå¤±è®¡ç®—ç¤ºä¾‹**

```
åºåˆ—:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚ <img> â”‚ [256 img toks] â”‚ </img> â”‚ è¿™  â”‚ æ˜¯ â”‚ ä¸€ â”‚ åª â”‚ ç‹— â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜

Labels (è¦é¢„æµ‹çš„ä¸‹ä¸€ä¸ªtoken):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ [256 img toks] â”‚ </img> â”‚ è¿™  â”‚ æ˜¯ â”‚ ä¸€ â”‚ åª â”‚ ç‹— â”‚ EOS â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

Loss Mask (å“ªäº›ä½ç½®è®¡ç®—æŸå¤±):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚    IGNORE      â”‚ IGNORE â”‚  âœ“  â”‚ âœ“ â”‚ âœ“ â”‚ âœ“ â”‚ âœ“ â”‚  âœ“  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
                           â””â”€â”€â”€â”€â”€â”€ ä»…è¿™äº›ä½ç½®è®¡ç®—loss â”€â”€â”€â”€â”€â”˜

å®é™…Loss:
loss = CE(pred_è¿™, label_è¿™) + CE(pred_æ˜¯, label_æ˜¯) + ... 
       + CE(pred_EOS, label_EOS)
```

**è®­ç»ƒç­–ç•¥**:
- **å†»ç»“**: LLMå…¨éƒ¨å‚æ•°
- **è®­ç»ƒ**: ViT + VL Adapter
- **æ•°æ®**: 1.4Bå›¾æ–‡å¯¹
- **ç›®æ ‡**: è®©è§†è§‰ç‰¹å¾å¯¹é½åˆ°LLMçš„è¯­ä¹‰ç©ºé—´

---

### 5.2 Stage 2: å¤šä»»åŠ¡é¢„è®­ç»ƒ

#### **æŸå¤±å‡½æ•° (æœ¬è´¨ç›¸åŒ,ä½†æ•°æ®æ ¼å¼å¤šæ ·)**

```python
def stage2_loss(logits, labels, task_type):
    """
    ä¸åŒä»»åŠ¡ä½¿ç”¨ç›¸åŒçš„losså‡½æ•°,ä½†labelæ„é€ ä¸åŒ
    """
    # åŸºç¡€lossä»æ˜¯Cross-Entropy
    loss = CrossEntropyLoss(ignore_index=-100)
    
    # æ ¹æ®ä»»åŠ¡ç±»å‹,labelæ ¼å¼ä¸åŒ:
    if task_type == "caption":
        # <img>...</img>Generate the caption: [caption]<eos>
        # ä»…å¯¹captionéƒ¨åˆ†è®¡ç®—loss
        
    elif task_type == "vqa":
        # <img>...</img>[question] Answer: [answer]<eos>
        # ä»…å¯¹answeréƒ¨åˆ†è®¡ç®—loss
        
    elif task_type == "grounding":
        # <img>...</img>Generate with grounding: 
        # <ref>text</ref><box>(x,y),(x,y)</box>...<eos>
        # å¯¹æ•´ä¸ªgrounded captionè®¡ç®—loss
        
    elif task_type == "ocr":
        # <img>...</img>OCR: <ref>text</ref><quad>...</quad><eos>
        # å¯¹OCRæ–‡æœ¬å’Œåæ ‡éƒ½è®¡ç®—loss
        
    return loss(logits, labels)
```

#### **å„ä»»åŠ¡çš„Lossè®¡ç®—**

**Captionä»»åŠ¡**:
```
è¾“å…¥åºåˆ—:
<img>[256 toks]</img>Generate the caption in English:

ç›®æ ‡åºåˆ— (è®¡ç®—loss):
                                                     A woman playing with her dog.<eos>
                                                     â””â”€â”€â”€â”€â”€â”€ ä»…æ­¤éƒ¨åˆ†å‚ä¸loss â”€â”€â”€â”€â”€â”€â”˜
```

**Groundingä»»åŠ¡**:
```
è¾“å…¥åºåˆ—:
<img>[256 toks]</img>Generate with grounding:

ç›®æ ‡åºåˆ— (è®¡ç®—loss):
                                            <ref>woman</ref><box>(100,200),(300,400)</box>
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ–‡æœ¬+åæ ‡éƒ½å‚ä¸loss â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**VQAä»»åŠ¡**:
```
è¾“å…¥åºåˆ—:
<img>[256 toks]</img>What is in the image? Answer:

ç›®æ ‡åºåˆ—:
                                                        A dog.<eos>
                                                        â””â”€ loss â”€â”˜
```

**è®­ç»ƒç­–ç•¥**:
- **è§£å†»**: å…¨æ¨¡å‹å‚æ•°å¯è®­ç»ƒ
- **åˆ†è¾¨ç‡**: 448Ã—448
- **æ•°æ®**: 70M+ å¤šä»»åŠ¡æ ·æœ¬
- **ç›®æ ‡**: å¢å¼ºå¤šä»»åŠ¡èƒ½åŠ›,å¼•å…¥groundingå’ŒOCR

---

### 5.3 Stage 3: æŒ‡ä»¤å¾®è°ƒ

#### **æŸå¤±å‡½æ•° (ChatMLæ ¼å¼)**

```python
def stage3_sft_loss(logits, labels, role_tokens):
    """
    ä»…å¯¹assistantå›ç­”è®¡ç®—æŸå¤±
    
    ChatMLæ ¼å¼:
    <im_start>user\n{user_input}<im_end>\n
    <im_start>assistant\n{assistant_output}<im_end>\n
    """
    masked_labels = labels.clone()
    
    # 1. System prompt â†’ IGNORE
    # 2. User input â†’ IGNORE
    # 3. Assistant output â†’ è®¡ç®—loss
    # 4. ç‰¹æ®Štoken (<im_end>ç­‰) â†’ è®¡ç®—loss
    
    for i, role in enumerate(role_tokens):
        if role == "user" or role == "system":
            # æ‰¾åˆ°è¿™æ®µçš„èµ·å§‹å’Œç»“æŸ
            start_idx = find_role_start(i)
            end_idx = find_role_end(i)
            # é™¤äº†<im_end>,å…¶ä½™éƒ½IGNORE
            masked_labels[start_idx:end_idx-1] = -100
            
        elif role == "assistant":
            # ä¿ç•™è¿™æ®µç”¨äºè®¡ç®—loss
            # ä½†role tokenæœ¬èº«è¦IGNORE
            role_token_idx = find_role_token_idx(i)
            masked_labels[role_token_idx] = -100
    
    loss = CrossEntropyLoss(ignore_index=-100)
    return loss(logits, masked_labels)
```

#### **SFT Lossè®¡ç®—ç¤ºä¾‹**

```
å®Œæ•´åºåˆ—:
<im_start>system
You are a helpful assistant.<im_end>
<im_start>user
Picture 1: <img>[256 toks]</img>
è¿™æ˜¯ä»€ä¹ˆ?<im_end>
<im_start>assistant
è¿™æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚<im_end>

Loss Mask:
<im_start>system                              â† IGNORE
You are a helpful assistant.                  â† IGNORE
<im_end>                                       â† âœ“ è®¡ç®—loss (ç‰¹æ®Štoken)

<im_start>user                                 â† IGNORE
Picture 1: <img>[256 toks]</img>è¿™æ˜¯ä»€ä¹ˆ?      â† IGNORE
<im_end>                                       â† âœ“ è®¡ç®—loss

<im_start>assistant                            â† IGNORE (role token)
è¿™æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚                             â† âœ“âœ“âœ“ è®¡ç®—loss
<im_end>                                       â† âœ“ è®¡ç®—loss

å®é™…Lossä»…æ¥è‡ª:
1. ç‰¹æ®Štoken: <im_end> (3æ¬¡)
2. Assistantå›ç­”: "è¿™æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚"
```

**ä»£ç å®ç°** (æ¥è‡ªfinetune.py:124-175):

```python
def preprocess(sources, tokenizer, max_len):
    im_start = tokenizer.im_start_id
    im_end = tokenizer.im_end_id
    
    for source in sources:
        input_id, target = [], []
        
        # System prompt
        system = [im_start] + _system + system_content + [im_end]
        input_id += system
        target += [im_start] + [IGNORE_TOKEN_ID]*(len(system)-3) + [im_end]
        
        # User/Assistant turns
        for sentence in source:
            role = sentence["from"]
            _input_id = tokenizer(role).input_ids + \
                        tokenizer(sentence["value"]).input_ids + [im_end]
            input_id += _input_id
            
            if role == 'user':
                # Userè¾“å…¥: å…¨éƒ¨IGNORE,é™¤äº†<im_end>
                _target = [im_start] + [IGNORE_TOKEN_ID]*(len(_input_id)-3) + [im_end]
            elif role == 'assistant':
                # Assistantå›ç­”: è®¡ç®—loss,ä½†role tokenè¦IGNORE
                _target = [im_start] + [IGNORE_TOKEN_ID]*len(role_tokens) + \
                          _input_id[len(role_tokens)+1:-2] + [im_end]
            
            target += _target
    
    return input_id, target
```

**è®­ç»ƒç­–ç•¥**:
- **å†»ç»“**: ViT
- **è®­ç»ƒ**: LLM + VL Adapter
- **æ•°æ®**: 350Kå¯¹è¯æ ·æœ¬
- **ç›®æ ‡**: å¯¹é½äººç±»åå¥½,æå‡å¯¹è¯èƒ½åŠ›

---

## å…­ã€ä¸‰é˜¶æ®µå¯¹æ¯”æ€»ç»“

| ç»´åº¦ | Stage 1 | Stage 2 | Stage 3 |
|------|---------|---------|---------|
| **åç§°** | é¢„è®­ç»ƒ | å¤šä»»åŠ¡é¢„è®­ç»ƒ | æŒ‡ä»¤å¾®è°ƒ |
| **åˆ†è¾¨ç‡** | 224Ã—224 | 448Ã—448 | 448Ã—448 |
| **å¯è®­ç»ƒå‚æ•°** | ViT + Adapter | å…¨éƒ¨ | LLM + Adapter |
| **æ•°æ®é‡** | 1.4B | 70M+ | 350K |
| **æ•°æ®ç±»å‹** | å›¾æ–‡å¯¹ | 7ç§ä»»åŠ¡ | å¯¹è¯æ•°æ® |
| **LossèŒƒå›´** | æ–‡æœ¬caption | ä»»åŠ¡ç­”æ¡ˆ | Assistantå›ç­” |
| **Losså‡½æ•°** | CrossEntropy | CrossEntropy | CrossEntropy |
| **ç‰¹æ®Šä¹‹å¤„** | å›¾åƒtokenä¸è®¡ç®—loss | ä¸åŒä»»åŠ¡labelæ ¼å¼ä¸åŒ | user/systeméƒ¨åˆ†IGNORE |
| **è®­ç»ƒæ­¥æ•°** | 50K | 19K | 8K |
| **Batch Size** | 30720 | 4096 | 128 |
| **Peak LR** | 2e-4 | 5e-5 | 1e-5 |

---

## ä¸ƒã€å®Œæ•´æ¨ç†è¿‡ç¨‹ç¤ºä¾‹

è®©æˆ‘ç”¨ä¸€ä¸ªå®Œæ•´ä¾‹å­ä¸²è”æ•´ä¸ªæµç¨‹:

### è¾“å…¥
```python
image = "dog.jpg"  # ä¸€å¼ æ‹‰å¸ƒæ‹‰å¤šçš„ç…§ç‰‡
query = "è¿™æ˜¯ä»€ä¹ˆå“ç§çš„ç‹—?"
```

### Step 1: è§†è§‰ç¼–ç 
```
dog.jpg (åŸå§‹å›¾åƒ)
  â†“ resize
[3, 448, 448] (å½’ä¸€åŒ–tensor)
  â†“ ViT Encoder
[1024, 1536] (1024ä¸ªpatchç‰¹å¾)
  â†“ VL Adapter (Cross-Attention + 2D PE)
[256, 4096] (å‹ç¼©åçš„è§†è§‰token)
```

### Step 2: æ–‡æœ¬ç¼–ç 
```
"è¿™æ˜¯ä»€ä¹ˆå“ç§çš„ç‹—?"
  â†“ Tokenizer
[151857, 28450, 25608, 31893, 102, 151858]
# <img>   è¿™     æ˜¯    ä»€ä¹ˆ   ç‹—    </img>
  â†“ Embedding
[6, 4096] (6ä¸ªtokençš„embedding)
```

### Step 3: åºåˆ—æ‹¼æ¥
```
[1 + 256 + 1 + 6, 4096]
= [264, 4096]

è¯¦ç»†ç»“æ„:
Position 0:     <img> embedding
Position 1-256: 256ä¸ªè§†è§‰token (æ¥è‡ªVL Adapter)
Position 257:   </img> embedding
Position 258:   "è¿™" embedding
Position 259:   "æ˜¯" embedding
Position 260:   "ä»€ä¹ˆ" embedding
Position 261:   "å“ç§" embedding
Position 262:   "çš„" embedding
Position 263:   "ç‹—" embedding
```

### Step 4: LLMç”Ÿæˆ
```
[264, 4096] (è¾“å…¥embeddings)
  â†“ 32å±‚Transformer Decoder
[264, 4096] (hidden states)
  â†“ LM Head
[264, 151851] (logits over vocabulary)
  â†“ Sampling (æœ€åä¸€ä¸ªä½ç½®)
Token ID: 45892 ("è¿™")
  â†“ è‡ªå›å½’ç»§ç»­ç”Ÿæˆ
"è¿™æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚"
```

### Step 5: è¾“å‡º
```
å®Œæ•´è¾“å‡ºåºåˆ—:
<img>[256 toks]</img>è¿™æ˜¯ä»€ä¹ˆå“ç§çš„ç‹—?è¿™æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚<eos>
                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      ç”Ÿæˆçš„å›ç­”éƒ¨åˆ†
```

---

## å…«ã€å…³é”®è®¾è®¡æ€æƒ³æ€»ç»“

### 8.1 ä¸ºä»€ä¹ˆå›¾åƒtokenä¸è®¡ç®—loss?

**åŸå› **:
1. **æ— ç›‘ç£ä¿¡å·**: å›¾åƒpatchæ²¡æœ‰"æ­£ç¡®ç­”æ¡ˆ"
2. **å¯¹é½ç›®æ ‡**: è®©è§†è§‰ç‰¹å¾å­¦ä¼šé¢„æµ‹æ–‡æœ¬,è€Œéé‡å»ºå›¾åƒ
3. **æ•ˆç‡**: é¿å…åœ¨é«˜ç»´è§†è§‰ç©ºé—´è®¡ç®—æ— æ„ä¹‰çš„loss

### 8.2 ä¸ºä»€ä¹ˆéœ€è¦VL Adapter?

**åŸå› **:
1. **ç»´åº¦å‹ç¼©**: 1024â†’256,é™ä½è®¡ç®—æˆæœ¬
2. **ç‰¹å¾å¯¹é½**: ViTç‰¹å¾(1536ç»´) â†’ LLMç©ºé—´(4096ç»´)
3. **ä½ç½®ä¿ç•™**: 2Dä½ç½®ç¼–ç ä¿ç•™ç©ºé—´ä¿¡æ¯,æ”¯æŒgrounding

### 8.3 ä¸ºä»€ä¹ˆä¸‰é˜¶æ®µè®­ç»ƒ?

**åŸå› **:
1. **Stage 1**: å¤§è§„æ¨¡å¼±ç›‘ç£æ•°æ®å»ºç«‹è§†è§‰-è¯­è¨€æ˜ å°„
2. **Stage 2**: é«˜è´¨é‡ä»»åŠ¡æ•°æ®å¢å¼ºç»†ç²’åº¦èƒ½åŠ›
3. **Stage 3**: å¯¹è¯æ•°æ®å¯¹é½äººç±»åå¥½

### 8.4 ç»Ÿä¸€Lossçš„ä¼˜åŠ¿

**ä¼˜åŠ¿**:
- **ç®€æ´**: æ‰€æœ‰ä»»åŠ¡å…±äº«åŒä¸€losså‡½æ•°
- **çµæ´»**: é€šè¿‡æ•°æ®æ ¼å¼æ§åˆ¶ä¸åŒä»»åŠ¡
- **å¯æ‰©å±•**: æ·»åŠ æ–°ä»»åŠ¡æ— éœ€ä¿®æ”¹æ¨¡å‹æ¶æ„

---

å¸Œæœ›è¿™ä¸ªè¯¦ç»†çš„æ•°æ®æµå’ŒæŸå¤±å‡½æ•°åˆ†æèƒ½å¸®åŠ©ä½ æ·±å…¥ç†è§£Qwen-VLçš„å·¥ä½œåŸç†ï¼å¦‚æœ‰ç–‘é—®æ¬¢è¿ç»§ç»­æé—®ã€‚

---

## ğŸ‘¤ User

æˆ‘å¯¹äºé¢„è®­ç»ƒé˜¶æ®µçš„lossè®¡ç®—è¿˜ä¸æ˜¯å¾ˆæ¸…æ¥š,æ¯”å¦‚gtæ˜¯"this is a dog."ä½†æ˜¯æ¨¡å‹è¾“å‡ºæ˜¯"I can see this cat." è¿™ä¸ªæ˜¯å¦‚ä½•è®¡ç®—losså‘¢

---

## ğŸ¤– Claude

éå¸¸å¥½çš„é—®é¢˜! è¿™æ¶‰åŠåˆ°è¯­è¨€æ¨¡å‹è®­ç»ƒçš„æ ¸å¿ƒæœºåˆ¶ - **Teacher Forcing** å’Œ **äº¤å‰ç†µæŸå¤±çš„é€tokenè®¡ç®—**ã€‚è®©æˆ‘è¯¦ç»†è§£é‡Š:

---

# é¢„è®­ç»ƒé˜¶æ®µLossè®¡ç®—è¯¦è§£

## ä¸€ã€å…³é”®æ¦‚å¿µ:Teacher Forcing

### 1.1 ä»€ä¹ˆæ˜¯Teacher Forcing?

**è®­ç»ƒæ—¶**çš„å…³é”®æœºåˆ¶:
- æ¨¡å‹**æ¯ä¸€æ­¥çš„è¾“å…¥ä½¿ç”¨çœŸå®æ ‡ç­¾(ground truth)**,è€Œéè‡ªå·±çš„é¢„æµ‹
- å³ä½¿æ¨¡å‹é¢„æµ‹é”™è¯¯,ä¸‹ä¸€æ­¥ä»ç„¶å–‚å…¥æ­£ç¡®çš„token
- è¿™æ ·å¯ä»¥åŠ é€Ÿè®­ç»ƒæ”¶æ•›,é¿å…é”™è¯¯ç´¯ç§¯

è®©æˆ‘ç”¨ä½ çš„ä¾‹å­è¯´æ˜:

```
Ground Truth: "This is a dog."
æ¨¡å‹å®é™…é¢„æµ‹: "I can see this cat."

ä½†è®­ç»ƒè¿‡ç¨‹ä¸­,æ¨¡å‹çœ‹åˆ°çš„è¾“å…¥åºåˆ—å§‹ç»ˆæ˜¯ground truth!
```

---

## äºŒã€å…·ä½“Lossè®¡ç®—æµç¨‹

### 2.1 å®Œæ•´ç¤ºä¾‹è®¾ç½®

å‡è®¾:
```python
# è¾“å…¥å›¾åƒ
Image: [ä¸€å¼ ç‹—çš„ç…§ç‰‡]

# Ground Truth Caption
GT: "This is a dog."

# è¯æ±‡è¡¨ (ç®€åŒ–ç¤ºä¾‹)
vocab = {
    "<img>": 0, "</img>": 1, "This": 2, "is": 3, "a": 4, 
    "dog": 5, "cat": 6, ".": 7, "<eos>": 8,
    "I": 9, "can": 10, "see": 11, "this": 12
}

# Tokenized Ground Truth
tokens = [0, ...(256ä¸ªå›¾åƒtokens), 1, 2, 3, 4, 5, 7, 8]
#        <img>  [image features]   </img> This is a dog . <eos>
```

### 2.2 æ¨¡å‹å‰å‘ä¼ æ’­

```python
# Step 1: è¾“å…¥åºåˆ—æ„é€ 
input_sequence = [0, ...256 image tokens..., 1, 2, 3, 4, 5, 7]
#                <img>  [image features]  </img> This is a dog .

# Step 2: æ¨¡å‹è¾“å‡ºlogits (æœªå½’ä¸€åŒ–çš„åˆ†æ•°)
# å½¢çŠ¶: [seq_len, vocab_size]
logits = model(input_sequence)  # [259, 13]  (259 = 1+256+1+5+1)

# Step 3: å¯¹logitsåšsoftmaxå¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ
probs = softmax(logits, dim=-1)  # [259, 13]
```

### 2.3 æ¯ä¸ªä½ç½®çš„é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ

è®©æˆ‘è¯¦ç»†å±•ç¤º**æ¯ä¸ªæ–‡æœ¬ä½ç½®**çš„æ¦‚ç‡åˆ†å¸ƒ:

```
ä½ç½® 258 (åœ¨"This"ä¹‹å,é¢„æµ‹"is"):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
è¾“å…¥: <img>[256 toks]</img>This
é¢„æµ‹ç›®æ ‡: "is" (token_id = 3)

æ¨¡å‹è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ (softmaxå):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Token   â”‚ Probability â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ <img>   â”‚ 0.001       â”‚
â”‚ </img>  â”‚ 0.002       â”‚
â”‚ This    â”‚ 0.05        â”‚
â”‚ is      â”‚ 0.65 âœ“      â”‚ â† Ground Truth
â”‚ a       â”‚ 0.10        â”‚
â”‚ dog     â”‚ 0.08        â”‚
â”‚ cat     â”‚ 0.03        â”‚
â”‚ .       â”‚ 0.02        â”‚
â”‚ <eos>   â”‚ 0.01        â”‚
â”‚ I       â”‚ 0.02        â”‚
â”‚ can     â”‚ 0.01        â”‚
â”‚ see     â”‚ 0.01        â”‚
â”‚ this    â”‚ 0.01        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Cross-Entropy Loss (æ­¤ä½ç½®):
loss_258 = -log(P(is)) = -log(0.65) = 0.43
```

```
ä½ç½® 259 (åœ¨"is"ä¹‹å,é¢„æµ‹"a"):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
è¾“å…¥: <img>[256 toks]</img>This is
é¢„æµ‹ç›®æ ‡: "a" (token_id = 4)

æ¨¡å‹è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Token   â”‚ Probability â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ <img>   â”‚ 0.001       â”‚
â”‚ </img>  â”‚ 0.001       â”‚
â”‚ This    â”‚ 0.02        â”‚
â”‚ is      â”‚ 0.03        â”‚
â”‚ a       â”‚ 0.70 âœ“      â”‚ â† Ground Truth
â”‚ dog     â”‚ 0.15        â”‚
â”‚ cat     â”‚ 0.04        â”‚
â”‚ .       â”‚ 0.02        â”‚
â”‚ <eos>   â”‚ 0.01        â”‚
â”‚ I       â”‚ 0.01        â”‚
â”‚ can     â”‚ 0.005       â”‚
â”‚ see     â”‚ 0.005       â”‚
â”‚ this    â”‚ 0.01        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Cross-Entropy Loss:
loss_259 = -log(0.70) = 0.36
```

```
ä½ç½® 260 (åœ¨"a"ä¹‹å,é¢„æµ‹"dog"):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
è¾“å…¥: <img>[256 toks]</img>This is a
é¢„æµ‹ç›®æ ‡: "dog" (token_id = 5)

å‡è®¾æ¨¡å‹è¿™é‡Œé¢„æµ‹é”™äº†,ç»™"cat"æ›´é«˜æ¦‚ç‡:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Token   â”‚ Probability â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ <img>   â”‚ 0.001       â”‚
â”‚ </img>  â”‚ 0.001       â”‚
â”‚ This    â”‚ 0.01        â”‚
â”‚ is      â”‚ 0.02        â”‚
â”‚ a       â”‚ 0.05        â”‚
â”‚ dog     â”‚ 0.25 âœ“      â”‚ â† Ground Truth (ä½†æ¦‚ç‡ä¸æ˜¯æœ€é«˜!)
â”‚ cat     â”‚ 0.60 âœ—      â”‚ â† æ¨¡å‹é”™è¯¯é¢„æµ‹è¿™ä¸ª
â”‚ .       â”‚ 0.03        â”‚
â”‚ <eos>   â”‚ 0.01        â”‚
â”‚ I       â”‚ 0.01        â”‚
â”‚ can     â”‚ 0.005       â”‚
â”‚ see     â”‚ 0.005       â”‚
â”‚ this    â”‚ 0.01        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Cross-Entropy Loss:
loss_260 = -log(P(dog))  â† æ³¨æ„:åªçœ‹ground truthçš„æ¦‚ç‡!
         = -log(0.25) 
         = 1.39  â† æ¯”å‰é¢çš„losså¤§å¾ˆå¤š!
```

### 2.4 æ€»Lossè®¡ç®—

```python
# åªå¯¹æ–‡æœ¬tokenä½ç½®è®¡ç®—loss (å¿½ç•¥å›¾åƒtoken)
total_loss = (loss_258 + loss_259 + loss_260 + loss_261 + loss_262) / 5

å‡è®¾å„ä½ç½®loss:
position 258 ("is"):   -log(0.65) = 0.43
position 259 ("a"):    -log(0.70) = 0.36
position 260 ("dog"):  -log(0.25) = 1.39  â† é¢„æµ‹é”™è¯¯,losså¾ˆå¤§
position 261 ("."):    -log(0.80) = 0.22
position 262 ("<eos>"): -log(0.75) = 0.29

total_loss = (0.43 + 0.36 + 1.39 + 0.22 + 0.29) / 5
           = 2.69 / 5
           = 0.54
```

---

## ä¸‰ã€å…³é”®æœºåˆ¶è¯¦è§£

### 3.1 Teacher Forcingçš„å®Œæ•´æµç¨‹

```
è®­ç»ƒæ—¶çš„åºåˆ—å¤„ç†:

Time Step    Input Tokens                    Target    Model Prediction    Loss
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
t=258        <img>[256]</img>This           is        P(is)=0.65          -log(0.65)
             â””â”€ ç”¨GT "This" â”€â”˜                        P(cat)=0.03

t=259        <img>[256]</img>This is        a         P(a)=0.70           -log(0.70)
             â””â”€ ç”¨GT "is" â”€â”˜                          P(dog)=0.15

t=260        <img>[256]</img>This is a      dog       P(dog)=0.25         -log(0.25) â† é«˜loss
             â””â”€ ç”¨GT "a" â”€â”˜                           P(cat)=0.60 â† è™½ç„¶è¿™ä¸ªæœ€é«˜,ä½†ä¸ç”¨!

t=261        <img>[256]</img>This is a dog  .         P(.)=0.80           -log(0.80)
             â””â”€ ç”¨GT "dog" (ä¸æ˜¯æ¨¡å‹é¢„æµ‹çš„"cat")! â”€â”˜
```

**é‡ç‚¹**:
- å³ä½¿t=260æ—¶æ¨¡å‹é¢„æµ‹"cat"æ¦‚ç‡æœ€é«˜(0.60)
- t=261çš„è¾“å…¥ä»ç„¶ä½¿ç”¨GTä¸­çš„"dog"
- **è¿™å°±æ˜¯Teacher Forcing**: å§‹ç»ˆç”¨çœŸå®æ ‡ç­¾ä½œä¸ºä¸‹ä¸€æ­¥è¾“å…¥

### 3.2 ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡?

**ä¼˜åŠ¿**:
1. **åŠ é€Ÿæ”¶æ•›**: ä¸ä¼šå› ä¸ºæ—©æœŸé”™è¯¯é¢„æµ‹å¯¼è‡´åç»­å…¨éƒ¨é”™è¯¯
2. **ç¨³å®šè®­ç»ƒ**: æ¯ä¸ªä½ç½®çš„losséƒ½æœ‰æ˜ç¡®çš„ç›‘ç£ä¿¡å·
3. **å¹¶è¡Œè®¡ç®—**: å¯ä»¥ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰ä½ç½®çš„loss

**åŠ£åŠ¿**(æ¨ç†æ—¶çš„exposure bias):
- è®­ç»ƒæ—¶è§çš„æ˜¯GT,æ¨ç†æ—¶è§çš„æ˜¯è‡ªå·±çš„é¢„æµ‹
- å› æ­¤éœ€è¦inferenceæ—¶çš„è‡ªå›å½’ç”Ÿæˆç­–ç•¥

---

## å››ã€å®Œæ•´ä»£ç å®ç°

è®©æˆ‘ç”¨å®é™…ä»£ç å±•ç¤º:

```python
import torch
import torch.nn.functional as F

def compute_language_modeling_loss(logits, labels, ignore_index=-100):
    """
    å‚æ•°:
        logits: [batch_size, seq_len, vocab_size] æ¨¡å‹è¾“å‡ºçš„æœªå½’ä¸€åŒ–åˆ†æ•°
        labels: [batch_size, seq_len] ground truth token IDs
        ignore_index: éœ€è¦å¿½ç•¥çš„token ID (å¦‚å›¾åƒtokenä½ç½®)
    
    è¿”å›:
        loss: æ ‡é‡
    """
    # Step 1: Shiftæ“ä½œ (é¢„æµ‹ä¸‹ä¸€ä¸ªtoken)
    # è¾“å…¥: <img>[256]</img>This is a dog .
    # é¢„æµ‹: [256]</img>This is a dog . <eos>
    shift_logits = logits[:, :-1, :].contiguous()  # [B, seq_len-1, vocab]
    shift_labels = labels[:, 1:].contiguous()      # [B, seq_len-1]
    
    # Step 2: Flatten
    shift_logits = shift_logits.view(-1, shift_logits.size(-1))  # [B*(seq_len-1), vocab]
    shift_labels = shift_labels.view(-1)                          # [B*(seq_len-1)]
    
    # Step 3: è®¡ç®—Cross-Entropy
    # reduction='mean': è‡ªåŠ¨å¯¹æ‰€æœ‰éignoreä½ç½®æ±‚å¹³å‡
    loss = F.cross_entropy(
        shift_logits,
        shift_labels,
        ignore_index=ignore_index,
        reduction='mean'
    )
    
    return loss


# å…·ä½“ä¾‹å­
batch_size = 1
seq_len = 264  # 1 + 256 + 1 + 6 (åŒ…æ‹¬<img>, image tokens, </img>, æ–‡æœ¬)
vocab_size = 151851

# æ¨¡æ‹Ÿæ¨¡å‹è¾“å‡º
logits = torch.randn(batch_size, seq_len, vocab_size)  # éšæœºåˆå§‹åŒ–

# Ground Truth labels
labels = torch.tensor([[
    0,     # <img>
    *[100]*256,  # 256ä¸ªå›¾åƒtoken IDs
    1,     # </img>
    2,     # This
    3,     # is
    4,     # a
    5,     # dog
    7,     # .
    8      # <eos>
]])

# æ ‡è®°å›¾åƒtokenä½ç½®ä¸ºignore
labels[:, 1:258] = -100  # å›¾åƒtokenä¸è®¡ç®—loss

# è®¡ç®—loss
loss = compute_language_modeling_loss(logits, labels)
print(f"Total Loss: {loss.item()}")
```

### 4.1 é€token lossè®¡ç®—ç¤ºä¾‹

```python
def detailed_loss_computation(logits, labels):
    """
    å±•ç¤ºæ¯ä¸ªä½ç½®çš„lossè®¡ç®—ç»†èŠ‚
    """
    seq_len = logits.size(1)
    vocab_size = logits.size(2)
    
    print("Position-wise Loss Breakdown:")
    print("=" * 80)
    
    total_loss = 0
    num_tokens = 0
    
    for pos in range(seq_len - 1):
        # å½“å‰ä½ç½®çš„logits
        current_logits = logits[0, pos, :]  # [vocab_size]
        
        # ä¸‹ä¸€ä¸ªä½ç½®çš„ground truth
        target_token_id = labels[0, pos + 1].item()
        
        # è·³è¿‡ignore token
        if target_token_id == -100:
            continue
        
        # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
        probs = F.softmax(current_logits, dim=0)
        
        # ç›®æ ‡tokençš„æ¦‚ç‡
        target_prob = probs[target_token_id].item()
        
        # é¢„æµ‹æœ€å¯èƒ½çš„token
        pred_token_id = torch.argmax(probs).item()
        
        # è¯¥ä½ç½®çš„loss
        token_loss = -torch.log(probs[target_token_id])
        
        print(f"Position {pos:3d}:")
        print(f"  Input: tokens[0:{pos+1}]")
        print(f"  Target: token_id={target_token_id}")
        print(f"  P(target)={target_prob:.4f}")
        print(f"  Predicted: token_id={pred_token_id}")
        print(f"  Loss: -log({target_prob:.4f}) = {token_loss.item():.4f}")
        
        if pred_token_id != target_token_id:
            print(f"  âš ï¸  WRONG PREDICTION! (GT: {target_token_id}, Pred: {pred_token_id})")
        
        print()
        
        total_loss += token_loss.item()
        num_tokens += 1
    
    avg_loss = total_loss / num_tokens
    print(f"Average Loss: {avg_loss:.4f}")
    return avg_loss
```

---

## äº”ã€ä½ çš„ä¾‹å­è¯¦ç»†åˆ†æ

### 5.1 åœºæ™¯è®¾å®š

```
Ground Truth: "This is a dog."
æ¨¡å‹é¢„æµ‹(å‡è®¾): "I can see this cat."
```

### 5.2 å®é™…Lossè®¡ç®—è¿‡ç¨‹

```python
åºåˆ—ä½ç½®åˆ†è§£:
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Pos â”‚ Input Seq    â”‚ Target â”‚ GT Token     â”‚ Prob    â”‚ Loss     â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 257 â”‚ <img>[..]</img>     â”‚ This   â”‚ This         â”‚ 0.01    â”‚ 4.61 âœ—   â”‚
â”‚     â”‚              â”‚        â”‚ (æ¨¡å‹å¯èƒ½è¾“å‡º"I") â”‚         â”‚          â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 258 â”‚ <img>[..]</img>This â”‚ is     â”‚ is           â”‚ 0.02    â”‚ 3.91 âœ—   â”‚
â”‚     â”‚              â”‚        â”‚ (æ¨¡å‹å¯èƒ½è¾“å‡º"can")â”‚        â”‚          â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 259 â”‚ ...This is   â”‚ a      â”‚ a            â”‚ 0.03    â”‚ 3.51 âœ—   â”‚
â”‚     â”‚              â”‚        â”‚ (æ¨¡å‹å¯èƒ½è¾“å‡º"see")â”‚        â”‚          â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 260 â”‚ ...This is a â”‚ dog    â”‚ dog          â”‚ 0.05    â”‚ 3.00 âœ—   â”‚
â”‚     â”‚              â”‚        â”‚ (æ¨¡å‹å¯èƒ½è¾“å‡º"this")â”‚       â”‚          â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 261 â”‚ ...is a dog  â”‚ .      â”‚ .            â”‚ 0.10    â”‚ 2.30 âœ—   â”‚
â”‚     â”‚              â”‚        â”‚ (æ¨¡å‹å¯èƒ½è¾“å‡º"cat")â”‚        â”‚          â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total Loss = (4.61 + 3.91 + 3.51 + 3.00 + 2.30) / 5 = 3.47
```

**å…³é”®ç‚¹**:
1. **æ¯ä¸ªä½ç½®çš„è¾“å…¥éƒ½æ˜¯GTåºåˆ—çš„å‰ç¼€**
2. **Lossåªçœ‹GT tokençš„æ¦‚ç‡**,ä¸ç®¡æ¨¡å‹å®é™…é¢„æµ‹ä»€ä¹ˆ
3. **é¢„æµ‹é”™è¯¯ â†’ GT tokenæ¦‚ç‡ä½ â†’ Losså¤§**

### 5.3 è®­ç»ƒä¸­çš„æ¢¯åº¦æ›´æ–°

```python
# ä¼ªä»£ç 
for batch in dataloader:
    images, captions = batch
    
    # å‰å‘ä¼ æ’­
    logits = model(images, captions[:, :-1])  # è¾“å…¥: é™¤äº†æœ€åä¸€ä¸ªtoken
    
    # è®¡ç®—loss
    loss = cross_entropy(logits, captions[:, 1:])  # ç›®æ ‡: é™¤äº†ç¬¬ä¸€ä¸ªtoken
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    # å‚æ•°æ›´æ–°
    optimizer.step()
    
    # Losså¤§ â†’ æ¢¯åº¦å¤§ â†’ å‚æ•°è°ƒæ•´å¹…åº¦å¤§
    # ç›®æ ‡: ä¸‹æ¬¡é¢„æµ‹æ—¶,P(dog) â†‘, P(cat) â†“
```

---

## å…­ã€å¸¸è§è¯¯è§£æ¾„æ¸…

### âŒ è¯¯è§£1: "æ¨¡å‹é¢„æµ‹é”™äº†,å°±ä¸è®¡ç®—loss?"
**âœ… æ­£ç¡®**: æ— è®ºé¢„æµ‹å¯¹é”™,éƒ½è®¡ç®—lossã€‚é¢„æµ‹é”™è¯¯æ—¶,GT tokenæ¦‚ç‡ä½,lossæ›´å¤§ã€‚

### âŒ è¯¯è§£2: "é¢„æµ‹'cat'æ—¶,lossæ˜¯æ ¹æ®'cat'è®¡ç®—çš„?"
**âœ… æ­£ç¡®**: lossæ°¸è¿œåªçœ‹GT token("dog")çš„æ¦‚ç‡,ä¸é¢„æµ‹çš„"cat"æ— ç›´æ¥å…³ç³»ã€‚

### âŒ è¯¯è§£3: "æ¨¡å‹è¾“å‡º'I can see this cat'ä¼šç›´æ¥ç”¨äºä¸‹ä¸€æ­¥?"
**âœ… æ­£ç¡®**: è®­ç»ƒæ—¶ä½¿ç”¨Teacher Forcing,ä¸‹ä¸€æ­¥è¾“å…¥ä»æ˜¯GTã€‚åªæœ‰æ¨ç†æ—¶æ‰ç”¨æ¨¡å‹è‡ªå·±çš„é¢„æµ‹ã€‚

### âŒ è¯¯è§£4: "æ‰€æœ‰ä½ç½®çš„lossæƒé‡ç›¸åŒ?"
**âœ… æ­£ç¡®**: é»˜è®¤ç›¸åŒ,ä½†å¯ä»¥é€šè¿‡è°ƒæ•´å®ç°ä¸åŒæƒé‡(å¦‚SFTé˜¶æ®µåªç›‘ç£assistantå›ç­”)ã€‚

---

## ä¸ƒã€æ¨ç†æ—¶çš„åŒºåˆ«

### 7.1 è®­ç»ƒ vs æ¨ç†å¯¹æ¯”

```
ã€è®­ç»ƒæ—¶ - Teacher Forcingã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Step 1: Input: <img>[..]</img>
        Output Probs: {"I": 0.6, "This": 0.01, ...}
        Next Input: <img>[..]</img>This  â† ç”¨GT!

Step 2: Input: <img>[..]</img>This
        Output Probs: {"can": 0.5, "is": 0.02, ...}
        Next Input: <img>[..]</img>This is  â† ç”¨GT!

...æŒç»­ä½¿ç”¨GTä½œä¸ºè¾“å…¥


ã€æ¨ç†æ—¶ - Autoregressive Generationã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Step 1: Input: <img>[..]</img>
        Output Probs: {"I": 0.6, "This": 0.01, ...}
        Sample: "I"
        Next Input: <img>[..]</img>I  â† ç”¨è‡ªå·±çš„é¢„æµ‹!

Step 2: Input: <img>[..]</img>I
        Output Probs: {"can": 0.5, "think": 0.3, ...}
        Sample: "can"
        Next Input: <img>[..]</img>I can  â† ç”¨è‡ªå·±çš„é¢„æµ‹!

...æŒç»­ä½¿ç”¨æ¨¡å‹è‡ªå·±çš„é¢„æµ‹
```

### 7.2 ä¸ºä»€ä¹ˆè®­ç»ƒå’Œæ¨ç†ä¸ä¸€è‡´?

è¿™å°±æ˜¯è‘—åçš„ **Exposure Bias** é—®é¢˜:
- è®­ç»ƒæ—¶æ€»æ˜¯çœ‹åˆ°"æ­£ç¡®"çš„å†å²
- æ¨ç†æ—¶å¯èƒ½çœ‹åˆ°"é”™è¯¯"çš„å†å²
- å¯¼è‡´è¯¯å·®ç´¯ç§¯

**ç¼“è§£æ–¹æ³•**:
1. Scheduled Sampling: è®­ç»ƒæ—¶å¶å°”ä½¿ç”¨æ¨¡å‹é¢„æµ‹
2. å¤§è§„æ¨¡é¢„è®­ç»ƒ: è®©æ¨¡å‹æ›´é²æ£’
3. RLHF: å¼ºåŒ–å­¦ä¹ å¾®è°ƒ

---

## å…«ã€æ€»ç»“

### Lossè®¡ç®—çš„æœ¬è´¨

```python
# å¯¹äºæ¯ä¸ªä½ç½® i:
loss_i = -log(P(ground_truth_token_i | history))

# å…¶ä¸­ history åœ¨è®­ç»ƒæ—¶æ˜¯ ground_truth[:i]
# P(...) æ˜¯æ¨¡å‹è¾“å‡ºçš„softmaxæ¦‚ç‡åˆ†å¸ƒä¸­,GT tokenå¯¹åº”çš„æ¦‚ç‡

# æ€»loss:
total_loss = mean(loss_i for all non-ignored positions)
```

**æ ¸å¿ƒè¦ç‚¹**:
1. âœ… Lossåªå…³å¿ƒGT tokençš„æ¦‚ç‡,ä¸ç®¡é¢„æµ‹ä»€ä¹ˆ
2. âœ… é¢„æµ‹é”™è¯¯ = GTæ¦‚ç‡ä½ = Losså¤§ = æ¢¯åº¦å¤§ = æ›´å¤šå­¦ä¹ 
3. âœ… Teacher Forcingç¡®ä¿è®­ç»ƒç¨³å®š,ä½†å¸¦æ¥exposure bias
4. âœ… äº¤å‰ç†µè‡ªåŠ¨å®ç°"é¢„æµ‹è¶Šé”™,æƒ©ç½šè¶Šå¤§"

å¸Œæœ›è¿™ä¸ªè¯¦ç»†çš„è§£é‡Šè§£ç­”äº†ä½ çš„ç–‘é—®!

---

## ğŸ‘¤ User

åƒé—®ç¬¬ä¸€é˜¶æ®µçš„é¢„è®­ç»ƒçš„è®­ç»ƒä»»åŠ¡æ˜¯ä»€ä¹ˆ

---

## ğŸ‘¤ User

[Request interrupted by user]

---

## ğŸ‘¤ User

åƒé—®ç¬¬ä¸€é˜¶æ®µçš„é¢„è®­ç»ƒçš„è®­ç»ƒä»»åŠ¡æ˜¯ä»€ä¹ˆ?æ˜¯VQAè¿˜æ˜¯å…¶ä»–ä»»åŠ¡å‘¢

---

## ğŸ¤– Claude

æ ¹æ®è®ºæ–‡å†…å®¹,**Qwen-VLç¬¬ä¸€é˜¶æ®µçš„é¢„è®­ç»ƒä»»åŠ¡æ˜¯Image Captioning(å›¾åƒæè¿°ç”Ÿæˆ)**,è€Œä¸æ˜¯VQAã€‚

---

## ç¬¬ä¸€é˜¶æ®µé¢„è®­ç»ƒä»»åŠ¡è¯¦è§£

### æ ¸å¿ƒä»»åŠ¡: **Image Captioning**

**è®­ç»ƒæ•°æ®**(è®ºæ–‡3.1èŠ‚):
```
ä»»åŠ¡ç±»å‹: å›¾æ–‡å¯¹(Image-Text Pairs)
æ•°æ®æ¥æº: ç½‘ç»œçˆ¬å–çš„å¼±æ ‡æ³¨æ•°æ®
æ•°æ®è§„æ¨¡: 1.4Bæ ·æœ¬(æ¸…æ´—å)
```

**å…·ä½“æ•°æ®é›†**:
| æ•°æ®é›† | åŸå§‹é‡ | æ¸…æ´—å | è¯­è¨€ |
|--------|--------|--------|------|
| LAION-en | 2B | 280M | è‹±æ–‡ |
| LAION-COCO | 600M | 300M | è‹±æ–‡ |
| DataComp | 1.4B | 300M | è‹±æ–‡ |
| Coyo | 700M | 200M | è‹±æ–‡ |
| CC12M/3M | 15M | 11M | è‹±æ–‡ |
| SBU | 1M | 0.8M | è‹±æ–‡ |
| COCO Caption | 0.6M | 0.6M | è‹±æ–‡ |
| LAION-zh | 108M | 105M | ä¸­æ–‡ |
| In-house Data | 220M | 220M | ä¸­æ–‡ |

### æ•°æ®æ ¼å¼ç¤ºä¾‹

```
è¾“å…¥åºåˆ—:
<img>[256ä¸ªå›¾åƒtoken]</img>Generate the caption in English:

ç›®æ ‡åºåˆ—(è®¡ç®—loss):
A woman and her dog playing on the beach.<eos>
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ä»…å¯¹captionæ–‡æœ¬è®¡ç®—loss â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### è®­ç»ƒç›®æ ‡

**Losså‡½æ•°**:
```python
# æœ€å°åŒ–æ–‡æœ¬tokençš„äº¤å‰ç†µ
Loss = CrossEntropy(predicted_text_tokens, ground_truth_caption)

# å…³é”®:å›¾åƒtokenä¸å‚ä¸lossè®¡ç®—
```

### é‡è¦å‘ç°: Zero-shot VQAèƒ½åŠ›æ¶Œç°

è®ºæ–‡é™„å½•E.1æ˜ç¡®æŒ‡å‡º:
> "Note that, the pre-training stage (Stage one) **has no VQA data being added**, but the **Zero-shot VQA score increases** amidst fluctuations."

**å®éªŒè¯æ®**(Figure 6):
```
è®­ç»ƒå›¾åƒæ•°é‡ â†’ VQAv2 Zero-shotå‡†ç¡®ç‡
0.0B        â†’ 48%
0.4B        â†’ 50%
0.8B        â†’ 52%
1.2B        â†’ 54%
1.6B        â†’ 56%  (æ¶¨äº†8ä¸ªç™¾åˆ†ç‚¹!)
```

---

## ä¸ºä»€ä¹ˆStage 1ä¸ç”¨VQA?

### è®¾è®¡ç†å¿µ

1. **å…ˆå»ºç«‹è§†è§‰-è¯­è¨€å¯¹é½**
   - Image Captioningæ˜¯æœ€åŸºç¡€çš„è§†è§‰ç†è§£ä»»åŠ¡
   - è®©æ¨¡å‹å­¦ä¼š"çœ‹å›¾è¯´è¯"
   - å»ºç«‹å›¾åƒç‰¹å¾åˆ°è¯­è¨€ç©ºé—´çš„æ˜ å°„

2. **å¤§è§„æ¨¡å¼±ç›‘ç£é¢„è®­ç»ƒ**
   - å›¾æ–‡å¯¹æ•°æ®é‡å¤§(1.4B)ã€è·å–æˆæœ¬ä½
   - VQAæ•°æ®éœ€è¦äººå·¥æ ‡æ³¨,æˆæœ¬é«˜ã€è§„æ¨¡å°

3. **åˆ†é˜¶æ®µèƒ½åŠ›åŸ¹å…»**
   - Stage 1: è§†è§‰-è¯­è¨€åŸºç¡€å¯¹é½(Captioning)
   - Stage 2: å¤šä»»åŠ¡èƒ½åŠ›å¢å¼º(VQA + Grounding + OCRç­‰)
   - Stage 3: æŒ‡ä»¤éµå¾ªå¯¹é½(Chat)

---

## ä¸Stage 2çš„å¯¹æ¯”

| ç»´åº¦ | Stage 1 (é¢„è®­ç»ƒ) | Stage 2 (å¤šä»»åŠ¡é¢„è®­ç»ƒ) |
|------|------------------|----------------------|
| **ä»»åŠ¡** | ä»…Image Captioning | 7ç§ä»»åŠ¡(Captioning + VQA + Grounding + OCRç­‰) |
| **æ•°æ®é‡** | 1.4Bæ ·æœ¬ | 70M+æ ·æœ¬ |
| **æ•°æ®è´¨é‡** | å¼±æ ‡æ³¨(ç½‘ç»œçˆ¬å–) | é«˜è´¨é‡æ ‡æ³¨ |
| **åˆ†è¾¨ç‡** | 224Ã—224 | 448Ã—448 |
| **å¯è®­ç»ƒå‚æ•°** | ViT + VL Adapter | å…¨éƒ¨å‚æ•° |
| **è®­ç»ƒç›®æ ‡** | å­¦ä¼šè§†è§‰-è¯­è¨€å¯¹é½ | å¢å¼ºç»†ç²’åº¦ç†è§£èƒ½åŠ› |

---

## Stage 1çš„è®­ç»ƒç»†èŠ‚

**æ¨¡å‹çŠ¶æ€**:
```
å†»ç»“: LLM(Qwen-7B)å…¨éƒ¨å‚æ•°
è®­ç»ƒ: ViT + VL Adapter

åŸå› :
1. ä¿æŠ¤LLMçš„æ–‡æœ¬ç†è§£èƒ½åŠ›
2. è®©è§†è§‰ç¼–ç å™¨é€‚åº”ä»»åŠ¡
3. è®©VL Adapterå­¦ä¼šå‹ç¼©å’Œå¯¹é½
```

**è®­ç»ƒè¶…å‚**(è®ºæ–‡Table 8):
```python
Learning Rate: 2e-4 (peak) â†’ 1e-6 (min)
Batch Size: 30,720
Training Steps: 50,000
Warm-up: 500 steps
Resolution: 224Ã—224
Loss: CrossEntropy (ä»…æ–‡æœ¬token)
```

---

## å…³é”®æ´å¯Ÿ

### 1. Captioningæ˜¯VQAçš„åŸºç¡€
é€šè¿‡Image Captioningè®­ç»ƒ:
- æ¨¡å‹å­¦ä¼šæå–å›¾åƒä¸­çš„å…³é”®ä¿¡æ¯
- å»ºç«‹è§†è§‰æ¦‚å¿µåˆ°è¯­è¨€æè¿°çš„æ˜ å°„
- è‡ªç„¶æ¶Œç°å‡ºå›ç­”ç®€å•è§†è§‰é—®é¢˜çš„èƒ½åŠ›

### 2. Zero-shotèƒ½åŠ›çš„æ¶Œç°
å³ä½¿æ²¡æœ‰VQAè®­ç»ƒæ•°æ®,æ¨¡å‹ä¹Ÿèƒ½:
- ç†è§£é—®é¢˜çš„è¯­ä¹‰
- åœ¨å›¾åƒä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯
- ç”Ÿæˆåˆç†çš„å›ç­”

è¿™éªŒè¯äº†**å¤§è§„æ¨¡é¢„è®­ç»ƒ**çš„æœ‰æ•ˆæ€§!

---

## æ€»ç»“

**Qwen-VLç¬¬ä¸€é˜¶æ®µçš„è®­ç»ƒä»»åŠ¡æ˜¯Image Captioning**,å…·ä½“ç‰¹ç‚¹:

âœ… **å”¯ä¸€ä»»åŠ¡**: Image Captioning(å›¾åƒæè¿°ç”Ÿæˆ)  
âœ… **æ•°æ®è§„æ¨¡**: 1.4Bå›¾æ–‡å¯¹  
âœ… **è®­ç»ƒç›®æ ‡**: æœ€å°åŒ–captionæ–‡æœ¬çš„äº¤å‰ç†µloss  
âœ… **æƒŠå–œå‘ç°**: é›¶æ ·æœ¬VQAèƒ½åŠ›è‡ªç„¶æ¶Œç°  
âŒ **ä¸åŒ…å«**: VQAã€Groundingã€OCRç­‰ä»»åŠ¡(è¿™äº›åœ¨Stage 2å¼•å…¥)

è¿™ç§è®¾è®¡ä½“ç°äº†**ä»ç®€å•åˆ°å¤æ‚ã€ä»ç²—ç²’åº¦åˆ°ç»†ç²’åº¦**çš„æ¸è¿›å¼è®­ç»ƒç­–ç•¥,æ˜¯Qwen-VLæˆåŠŸçš„å…³é”®ä¹‹ä¸€!

---

## ğŸ‘¤ User

åƒé—®ç¬¬ä¸‰é˜¶æ®µçš„é¢„è®­ç»ƒçš„è®­ç»ƒä»»åŠ¡æ˜¯ä»€ä¹ˆ?æ˜¯VQAè¿˜æ˜¯å…¶ä»–ä»»åŠ¡å‘¢

---

## ğŸ¤– Claude

æ ¹æ®è®ºæ–‡,**Qwen-VLç¬¬ä¸‰é˜¶æ®µä¸å«"é¢„è®­ç»ƒ"ï¼Œè€Œæ˜¯"Supervised Fine-tuning(ç›‘ç£å¾®è°ƒ/æŒ‡ä»¤å¾®è°ƒ)"**ã€‚å®ƒçš„æ ¸å¿ƒä»»åŠ¡æ˜¯**å¤šæ¨¡æ€å¯¹è¯(Multi-modal Dialogue)**,è€Œä¸æ˜¯VQA!

---

## ç¬¬ä¸‰é˜¶æ®µ: Supervised Fine-tuningè¯¦è§£

### é˜¶æ®µå‘½åçº æ­£

```
âŒ é”™è¯¯: "ç¬¬ä¸‰é˜¶æ®µé¢„è®­ç»ƒ"
âœ… æ­£ç¡®: "ç¬¬ä¸‰é˜¶æ®µç›‘ç£å¾®è°ƒ" (Supervised Fine-tuning / Instruction Fine-tuning)

ä¸‰é˜¶æ®µå®Œæ•´å‘½å:
- Stage 1: Pre-training (é¢„è®­ç»ƒ)
- Stage 2: Multi-task Pre-training (å¤šä»»åŠ¡é¢„è®­ç»ƒ)  
- Stage 3: Supervised Fine-tuning (ç›‘ç£å¾®è°ƒ) â† è¿™é‡Œ!
```

### æ ¸å¿ƒä»»åŠ¡: **Instruction-following Dialogue (æŒ‡ä»¤éµå¾ªå¯¹è¯)**

**è®ºæ–‡3.3èŠ‚åŸæ–‡**:
> "During this stage, we finetuned the Qwen-VL pre-trained model through **instruction fine-tuning** to enhance its **instruction following and dialogue capabilities**, resulting in the interactive **Qwen-VL-Chat** model."

---

## Stage 3çš„è®­ç»ƒæ•°æ®æ„æˆ

### æ•°æ®è§„æ¨¡
```
æ€»æ•°æ®é‡: 350K å¯¹è¯æ ·æœ¬
æ•°æ®æ ¼å¼: ChatMLå¯¹è¯æ ¼å¼
```

### æ•°æ®æ¥æº(è®ºæ–‡3.3èŠ‚)

#### 1. **LLMè‡ªæŒ‡ä»¤ç”Ÿæˆæ•°æ®**
```
æ¥æº: Captionæ•°æ® â†’ LLMç”Ÿæˆå¯¹è¯
ç‰¹ç‚¹: å•å›¾å¯¹è¯,å†…å®¹ç†è§£ä¸ºä¸»
å±€é™: ç¼ºä¹å®šä½å’Œå¤šå›¾èƒ½åŠ›
```

#### 2. **äººå·¥æ ‡æ³¨æ•°æ®**
```
ä»»åŠ¡ç±»å‹:
- Localization (å®šä½ä»»åŠ¡)
- Multi-image comprehension (å¤šå›¾ç†è§£)

ç›®çš„: è¡¥å……ç»†ç²’åº¦èƒ½åŠ›
```

#### 3. **æ¨¡å‹ç”Ÿæˆ + ç­–ç•¥æ‹¼æ¥**
```
æ–¹æ³•: é€šè¿‡ç­–ç•¥ç»„åˆä¸åŒæ•°æ®
ç›®çš„: å¢å¼ºæ•°æ®å¤šæ ·æ€§
```

#### 4. **çº¯æ–‡æœ¬å¯¹è¯æ•°æ®**
```
ç›®çš„: ä¿æŒé€šç”¨å¯¹è¯èƒ½åŠ›
é˜²æ­¢: ç¾éš¾æ€§é—å¿˜
```

---

## æ•°æ®æ ¼å¼: ChatMLå¯¹è¯æ ¼å¼

### å®Œæ•´ç¤ºä¾‹(è®ºæ–‡Appendix B.2)

```
<im_start>user
Picture 1: <img>vg/VG_100K_2/649.jpg</img>What is the sign in the picture?<im_end>

<im_start>assistant
The sign is a road closure with an orange rhombus.<im_end>

<im_start>user
How is the weather in the picture?<im_end>

<im_start>assistant
The shape of the road closure sign is an orange rhombus.<im_end>
```

### Lossè®¡ç®—ç­–ç•¥

**å…³é”®ç‰¹æ€§**: åªç›‘ç£assistantçš„å›ç­”!

```
<im_start>system          â† IGNORE (ä¸è®¡ç®—loss)
You are a helpful...      â† IGNORE

<im_start>user            â† IGNORE
Picture 1: <img>...       â† IGNORE (é—®é¢˜éƒ¨åˆ†)
<im_end>                  â† âœ“ è®¡ç®—loss (ç‰¹æ®Štoken)

<im_start>assistant       â† IGNORE (è§’è‰²å)
This is a dog.            â† âœ“âœ“âœ“ è®¡ç®—loss (ç­”æ¡ˆéƒ¨åˆ†)
<im_end>                  â† âœ“ è®¡ç®—loss
```

**Lossè®¡ç®—å…¬å¼**:
```python
# ä»…å¯¹è“è‰²éƒ¨åˆ†è®¡ç®—loss
loss = CrossEntropy(
    logits[assistant_answer_positions],
    labels[assistant_answer_positions]
)
```

---

## Stage 3çš„è®­ç»ƒç›®æ ‡

### ä¸»è¦ç›®æ ‡

1. **æŒ‡ä»¤éµå¾ªèƒ½åŠ›**
   - ç†è§£ç”¨æˆ·æ„å›¾
   - æŒ‰è¦æ±‚ç”Ÿæˆå›ç­”
   - æ”¯æŒå¤æ‚æŒ‡ä»¤

2. **å¤šè½®å¯¹è¯èƒ½åŠ›**
   - ç»´æŠ¤å¯¹è¯ä¸Šä¸‹æ–‡
   - å¤šè½®äº¤äº’ç†è§£
   - è¯é¢˜è¿è´¯æ€§

3. **å¤šå›¾ç†è§£èƒ½åŠ›**
   - æ¯”è¾ƒå¤šå¼ å›¾ç‰‡
   - è·¨å›¾æ¨ç†
   - å¤šå›¾å…³è”ç†è§£

4. **ç»†ç²’åº¦å®šä½èƒ½åŠ›**
   - ç”Ÿæˆè¾¹ç•Œæ¡†åæ ‡
   - ç†è§£ä½ç½®æè¿°
   - ç©ºé—´æ¨ç†

5. **ä¿æŒé€šç”¨èƒ½åŠ›**
   - çº¯æ–‡æœ¬å¯¹è¯
   - çŸ¥è¯†é—®ç­”
   - é˜²æ­¢ç¾éš¾æ€§é—å¿˜

---

## ä¸VQAçš„åŒºåˆ«

| ç»´åº¦ | VQAä»»åŠ¡ | Stage 3å¯¹è¯ä»»åŠ¡ |
|------|---------|----------------|
| **æ ¼å¼** | å•è½®é—®ç­” | å¤šè½®å¯¹è¯ |
| **è¾“å…¥** | å›¾åƒ+é—®é¢˜ | å›¾åƒ+å¤šè½®ä¸Šä¸‹æ–‡ |
| **è¾“å‡º** | çŸ­ç­”æ¡ˆ | è‡ªç„¶å¯¹è¯å›å¤ |
| **ç›®æ ‡** | ç­”å¯¹é—®é¢˜ | è‡ªç„¶äº¤äº’ |
| **è¯„ä¼°** | å‡†ç¡®ç‡ | GPT-4è¯„åˆ†/äººå·¥è¯„ä¼° |
| **æ•°æ®** | å›ºå®šQAå¯¹ | å¼€æ”¾å¯¹è¯ |

**ç¤ºä¾‹å¯¹æ¯”**:

```
ã€VQAé£æ ¼ - Stage 2ã€‘
Input: <img>dog.jpg</img> What is in the image? Answer:
Output: A dog.<eos>

ã€å¯¹è¯é£æ ¼ - Stage 3ã€‘
User: Picture 1: <img>dog.jpg</img> 
      è¿™å¼ å›¾é‡Œæœ‰ä»€ä¹ˆ?
Assistant: å›¾ä¸­æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬åœ¨æ²™æ»©ä¸Šç©è€ã€‚

User: å®ƒåœ¨åšä»€ä¹ˆ?
Assistant: å®ƒä¼¼ä¹åœ¨å’Œä¸»äººç©æ¥çƒæ¸¸æˆ,çœ‹èµ·æ¥å¾ˆå¼€å¿ƒã€‚
```

---

## Stage 3çš„è®­ç»ƒé…ç½®

**è®ºæ–‡Table 8è¯¦ç»†å‚æ•°**:

```python
# æ¨¡å‹åˆå§‹åŒ–
ViT init:         Qwen-VL 2nd-stage (å†»ç»“!)
LLM init:         Qwen-VL 2nd-stage
VL Adapter init:  Qwen-VL 2nd-stage

# è®­ç»ƒå‚æ•°
Image resolution: 448Ã—448
Peak LR:          1e-5
Min LR:           1e-6
Batch size:       128
Training steps:   8,000
Warm-up steps:    3,000

# å¯è®­ç»ƒå‚æ•°
ViT:              âœ— å†»ç»“
VL Adapter:       âœ“ è®­ç»ƒ
LLM:              âœ“ è®­ç»ƒ
```

**å…³é”®è®¾è®¡**:
- **å†»ç»“ViT**: è§†è§‰ç‰¹å¾æå–å·²ç»è¶³å¤Ÿå¥½
- **è®­ç»ƒLLM**: å­¦ä¹ å¯¹è¯æ¨¡å¼å’ŒæŒ‡ä»¤éµå¾ª
- **è®­ç»ƒAdapter**: å¾®è°ƒè§†è§‰-è¯­è¨€å¯¹é½

---

## Stage 3çš„è¯„ä¼°æ–¹å¼

ä¸Stage 1/2çš„VQAè¯„ä¼°ä¸åŒ,Stage 3ä½¿ç”¨**å¯¹è¯è´¨é‡è¯„ä¼°**:

### è¯„ä¼°Benchmark

1. **TouchStone** (GPT-4è¯„åˆ†)
   ```
   è¯„ä¼°ç»´åº¦: 27ä¸ªç±»åˆ«
   - å±æ€§é—®ç­”
   - åäººè¯†åˆ«
   - å†™è¯—
   - å¤šå›¾æ€»ç»“
   - äº§å“æ¯”è¾ƒ
   - æ•°å­¦è§£é¢˜
   ...
   
   Qwen-VL-Chatå¾—åˆ†:
   - è‹±æ–‡: 645.2 (vs mPLUG-Owl: 605.4)
   - ä¸­æ–‡: 401.2 (vs VisualGLM: 247.1)
   ```

2. **SEED-Bench** (å¤šé€‰é¢˜)
   ```
   ä»»åŠ¡: 19Kå¤šé€‰é¢˜
   ç»´åº¦: 12ä¸ª(åŒ…æ‹¬å›¾åƒ+è§†é¢‘)
   
   Qwen-VL-Chat: 65.4% (Image)
   ```

3. **MME** (æ„ŸçŸ¥+è®¤çŸ¥)
   ```
   æ„ŸçŸ¥ä»»åŠ¡: 1487.58
   è®¤çŸ¥ä»»åŠ¡: 360.71
   (å‡ä¸ºSOTA)
   ```

---

## Stage 3çš„ç‹¬ç‰¹ç‰¹æ€§

### 1. å¤šå›¾äº¤é”™å¯¹è¯

```
<im_start>user
Picture 1: <img>beijing.jpg</img>
Picture 2: <img>chongqing.jpg</img>
è¿™ä¸¤å¼ å›¾éƒ½æ˜¯å“ªé‡Œ?<im_end>

<im_start>assistant
ç¬¬ä¸€å¼ æ˜¯åŒ—äº¬çš„å¤©é™…çº¿,ç¬¬äºŒå¼ æ˜¯é‡åº†çš„å¤©é™…çº¿ã€‚<im_end>

<im_start>user
å®ƒä»¬æœ‰ä»€ä¹ˆåŒºåˆ«?<im_end>

<im_start>assistant
åŒ—äº¬çš„å»ºç­‘æ›´ç°ä»£åŒ–,è€Œé‡åº†æ˜¯å±±åŸ,å»ºç­‘ä¾å±±è€Œå»ºã€‚<im_end>
```

### 2. æ”¯æŒGroundingè¾“å‡º

```
User: æ¡†å‡ºå›¾ä¸­å‡»æŒçš„ä½ç½®
Assistant: <ref>å‡»æŒ</ref><box>(536,509),(588,602)</box>
```

### 3. ä¸­è‹±åŒè¯­å¯¹è¯

```
User: è¿™æ˜¯ä»€ä¹ˆå“ç§çš„ç‹—?
Assistant: è¿™æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚

User: Describe the dog's mood.
Assistant: The dog appears to be happy and energetic.
```

---

## ä¸‰é˜¶æ®µä»»åŠ¡å¯¹æ¯”æ€»ç»“

| é˜¶æ®µ | åç§° | æ ¸å¿ƒä»»åŠ¡ | æ•°æ®æ ¼å¼ | ç›®æ ‡ |
|------|------|----------|----------|------|
| **Stage 1** | Pre-training | Image Captioning | `<img>...</img>Caption:<eos>` | è§†è§‰-è¯­è¨€åŸºç¡€å¯¹é½ |
| **Stage 2** | Multi-task Pre-training | 7ç§ä»»åŠ¡(Caption/VQA/Grounding/OCRç­‰) | ä»»åŠ¡ç‰¹å®šæ ¼å¼ | ç»†ç²’åº¦å¤šä»»åŠ¡èƒ½åŠ› |
| **Stage 3** | **Supervised Fine-tuning** | **Multi-modal Dialogue** | **ChatMLå¯¹è¯æ ¼å¼** | **æŒ‡ä»¤éµå¾ª+è‡ªç„¶äº¤äº’** |

---

## å…³é”®æ´å¯Ÿ

### ä¸ºä»€ä¹ˆStage 3ä¸ç”¨VQA?

1. **VQAæ˜¯èƒ½åŠ›,å¯¹è¯æ˜¯åº”ç”¨**
   - Stage 2å·²ç»å­¦ä¼šäº†VQA
   - Stage 3è¦å­¦ä¼šå¦‚ä½•ä¸äººäº¤äº’

2. **å¯¹è¯åŒ…å«VQAä½†ä¸é™äºVQA**
   ```
   å¯¹è¯ âŠƒ VQA + å¤šè½®ä¸Šä¸‹æ–‡ + æŒ‡ä»¤ç†è§£ + è‡ªç„¶è¡¨è¾¾
   ```

3. **ç”¨æˆ·éœ€è¦çš„æ˜¯åŠ©æ‰‹,ä¸æ˜¯ç­”é¢˜æœºå™¨**
   - VQA: "A dog" (å†·å†°å†°)
   - Dialogue: "è¿™æ˜¯ä¸€åªå¯çˆ±çš„æ‹‰å¸ƒæ‹‰å¤šçŠ¬,å®ƒçœ‹èµ·æ¥å¾ˆå¼€å¿ƒ!" (æœ‰æ¸©åº¦)

---

## æ€»ç»“

**Qwen-VLç¬¬ä¸‰é˜¶æ®µçš„è®­ç»ƒä»»åŠ¡æ˜¯Multi-modal Dialogue(å¤šæ¨¡æ€å¯¹è¯)**,å…·ä½“ç‰¹ç‚¹:

âœ… **é˜¶æ®µåç§°**: Supervised Fine-tuning (ä¸æ˜¯é¢„è®­ç»ƒ!)  
âœ… **æ ¸å¿ƒä»»åŠ¡**: æŒ‡ä»¤éµå¾ªå¯¹è¯  
âœ… **æ•°æ®æ ¼å¼**: ChatMLå¯¹è¯æ ¼å¼  
âœ… **æ•°æ®è§„æ¨¡**: 350Kå¯¹è¯æ ·æœ¬  
âœ… **è®­ç»ƒç›®æ ‡**: äº§å‡ºQwen-VL-Chatäº¤äº’æ¨¡å‹  
âœ… **ç‹¬ç‰¹èƒ½åŠ›**: å¤šè½®å¯¹è¯ã€å¤šå›¾ç†è§£ã€ä¸­è‹±åŒè¯­  
âŒ **ä¸æ˜¯VQA**: VQAèƒ½åŠ›å·²åœ¨Stage 2è·å¾—,Stage 3æ˜¯å­¦ä¹ å¦‚ä½•å¯¹è¯

**å½¢è±¡æ¯”å–»**:
- Stage 1: å­¦ä¼š"çœ‹å›¾è¯´è¯"(Captioning)
- Stage 2: å­¦ä¼š"ç­”é¢˜"(VQA + Grounding + OCR)
- Stage 3: å­¦ä¼š"èŠå¤©"(Dialogue + Instruction Following)

è¿™ç§æ¸è¿›å¼è®­ç»ƒç­–ç•¥è®©Qwen-VLæ—¢æœ‰æ‰å®çš„è§†è§‰ç†è§£èƒ½åŠ›,åˆæœ‰å‡ºè‰²çš„äº¤äº’ä½“éªŒ!





# Qwen-VL æŠ€æœ¯æŠ¥å‘Šè¯¦è§£

## ä¸€ã€è®ºæ–‡åŸºæœ¬ä¿¡æ¯

**æ ‡é¢˜**: Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond

**ä½œè€…**: Jinze Bai, Shuai Bai, Shusheng Yangç­‰ (é˜¿é‡Œå·´å·´é›†å›¢)

**å‘è¡¨æ—¶é—´**: 2023å¹´8æœˆ (arXiv:2308.12966v3)

**å¼€æºåœ°å€**: https://github.com/QwenLM/Qwen-VL

---

## äºŒã€ç ”ç©¶èƒŒæ™¯ä¸åŠ¨æœº

### 2.1 ç°æœ‰é—®é¢˜

1. **å¼€æºLVLMè®­ç»ƒä¸è¶³**: å½“å‰å¼€æºå¤§è§„æ¨¡è§†è§‰-è¯­è¨€æ¨¡å‹(LVLMs)æ™®éå­˜åœ¨è®­ç»ƒå’Œä¼˜åŒ–ä¸å……åˆ†çš„é—®é¢˜ï¼Œè¿œè½åäºä¸“æœ‰æ¨¡å‹(å¦‚GPT-4Vã€Claude)

2. **ç²—ç²’åº¦ç†è§£å±€é™**: å¤§å¤šæ•°å¼€æºLVLMsåªèƒ½è¿›è¡Œç²—ç²’åº¦çš„å›¾åƒæ„ŸçŸ¥ï¼Œç¼ºä¹ç»†ç²’åº¦ç†è§£èƒ½åŠ›ï¼Œå¦‚**ç‰©ä½“å®šä½(grounding)**å’Œ**æ–‡æœ¬é˜…è¯»**

3. **å®é™…åº”ç”¨å—é™**: ç”±äºç¼ºä¹ç²¾ç»†åŒ–çš„è§†è§‰ç†è§£èƒ½åŠ›ï¼Œç°æœ‰æ¨¡å‹éš¾ä»¥æœ‰æ•ˆåœ°åœ¨å¤æ‚çœŸå®åœºæ™¯ä¸­è¾…åŠ©ç”¨æˆ·

### 2.2 ç ”ç©¶ç›®æ ‡

å¼€å‘ä¸€ä¸ª**é€šç”¨ä¸”é«˜æ€§èƒ½**çš„è§†è§‰-è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œå…·å¤‡ï¼š
- å›¾åƒæè¿°å’Œé—®ç­”
- **è§†è§‰å®šä½(Visual Grounding)**
- **æ–‡æœ¬é˜…è¯»(OCR)**
- **å¤šè¯­è¨€æ”¯æŒ**(è‹±æ–‡+ä¸­æ–‡ä¸ºä¸»)
- **å¤šå›¾åƒäº¤äº’å¯¹è¯**

---

## ä¸‰ã€æ¨¡å‹æ¶æ„è®¾è®¡

### 3.1 æ•´ä½“æ¶æ„

Qwen-VLé‡‡ç”¨**ä¸‰ç»„ä»¶æ¶æ„**ï¼Œæ€»å‚æ•°é‡**9.6B**ï¼š

```
[å›¾åƒè¾“å…¥] 
    â†“
[Visual Encoder (1.9B)]  â† ViT-bigG (OpenCLIPé¢„è®­ç»ƒ)
    â†“
[Position-aware VL Adapter (0.08B)]  â† Cross-Attentionå‹ç¼©
    â†“
[Large Language Model (7.7B)]  â† Qwen-7B
    â†“
[æ–‡æœ¬è¾“å‡º/è¾¹ç•Œæ¡†è¾“å‡º]
```

### 3.2 Visual Encoder (è§†è§‰ç¼–ç å™¨)

**è®¾è®¡ç»†èŠ‚**:
- **æ¶æ„**: Vision Transformer (ViT)
- **åˆå§‹åŒ–**: OpenCLIPçš„ViT-bigGé¢„è®­ç»ƒæƒé‡
- **å‚æ•°é‡**: 1.9B
- **åˆ†è¾¨ç‡**: 
  - Stage 1 (é¢„è®­ç»ƒ): 224Ã—224
  - Stage 2 (å¤šä»»åŠ¡é¢„è®­ç»ƒ): **448Ã—448** (æå‡ä¿¡æ¯å¯†åº¦)
  - Stage 3 (SFT): 448Ã—448
- **Patch size**: 14Ã—14
- **è¾“å‡ºåºåˆ—é•¿åº¦**: 
  - 224åˆ†è¾¨ç‡: (224/14)Â² = 256
  - 448åˆ†è¾¨ç‡: (448/14)Â² = 1024

**å…³é”®æ”¹è¿›**: 
- åœ¨Stage 2æå‡åˆ†è¾¨ç‡åˆ°448Ã—448ï¼Œå‡å°‘ä¸‹é‡‡æ ·ä¿¡æ¯æŸå¤±
- å®éªŒå¯¹æ¯”äº†Window Attention vs Global Attentionï¼Œæœ€ç»ˆé€‰æ‹©**Global Attention**ï¼ˆè™½ç„¶è®¡ç®—é‡å¤§ï¼Œä½†æ”¶æ•›æ€§èƒ½æ›´å¥½ï¼‰

### 3.3 Position-aware Vision-Language Adapter

**æ ¸å¿ƒåˆ›æ–°**: å¼•å…¥ä½ç½®æ„ŸçŸ¥æœºåˆ¶

**æ¶æ„è®¾è®¡**:
```python
# ä¼ªä»£ç ç¤ºæ„
class PositionAwareAdapter:
    def __init__(self):
        self.learnable_queries = nn.Embedding(256, hidden_dim)  # 256ä¸ªå¯å­¦ä¹ query
        self.cross_attention = CrossAttention()
        self.pos_encoding_2d = AbsolutePositionEncoding2D()  # 2Dä½ç½®ç¼–ç 
    
    def forward(self, image_features):
        # image_features: [batch, 1024, dim] for 448x448 images
        
        # æ·»åŠ 2Dä½ç½®ç¼–ç åˆ°query-keyå¯¹
        query = self.learnable_queries.weight
        key = image_features + self.pos_encoding_2d
        
        # Cross-Attentionå‹ç¼©: 1024 â†’ 256
        compressed_features = self.cross_attention(query, key, image_features)
        
        return compressed_features  # [batch, 256, dim]
```

**è®¾è®¡åŠ¨æœº**:
1. **æ•ˆç‡é—®é¢˜**: ç›´æ¥è¾“å…¥1024é•¿åº¦çš„è§†è§‰ç‰¹å¾åºåˆ—ä¼šå¯¼è‡´LLMè®¡ç®—å¼€é”€è¿‡å¤§
2. **ä¿¡æ¯ä¿ç•™**: å•å±‚Cross-Attentionå‹ç¼©åˆ°256é•¿åº¦
3. **ä½ç½®ä¿æŒ**: **2Dç»å¯¹ä½ç½®ç¼–ç **æ³¨å…¥åˆ°cross-attentionçš„query-keyå¯¹ä¸­ï¼Œç¼“è§£å‹ç¼©è¿‡ç¨‹ä¸­ä½ç½®ä¿¡æ¯çš„æŸå¤±

**æ¶ˆèå®éªŒ** (Appendix E.2):
- æµ‹è¯•äº†64ã€144ã€256ã€400å››ç§queryæ•°é‡
- **256**æ˜¯æœ€ä¼˜é€‰æ‹©ï¼š
  - 64å¤ªå°‘ï¼Œä¿¡æ¯æŸå¤±ä¸¥é‡
  - 400å¤ªå¤šï¼Œæ”¶æ•›å›°éš¾ä¸”è®¡ç®—é‡å¤§
  - 256åœ¨æ€§èƒ½å’Œæ•ˆç‡é—´è¾¾åˆ°æœ€ä½³å¹³è¡¡

### 3.4 Large Language Model

- **åŸºåº§æ¨¡å‹**: Qwen-7B (7.7Bå‚æ•°)
- **å†»ç»“ç­–ç•¥**:
  - Stage 1: **å†»ç»“LLM**ï¼Œåªè®­ç»ƒVision Encoderå’ŒAdapter
  - Stage 2: **è§£å†»LLM**ï¼Œç«¯åˆ°ç«¯è®­ç»ƒå…¨éƒ¨å‚æ•°
  - Stage 3: **å†»ç»“Vision Encoder**ï¼Œåªè®­ç»ƒLLMå’ŒAdapter

### 3.5 Input-Output Interface (è¾“å…¥è¾“å‡ºæ¥å£)

**å›¾åƒè¾“å…¥æ ¼å¼**:
```
<img>image_path.jpg</img>
```

**è¾¹ç•Œæ¡†æ ¼å¼è®¾è®¡**:

è¿™æ˜¯Qwen-VLçš„**é‡è¦åˆ›æ–°**ä¹‹ä¸€ï¼š

```
<ref>æè¿°å†…å®¹</ref><box>(x_topleft, y_topleft),(x_bottomright, y_bottomright)</box>
```

**å…³é”®ç‰¹ç‚¹**:
1. **å½’ä¸€åŒ–åæ ‡**: åæ ‡å½’ä¸€åŒ–åˆ°[0, 1000)èŒƒå›´
2. **å­—ç¬¦ä¸²åŒ–è¡¨ç¤º**: å°†åæ ‡ç›´æ¥è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œé€šè¿‡LLMçš„tokenizerå¤„ç†ï¼Œ**ä¸éœ€è¦é¢å¤–çš„ä½ç½®è¯æ±‡è¡¨**
3. **ç‰¹æ®Štokenæ ‡è®°**:
   - `<box>`, `</box>`: æ ‡è®°è¾¹ç•Œæ¡†å­—ç¬¦ä¸²
   - `<ref>`, `</ref>`: æ ‡è®°è¾¹ç•Œæ¡†æ‰€æŒ‡ä»£çš„å¯¹è±¡
   - `<img>`, `</img>`: æ ‡è®°å›¾åƒç‰¹å¾åºåˆ—

**ç¤ºä¾‹**:
```
<img>image.jpg</img>Generate the caption in English with grounding: 
Beautiful shot of <ref>bees</ref><box>(661,612),(833,812)</box> 
gathering nectars from <ref>an apricot flower</ref><box>(224,13),(399,313)</box>
```

---

## å››ã€è®­ç»ƒæµç¨‹

### 4.1 ä¸‰é˜¶æ®µè®­ç»ƒPipeline

```
Stage 1: Pre-training (50k steps, ~1.5B samples)
    â†“
Stage 2: Multi-task Pre-training (19k steps)
    â†“
Stage 3: Supervised Fine-tuning (8k steps, 350k samples)
```

### 4.2 Stage 1: é¢„è®­ç»ƒ (Pre-training)

**è®­ç»ƒç›®æ ‡**: å»ºç«‹åŸºç¡€çš„å›¾åƒ-æ–‡æœ¬å¯¹é½èƒ½åŠ›

**æ•°æ®è§„æ¨¡**: 
- **åŸå§‹æ•°æ®**: 50äº¿å›¾åƒ-æ–‡æœ¬å¯¹
- **æ¸…æ´—å**: 14äº¿å¯¹ (ä¿ç•™ç‡28%)
- **è¯­è¨€åˆ†å¸ƒ**: 77.3%è‹±æ–‡ + 22.7%ä¸­æ–‡

**æ•°æ®æ¥æº** (è¡¨2):
| æ•°æ®é›† | åŸå§‹é‡ | æ¸…æ´—å | ä¿ç•™ç‡ |
|--------|--------|--------|--------|
| LAION-en | 2B | 280M | 14% |
| LAION-COCO | 600M | 300M | 50% |
| DataComp | 1.4B | 300M | 21% |
| Coyo-700M | 700M | 200M | 28% |
| CC12M | 12M | 8M | 66% |
| CC3M | 3M | 3M | 100% |
| SBU | 1M | 0.8M | 80% |
| COCO Caption | 0.6M | 0.6M | 100% |
| LAION-zh | 108M | 105M | 97% |
| In-house Data | 220M | 220M | 100% |

**æ•°æ®æ¸…æ´—æµç¨‹** (Appendix A.1):
1. ç§»é™¤å®½é«˜æ¯”è¿‡å¤§çš„å›¾åƒ
2. ç§»é™¤è¿‡å°çš„å›¾åƒ
3. åŸºäºCLIP scoreè¿‡æ»¤(æ•°æ®é›†ç‰¹å®šé˜ˆå€¼)
4. ç§»é™¤åŒ…å«éè‹±æ–‡/éä¸­æ–‡å­—ç¬¦çš„æ–‡æœ¬
5. ç§»é™¤åŒ…å«emojiçš„æ–‡æœ¬
6. ç§»é™¤æ–‡æœ¬é•¿åº¦è¿‡çŸ­æˆ–è¿‡é•¿çš„æ ·æœ¬
7. æ¸…ç†HTMLæ ‡ç­¾
8. æ¸…ç†ç‰¹å®šä¸è§„åˆ™æ¨¡å¼

**è®­ç»ƒé…ç½®**:
- **å›¾åƒåˆ†è¾¨ç‡**: 224Ã—224
- **Batch size**: 30,720
- **ä¼˜åŒ–å™¨**: AdamW (Î²â‚=0.9, Î²â‚‚=0.98, eps=1e-6)
- **å­¦ä¹ ç‡**: 
  - Peak: 2eâ»â´
  - Min: 1eâ»â¶
  - Schedule: Cosine decay + 500æ­¥warm-up
- **ViTå­¦ä¹ ç‡è¡°å‡**: Layer-wise decay, factor=0.95
- **Weight decay**: 0.05
- **æ¢¯åº¦è£å‰ª**: 1.0
- **è®­ç»ƒæ­¥æ•°**: 50,000æ­¥
- **å‚æ•°å†»ç»“**: **å†»ç»“LLM**ï¼Œåªè®­ç»ƒViTå’ŒAdapter
- **Loss**: Next-token prediction (æ–‡æœ¬token)

**æ”¶æ•›æ›²çº¿åˆ†æ** (å›¾6):
- è®­ç»ƒlossç¨³å®šä¸‹é™
- Zero-shot VQAæ€§èƒ½åœ¨æ³¢åŠ¨ä¸­ä¸Šå‡
- Flickr30K CIDErä»62æå‡åˆ°76

### 4.3 Stage 2: å¤šä»»åŠ¡é¢„è®­ç»ƒ (Multi-task Pre-training)

**è®­ç»ƒç›®æ ‡**: å¼•å…¥**ç»†ç²’åº¦è§†è§‰ç†è§£èƒ½åŠ›**(Groundingã€OCR)å’Œ**äº¤é”™å›¾æ–‡æ•°æ®**

**æ•°æ®ç»„æˆ** (è¡¨3):

| ä»»åŠ¡ | æ ·æœ¬é‡ | æ•°æ®é›† |
|------|--------|--------|
| Captioning | 19.7M | LAION, DataComp, Coyo, CC12M/3M, SBU, COCO |
| VQA | 3.6M | GQA, VGQA, VQAv2, DVQA, OCR-VQA, DocVQA, TextVQA, ChartQA, AI2D |
| **Grounding** | 3.5M | **GRIT** |
| **Ref Grounding** | 8.7M | GRIT, Visual Genome, RefCOCO, RefCOCO+, RefCOCOg |
| **Grounded Cap.** | 8.7M | GRIT, Visual Genome, RefCOCO, RefCOCO+, RefCOCOg |
| **OCR** | 24.8M | **SynthDoG-en/zh**, Common Crawl PDF & HTML |
| Pure-text | 7.8M | In-house Data |

**å…³é”®æ•°æ®æ„å»º**:

1. **OCRæ•°æ®ç”Ÿæˆ** (Appendix A.4):
   - **SynthDoG**: ä½¿ç”¨COCOå›¾åƒä½œä¸ºèƒŒæ™¯ï¼Œåˆæˆè‹±æ–‡(41ç§å­—ä½“)å’Œä¸­æ–‡(11ç§å­—ä½“)æ–‡æœ¬
   - **PDFæ•°æ®**: ä½¿ç”¨PyMuPDFæ¸²æŸ“Common Crawlçš„PDFï¼Œæå–æ–‡æœ¬å’Œè¾¹ç•Œæ¡†
   - **HTMLæ•°æ®**: ä½¿ç”¨Puppeteeræ¸²æŸ“ç½‘é¡µï¼Œæå–æ–‡æœ¬å’Œè¾¹ç•Œæ¡†

2. **Groundingæ•°æ®å¤„ç†**:
   - **GRITæ•°æ®æ¸…æ´—**: å‘ç°é€’å½’åµŒå¥—çš„grounding boxæ ‡æ³¨ï¼Œä½¿ç”¨è´ªå¿ƒç®—æ³•æ¸…æ´—ï¼Œç¡®ä¿æ¯å¼ å›¾åƒåŒ…å«æœ€å¤šboxä¸”æ— é€’å½’åµŒå¥—

3. **äº¤é”™å›¾æ–‡æ•°æ®**:
   - å°†ç›¸åŒä»»åŠ¡çš„æ•°æ®æ‰“åŒ…æˆé•¿åº¦ä¸º2048çš„åºåˆ—
   - æ”¯æŒå¤šå›¾åƒè¾“å…¥

**è®­ç»ƒé…ç½®å˜åŒ–**:
- **å›¾åƒåˆ†è¾¨ç‡**: 224Ã—224 â†’ **448Ã—448** âš¡
- **ViTåºåˆ—é•¿åº¦**: 256 â†’ 1024
- **LLMåºåˆ—é•¿åº¦**: 512 â†’ 2048
- **Batch size**: 4,096
- **å­¦ä¹ ç‡**: 
  - Peak: 5eâ»âµ (é™ä½)
  - Min: 1eâ»âµ
- **è®­ç»ƒæ­¥æ•°**: 19,000æ­¥
- **å‚æ•°å†»ç»“**: **è§£å†»LLM**ï¼Œç«¯åˆ°ç«¯è®­ç»ƒ
- **æ¨¡å‹å¹¶è¡Œ**: 2è·¯å¹¶è¡Œ (ViTå’ŒLLM)

**æ•°æ®æ ¼å¼ç¤ºä¾‹** (Box B.1):
```
# Captioning
<img>cc3m/01581435.jpg</img>Generate the caption in English: 
the beautiful flowers for design.<eos>

# Grounded Captioning
<img>coyo700m/1.jpg</img>Generate the caption in English with grounding: 
Beautiful shot of <ref>bees</ref><box>(661,612),(833,812)</box><box>(120,555),(265,770)</box> 
gathering nectars from <ref>an apricot flower</ref><box>(224,13),(399,313)</box><eos>

# OCR
<img>synthdog/1.jpg</img>OCR with grounding: 
<ref>It is managed</ref><quad>(568,121),(625,131),(624,182),(567,172)</quad>...<eos>
```

### 4.4 Stage 3: ç›‘ç£å¾®è°ƒ (Supervised Fine-tuning)

**è®­ç»ƒç›®æ ‡**: æå‡**æŒ‡ä»¤è·Ÿéšèƒ½åŠ›**å’Œ**å¤šè½®å¯¹è¯èƒ½åŠ›**

**æ•°æ®è§„æ¨¡**: 350Kæ ·æœ¬

**æ•°æ®æ¥æº**:
1. **LLMè‡ªæˆ‘æŒ‡ä»¤ç”Ÿæˆ**çš„å¤šæ¨¡æ€å¯¹è¯æ•°æ®
2. **äººå·¥æ ‡æ³¨**çš„å¯¹è¯æ•°æ®
3. **ç­–ç•¥æ‹¼æ¥**æ„å»ºçš„å¯¹è¯æ•°æ®
4. **å¤šæ¨¡æ€+çº¯æ–‡æœ¬**æ··åˆå¯¹è¯æ•°æ®(ä¿æŒé€šç”¨å¯¹è¯èƒ½åŠ›)

**å…³é”®èƒ½åŠ›æ³¨å…¥**:
- **å¤šå›¾åƒç†è§£**: å›¾åƒå‰æ·»åŠ "Picture id:"å‰ç¼€
- **å®šä½èƒ½åŠ›**: groundingå’Œå¤šå›¾åƒç†è§£çš„æ³›åŒ–
- **å¤šè½®å¯¹è¯**: ChatMLæ ¼å¼

**ChatMLæ ¼å¼** (Appendix B.2):
```
<im_start>user
Picture 1: <img>vg/VG_100K_2/649.jpg</img>What is the sign in the picture?<im_end>
<im_start>assistant
The sign is a road closure with an orange rhombus.<im_end>
<im_start>user
How is the weather in the picture?<im_end>
<im_start>assistant
The shape of the road closure sign is an orange rhombus.<im_end>
```

**Lossè®¡ç®—**: åªå¯¹**assistantçš„å›å¤**å’Œ**ç‰¹æ®Štoken**è®¡ç®—lossï¼Œä¸å¯¹roleåç§°å’Œé—®é¢˜è®¡ç®—loss

**è®­ç»ƒé…ç½®**:
- **å›¾åƒåˆ†è¾¨ç‡**: 448Ã—448
- **Batch size**: 128
- **å­¦ä¹ ç‡**: 
  - Peak: 1eâ»âµ
  - Min: 1eâ»â¶
- **è®­ç»ƒæ­¥æ•°**: 8,000æ­¥
- **Warm-up**: 3,000æ­¥
- **å‚æ•°å†»ç»“**: **å†»ç»“ViT**ï¼Œè®­ç»ƒLLMå’ŒAdapter
- **ViTå­¦ä¹ ç‡è¡°å‡**: 0 (å®Œå…¨å†»ç»“)

---

## äº”ã€å®éªŒç»“æœ

### 5.1 å›¾åƒæè¿°å’Œé€šç”¨VQA (è¡¨4)

**Image Captioning**:
| æ¨¡å‹ | Nocaps (0-shot) | Flickr30K (0-shot) |
|------|-----------------|---------------------|
| Flamingo-80B | - | 67.2 |
| BLIP-2 (Vicuna-13B) | 103.9 | 71.6 |
| InstructBLIP (Vicuna-13B) | 121.9 | 82.8 |
| **Qwen-VL** | **121.4** | **85.8** âš¡ |
| Qwen-VL-Chat | 120.2 | 81.0 |
| SOTA (PALI-17B) | 127.0 | 84.5 |

**å…³é”®å‘ç°**: Qwen-VLåœ¨Flickr30Kä¸Šè¾¾åˆ°**85.8 CIDEr**ï¼Œè¶…è¶ŠFlamingo-80B (67.2)ï¼Œç”šè‡³è¶…è¶ŠSOTA

**General VQA**:
| æ¨¡å‹ | VQAv2 | OKVQA | GQA | SciQA-Img | VizWiz |
|------|-------|-------|-----|-----------|--------|
| BLIP-2 (Vicuna-13B) | 65.0 | 45.9 | 32.3 | 61.0 | 19.6 |
| InstructBLIP (Vicuna-13B) | - | - | 49.5 | 63.1 | 33.4 |
| Shikra (Vicuna-13B) | 77.36 | 47.16 | - | - | - |
| **Qwen-VL** | **79.5** | **58.6** | **59.3** | **67.1** | **35.2** |
| Qwen-VL-Chat | 78.2 | 56.6 | 57.5 | 68.2 | 38.9 |

**æ€§èƒ½äº®ç‚¹**:
- VQAv2: 79.5% (å¤§å¹…è¶…è¶ŠBLIP-2çš„65.0%)
- OKVQA: 58.6% (éœ€è¦å¤–éƒ¨çŸ¥è¯†)
- GQA: 59.3% (åœºæ™¯ç†è§£å’Œæ¨ç†)

### 5.2 æ–‡æœ¬å¯¼å‘VQA (è¡¨5)

| æ¨¡å‹ | TextVQA | DocVQA | ChartQA | AI2D | OCR-VQA |
|------|---------|--------|---------|------|---------|
| BLIP-2 (Vicuna-13B) | 42.4 | - | - | - | - |
| InstructBLIP (Vicuna-13B) | 50.7 | - | - | - | - |
| mPLUG-DocOwl (LLaMA-7B) | 52.6 | 62.2 | 57.4 | - | - |
| Pix2Struct-Large (1.3B) | - | 76.6 | 58.6 | 42.1 | 71.3 |
| **Qwen-VL** | **63.8** | **65.1** | **65.7** | **62.3** | **75.7** |
| Qwen-VL-Chat | 61.5 | 62.6 | 66.3 | 57.7 | 70.5 |

**å…³é”®äº®ç‚¹**:
- TextVQA: **63.8%** vs InstructBLIPçš„50.7% (+13.1%)
- åœ¨æ‰€æœ‰æ–‡æœ¬å¯¼å‘ä»»åŠ¡ä¸Šå…¨é¢é¢†å…ˆå¼€æºæ¨¡å‹

### 5.3 è§†è§‰å®šä½ (è¡¨6)

**RefCOCOç³»åˆ—**:
| æ¨¡å‹ | RefCOCO val | RefCOCO testA | RefCOCO testB |
|------|-------------|---------------|---------------|
| Shikra-13B | 87.83 | 91.11 | 81.81 |
| **Qwen-VL-7B** | **89.36** | **92.26** | **85.34** |
| Qwen-VL-Chat | 88.55 | 92.27 | 84.51 |
| G-DINO-L (SOTA) | 90.56 | 93.19 | 88.24 |

**GRIT (RefExp)**:
- Qwen-VL: **78.22%**
- Shikra-13B: 69.03%
- æå‡9.2ä¸ªç™¾åˆ†ç‚¹

**æ€§èƒ½åˆ†æ**:
- åœ¨æ‰€æœ‰groundingä»»åŠ¡ä¸Š**æ˜¾è‘—è¶…è¶Š**åŒç­‰è§„æ¨¡çš„generalistæ¨¡å‹
- æ¥è¿‘ä¸“é—¨çš„grounding SOTAæ¨¡å‹(G-DINO-L)

### 5.4 Few-shot Learning (å›¾4)

**æµ‹è¯•åŸºå‡†**: OKVQA, VizWiz, TextVQA, Flickr30k

**å¯¹æ¯”æ¨¡å‹**: Flamingo-9B/80B, OpenFlamingo-9B, IDEFICS-9B/80B

**å…³é”®å‘ç°**:
- Qwen-VL (9.6B) çš„few-shotæ€§èƒ½**è¶…è¶ŠFlamingo-9B**
- **æ¥è¿‘Flamingo-80B**çš„æ€§èƒ½
- éªŒè¯äº†æ¨¡å‹çš„**in-context learningèƒ½åŠ›**

### 5.5 æŒ‡ä»¤è·Ÿéšèƒ½åŠ› (è¡¨7)

| æ¨¡å‹ | TouchStone-En | TouchStone-Cn | SEED-Bench-All | MME-Perception | MME-Cognition |
|------|---------------|---------------|----------------|----------------|---------------|
| MiniGPT4 | 531.7 | - | 42.8 | 581.67 | 144.29 |
| InstructBLIP | 552.4 | - | 53.4 | 1212.82 | 291.79 |
| LLaVA | 602.7 | - | 33.5 | 502.82 | 214.64 |
| mPLUG-Owl | 605.4 | - | 34.0 | 967.34 | 276.07 |
| **Qwen-VL-Chat** | **645.2** | **401.2** | **58.2** | **1487.58** | **360.71** |

**æ€§èƒ½ä¼˜åŠ¿**:
- TouchStoneè‹±æ–‡: **645.2** (ç¬¬2åmPLUG-Owl: 605.4)
- TouchStoneä¸­æ–‡: **401.2** (å¤§å¹…é¢†å…ˆ)
- MME-Perception: **1487.58** (æ„ŸçŸ¥èƒ½åŠ›)
- MME-Cognition: **360.71** (è®¤çŸ¥èƒ½åŠ›)

**ç»†åˆ†èƒ½åŠ›ä¼˜åŠ¿** (TouchStone):
- æ–‡æœ¬è¯†åˆ«(Text Recognition)
- å›¾è¡¨åˆ†æ(Chart Analysis)
- å®šä½èƒ½åŠ›(Localization)

---

## å…­ã€å…³é”®æŠ€æœ¯åˆ›æ–°ç‚¹æ€»ç»“

### 6.1 æ¶æ„åˆ›æ–°

1. **Position-aware Adapter**
   - 2Dç»å¯¹ä½ç½®ç¼–ç æ³¨å…¥cross-attention
   - ç¼“è§£ç‰¹å¾å‹ç¼©è¿‡ç¨‹ä¸­çš„ä½ç½®ä¿¡æ¯æŸå¤±
   - 256ä¸ªlearnable queriesè¾¾åˆ°æ€§èƒ½ä¸æ•ˆç‡å¹³è¡¡

2. **è¾¹ç•Œæ¡†å­—ç¬¦ä¸²åŒ–è¡¨ç¤º**
   - ä¸éœ€è¦é¢å¤–ä½ç½®è¯æ±‡è¡¨
   - ç›´æ¥é€šè¿‡LLM tokenizerå¤„ç†
   - å½’ä¸€åŒ–åˆ°[0, 1000)èŒƒå›´

### 6.2 è®­ç»ƒç­–ç•¥åˆ›æ–°

1. **ä¸‰é˜¶æ®µæ¸è¿›å¼è®­ç»ƒ**
   - Stage 1: å†»ç»“LLMï¼Œå»ºç«‹åŸºç¡€å¯¹é½
   - Stage 2: è§£å†»LLMï¼Œæå‡åˆ†è¾¨ç‡ï¼Œå¼•å…¥ç»†ç²’åº¦ä»»åŠ¡
   - Stage 3: å†»ç»“ViTï¼Œå¢å¼ºå¯¹è¯èƒ½åŠ›

2. **åˆ†è¾¨ç‡æ¸è¿›æå‡**
   - 224Ã—224 â†’ 448Ã—448
   - å‡å°‘ä¿¡æ¯æŸå¤±ï¼Œæå‡ç»†ç²’åº¦ç†è§£

3. **çº¯æ–‡æœ¬æ•°æ®æ··åˆ**
   - åœ¨Stage 2å’ŒStage 3å¼•å…¥çº¯æ–‡æœ¬æ•°æ®
   - é˜²æ­¢catastrophic forgetting
   - çº¯æ–‡æœ¬èƒ½åŠ›ä¸é™åå‡(è¡¨11)

### 6.3 æ•°æ®åˆ›æ–°

1. **å¤§è§„æ¨¡OCRæ•°æ®åˆæˆ**
   - SynthDoG: 24.8Mæ ·æœ¬
   - PDF/HTMLæ¸²æŸ“ + è‡ªåŠ¨æ ‡æ³¨
   - æ”¯æŒè‹±æ–‡å’Œä¸­æ–‡

2. **Grounded Captionæ•°æ®**
   - 8.7M image-caption-boxä¸‰å…ƒç»„
   - åŒæ—¶è®­ç»ƒgroundingå’Œcaptionèƒ½åŠ›

3. **ä¸¥æ ¼çš„æ•°æ®æ¸…æ´—**
   - 50äº¿ â†’ 14äº¿ (ä¿ç•™ç‡28%)
   - å¤šè¯­è¨€ã€å¤šä»»åŠ¡ã€é«˜è´¨é‡

### 6.4 èƒ½åŠ›åˆ›æ–°

1. **å¤šå›¾åƒäº¤äº’**: Picture idæœºåˆ¶
2. **ç»†ç²’åº¦å®šä½**: è¾¹ç•Œæ¡†è¾“å…¥è¾“å‡º
3. **ä¸­è‹±åŒè¯­**: 22.7%ä¸­æ–‡æ•°æ®
4. **Few-shotèƒ½åŠ›**: æ¥è¿‘80Bæ¨¡å‹
5. **OCRèƒ½åŠ›**: TextVQA 63.8%

---

## ä¸ƒã€æ¶ˆèå®éªŒä¸åˆ†æ

### 7.1 Learnable Queriesæ•°é‡ (å›¾7, Appendix E.2)

**å®éªŒè®¾ç½®**: 64, 144, 256, 400

**ç»“æœ**:
- **åˆå§‹loss**: queriesè¶Šå°‘ï¼Œåˆå§‹lossè¶Šä½(å› ä¸ºæ›´ç®€å•)
- **æ”¶æ•›æ€§èƒ½**: 256æœ€ä¼˜
  - 64: ä¿¡æ¯æŸå¤±ä¸¥é‡ï¼Œæ”¶æ•›åæ€§èƒ½å·®
  - 400: æ”¶æ•›å›°éš¾ï¼Œè®­ç»ƒä¸ç¨³å®š
  - 256: æ€§èƒ½ä¸æ•ˆç‡æœ€ä½³å¹³è¡¡

### 7.2 Window Attention vs Global Attention (å›¾8, è¡¨10, Appendix E.3)

**æµ‹è¯•é…ç½®**:
- 448Ã—448 + Window Attention: 9s/iter
- 448Ã—448 + Global Attention: 10s/iter
- 896Ã—896 + Window Attention: 25s/iter
- 896Ã—896 + Global Attention: 60s/iter

**ç»“æœ**:
- Window Attentionè™½ç„¶å¿«ï¼Œä½†**lossæ˜¾è‘—æ›´é«˜**
- **æœ€ç»ˆé€‰æ‹©**: 448Ã—448 + Global Attention
  - è®­ç»ƒé€Ÿåº¦å¯æ¥å—(10s/iter)
  - æ”¶æ•›æ€§èƒ½æœ€ä¼˜
  - 896Ã—896è¿‡æ…¢(60s/iter)ï¼Œä¸é‡‡ç”¨

## 7.3 çº¯æ–‡æœ¬èƒ½åŠ›

ä¸ºäº†ç ”ç©¶å¤šæ¨¡æ€è®­ç»ƒå¯¹çº¯æ–‡æœ¬èƒ½åŠ›çš„å½±å“,è®ºæ–‡å±•ç¤ºäº†Qwen-VLä¸å¼€æºLLMåœ¨çº¯æ–‡æœ¬ä»»åŠ¡ä¸Šçš„æ€§èƒ½å¯¹æ¯”(Table 11)ã€‚

**åˆå§‹åŒ–è¯´æ˜**:
- Qwen-VLä½¿ç”¨Qwen-7Bçš„**ä¸­é—´æ£€æŸ¥ç‚¹**(intermediate checkpoint)ä½œä¸ºLLMåˆå§‹åŒ–
- ä¹‹æ‰€ä»¥æ²¡æœ‰ä½¿ç”¨Qwen-7Bçš„æœ€ç»ˆå‘å¸ƒç‰ˆæœ¬,æ˜¯å› ä¸ºQwen-VLå’ŒQwen-7Båœ¨éå¸¸ç›¸ä¼¼çš„æ—¶æœŸå¼€å‘

**çº¯æ–‡æœ¬åŸºå‡†æµ‹è¯•ç»“æœ**:

| æ¨¡å‹ | MMLU | CMMLU | C-Eval |
|------|------|-------|--------|
| LLaMA-7B | 35.1 | 26.8 | - |
| LLaMA2-7B | 46.8 | 31.8 | 32.5 |
| Baichuan-7B | 42.3 | 44.4 | 42.8 |
| Baichuan2-7B | 54.2 | 57.1 | 54.0 |
| ChatGLM2-6B | 47.9 | 48.8 | 51.7 |
| InternLM-7B | 51.0 | 51.8 | 52.8 |
| **Qwen-7B (æœ€ç»ˆç‰ˆ)** | 58.2 | 62.2 | 63.5 |
| **Qwen-7B (ä¸­é—´ç‰ˆ,ç”¨ä½œQwen-VLåˆå§‹åŒ–)** | 49.9 | - | 48.5 |
| **Qwen-VL** | **50.7** | **49.5** | **51.1** |

**å…³é”®å‘ç°**:

1. **é˜²æ­¢ç¾éš¾æ€§é—å¿˜**: åœ¨å¤šä»»åŠ¡è®­ç»ƒ(Stage 2)å’Œç›‘ç£å¾®è°ƒ(Stage 3)é˜¶æ®µ,Qwen-VLä¸ä»…ä½¿ç”¨è§†è§‰-è¯­è¨€æ•°æ®,è¿˜æ··å…¥äº†**çº¯æ–‡æœ¬æ•°æ®**è¿›è¡Œè®­ç»ƒ,ç›®çš„æ˜¯é˜²æ­¢æ–‡æœ¬ç†è§£èƒ½åŠ›çš„ç¾éš¾æ€§é—å¿˜(catastrophic forgetting)

2. **èƒ½åŠ›ä¿æŒä¸æå‡**: å¯¹æ¯”ç»“æœè¡¨æ˜,Qwen-VLåœ¨çº¯æ–‡æœ¬èƒ½åŠ›ä¸Š:
   - **æ²¡æœ‰é€€åŒ–**: ç›¸æ¯”åˆå§‹åŒ–çš„ä¸­é—´æ£€æŸ¥ç‚¹,å„é¡¹æŒ‡æ ‡å‡æœ‰æå‡
   - MMLU: 49.9 â†’ 50.7 (+0.8)
   - CMMLU: â†’ 49.5 (æ–°å¢)
   - C-Eval: 48.5 â†’ 51.1 (+2.6)

3. **ä¸çº¯æ–‡æœ¬LLMç›¸å½“**: ç”±äºQwen-7Bæä¾›äº†è‰¯å¥½çš„LLMåˆå§‹åŒ–,Qwen-VLåœ¨çº¯æ–‡æœ¬ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸è®¸å¤šçº¯æ–‡æœ¬LLMç›¸å½“,ç”šè‡³è¶…è¿‡äº†LLaMA2-7Bã€Baichuan-7Bç­‰æ¨¡å‹

**æ··åˆè®­ç»ƒç­–ç•¥çš„æœ‰æ•ˆæ€§**:
- é€šè¿‡åœ¨è§†è§‰-è¯­è¨€è®­ç»ƒä¸­æ··å…¥çº¯æ–‡æœ¬æ•°æ®,Qwen-VLæˆåŠŸä¿ç•™äº†LLMçš„åŸç”Ÿæ–‡æœ¬ç†è§£èƒ½åŠ›
- è¿™ç§ç­–ç•¥è¯æ˜äº†å¤šæ¨¡æ€æ¨¡å‹å¯ä»¥åœ¨è·å¾—è§†è§‰èƒ½åŠ›çš„åŒæ—¶,ä¸ç‰ºç‰²æ–‡æœ¬èƒ½åŠ›

---

## å…«ã€æ•°æ®é›†è¯¦ç»†ä¿¡æ¯

### 8.1 å›¾åƒ-æ–‡æœ¬å¯¹æ•°æ®æ¸…æ´—

è®ºæ–‡ä½¿ç”¨çš„ç½‘ç»œçˆ¬å–å›¾åƒ-æ–‡æœ¬å¯¹æ•°æ®é›†åŒ…æ‹¬:
- LAION-en, LAION-zh (Schuhmann et al., 2022a)
- LAION-COCO (Schuhmann et al., 2022b)
- DataComp (Gadre et al., 2023)
- Coyo (Byeon et al., 2022)

**æ¸…æ´—æ­¥éª¤**:
1. ç§»é™¤å›¾åƒå®½é«˜æ¯”è¿‡å¤§çš„æ ·æœ¬
2. ç§»é™¤å›¾åƒå°ºå¯¸è¿‡å°çš„æ ·æœ¬
3. ç§»é™¤CLIPåˆ†æ•°è¿‡ä½çš„æ ·æœ¬(æ•°æ®é›†ç‰¹å®šé˜ˆå€¼)
4. ç§»é™¤åŒ…å«éè‹±æ–‡æˆ–éä¸­æ–‡å­—ç¬¦çš„æ–‡æœ¬
5. ç§»é™¤åŒ…å«emojiå­—ç¬¦çš„æ–‡æœ¬
6. ç§»é™¤æ–‡æœ¬é•¿åº¦è¿‡çŸ­æˆ–è¿‡é•¿çš„æ ·æœ¬
7. æ¸…ç†æ–‡æœ¬ä¸­çš„HTMLæ ‡ç­¾éƒ¨åˆ†
8. æ¸…ç†å…·æœ‰ç‰¹å®šä¸è§„åˆ™æ¨¡å¼çš„æ–‡æœ¬

å¯¹äºå­¦æœ¯captionæ•°æ®é›†(CC12M, SBUç­‰),è¿˜ç§»é™¤äº†åŒ…å«ç‰¹æ®Šæ ‡ç­¾çš„æ ·æœ¬,å¹¶é€‰æ‹©æœ€é•¿çš„æ–‡æœ¬ä½œä¸ºæ ‡æ³¨ã€‚

### 8.2 OCRæ•°æ®ç”Ÿæˆ

**åˆæˆOCRæ•°æ®**:
- ä½¿ç”¨**Synthdog** (Kim et al., 2022)ç”Ÿæˆ
- èƒŒæ™¯å›¾åƒ: COCO train2017 å’Œ unlabeled2017ä½œä¸ºè‡ªç„¶åœºæ™¯èƒŒæ™¯
- å­—ä½“é€‰æ‹©: 41ç§è‹±æ–‡å­—ä½“ + 11ç§ä¸­æ–‡å­—ä½“
- åæ ‡æ ‡æ³¨: ç”Ÿæˆæ–‡æœ¬çš„å››è¾¹å½¢åæ ‡ä½œä¸ºè®­ç»ƒæ ‡ç­¾

**PDFæ•°æ®å¤„ç†**(ä½¿ç”¨PyMuPDF):
1. æå–æ¯é¡µçš„æ‰€æœ‰æ–‡æœ¬åŠå…¶è¾¹ç•Œæ¡†
2. æ¸²æŸ“æ¯é¡µå¹¶ä¿å­˜ä¸ºå›¾åƒæ–‡ä»¶
3. ç§»é™¤è¿‡å°çš„å›¾åƒ
4. ç§»é™¤å­—ç¬¦æ•°é‡è¿‡å¤šæˆ–è¿‡å°‘çš„å›¾åƒ
5. ç§»é™¤åŒ…å«"Latin Extended-A/B"å—ä¸­Unicodeå­—ç¬¦çš„å›¾åƒ
6. ç§»é™¤åŒ…å«"Private Use Area (PUA)"å—ä¸­Unicodeå­—ç¬¦çš„å›¾åƒ

**HTMLç½‘é¡µå¤„ç†**(ä½¿ç”¨Puppeteer):
æµç¨‹ä¸PDFç±»ä¼¼,ä½†ä½¿ç”¨Puppeteeræ›¿ä»£PyMuPDFæ¥æ¸²æŸ“HTMLé¡µé¢å¹¶è·å–çœŸå®æ ‡æ³¨ã€‚

---

## ä¹ã€è®­ç»ƒæ”¶æ•›æ€§åˆ†æ

### 9.1 é¢„è®­ç»ƒé˜¶æ®µæ”¶æ•›æ›²çº¿(Figure 6)

**è®­ç»ƒé…ç½®**:
- æ··åˆç²¾åº¦: BFloat16
- Batch size: 30720
- å­¦ä¹ ç‡: 2eâ»â´
- è®­ç»ƒè½®æ•°: 1 epoch(æ‰€æœ‰å›¾åƒåªè®­ç»ƒä¸€æ¬¡)

**å…³é”®è§‚å¯Ÿ**:
1. **è®­ç»ƒæŸå¤±**: éšç€è®­ç»ƒå›¾åƒæ•°é‡å¢åŠ ,æŸå¤±ç¨³å®šä¸‹é™
2. **Zero-shot Captionèƒ½åŠ›**(Flickr30K): ä»62 CIDEræå‡è‡³76 CIDEr
3. **Zero-shot VQAèƒ½åŠ›**(VQAv2): ä»48%æå‡è‡³çº¦56%,å°½ç®¡Stage 1æ²¡æœ‰æ·»åŠ VQAæ•°æ®

è¿™è¡¨æ˜å¤§è§„æ¨¡å›¾åƒ-æ–‡æœ¬å¯¹çš„é¢„è®­ç»ƒä¸ºæ¨¡å‹å»ºç«‹äº†è‰¯å¥½çš„è§†è§‰-è¯­è¨€å¯¹é½åŸºç¡€ã€‚

### 9.2 Learnable Queriesæ•°é‡æ¶ˆèå®éªŒ(Figure 7)

**å®éªŒè®¾ç½®**: ä½¿ç”¨ViT-L/14,è¾“å…¥åˆ†è¾¨ç‡224Ã—224,è¾“å‡ºåºåˆ—é•¿åº¦256

**æµ‹è¯•çš„Queryæ•°é‡**: 64, 144, 256, 400

**ç»“æœ**:
- **è®­ç»ƒåˆæœŸ**(å‰50æ­¥): Queryè¶Šå°‘,åˆå§‹æŸå¤±è¶Šä½
- **æ”¶æ•›é˜¶æ®µ**(1k-5kæ­¥): Queryè¿‡å¤šæˆ–è¿‡å°‘éƒ½ä¼šå¯¼è‡´æ”¶æ•›å˜æ…¢
- **æœ€ç»ˆé€‰æ‹©**: 256ä¸ªqueries
  - åŸå› : Stage 2ä½¿ç”¨448Ã—448åˆ†è¾¨ç‡,ViTè¾“å‡ºåºåˆ—é•¿åº¦ä¸º1024,å¤ªå°‘çš„queriesä¼šå¯¼è‡´ä¿¡æ¯ä¸¢å¤±

### 9.3 Window Attention vs Global Attentionå¯¹æ¯”(Table 10 & Figure 8)

**è®­ç»ƒé€Ÿåº¦å¯¹æ¯”**:

| è¾“å…¥åˆ†è¾¨ç‡ & Attentionç±»å‹ | è®­ç»ƒé€Ÿåº¦ |
|--------------------------|---------|
| 448Ã—448, Global Attention | 10s/iter |
| 448Ã—448, Window Attention | 9s/iter |
| 896Ã—896, Global Attention | 60s/iter |
| 896Ã—896, Window Attention | 25s/iter |

**æŸå¤±å¯¹æ¯”**:
- ä½¿ç”¨Window Attentionæ—¶,æ¨¡å‹æŸå¤±æ˜¾è‘—æ›´é«˜
- 448Ã—448åˆ†è¾¨ç‡ä¸‹,ä¸¤ç§æ–¹æ¡ˆè®­ç»ƒé€Ÿåº¦ç›¸è¿‘

**æœ€ç»ˆå†³ç­–**: 
- Qwen-VLåœ¨Vision Transformerä¸­ä½¿ç”¨**Global Attention**
- ä¸ä½¿ç”¨896Ã—896åˆ†è¾¨ç‡,å› ä¸ºè®­ç»ƒé€Ÿåº¦è¿‡æ…¢(å³ä½¿ä½¿ç”¨Window Attentionä¹Ÿéœ€è¦2.5å€æ—¶é—´)

---

## åã€æ€»ç»“ä¸æœªæ¥å·¥ä½œ

### 10.1 Qwen-VLçš„æ ¸å¿ƒè´¡çŒ®

1. **å…¨é¢çš„è§†è§‰-è¯­è¨€èƒ½åŠ›**: é›†æˆäº†å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ã€OCRã€æ–‡æ¡£ç†è§£å’Œè§†è§‰å®šä½èƒ½åŠ›
2. **SOTAæ€§èƒ½**: åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šå–å¾—åŒç­‰è§„æ¨¡é€šç”¨æ¨¡å‹çš„æœ€ä½³æ€§èƒ½
3. **å¤šè¯­è¨€æ”¯æŒ**: å¤©ç„¶æ”¯æŒè‹±æ–‡ã€ä¸­æ–‡å’Œå¤šè¯­è¨€æŒ‡ä»¤
4. **ç»†ç²’åº¦ç†è§£**: é€šè¿‡é«˜åˆ†è¾¨ç‡è¾“å…¥(448Ã—448)å’Œç»†ç²’åº¦è¯­æ–™,å®ç°å‡ºè‰²çš„å®šä½å’Œæ–‡æœ¬è¯†åˆ«èƒ½åŠ›
5. **å¤šå›¾åƒå¯¹è¯**: æ”¯æŒä»»æ„äº¤é”™çš„å›¾åƒ-æ–‡æœ¬æ•°æ®ä½œä¸ºè¾“å…¥
6. **å¼€æºè´¡çŒ®**: æ‰€æœ‰æ¨¡å‹å…¬å¼€å‘å¸ƒ,ä¿ƒè¿›å¤šæ¨¡æ€ç ”ç©¶å‘å±•

### 10.2 æœªæ¥å‘å±•æ–¹å‘

è®ºæ–‡æå‡ºäº†ä¸‰ä¸ªä¸»è¦çš„æœªæ¥ç ”ç©¶æ–¹å‘:

1. **æ¨¡æ€æ‰©å±•**:
   - å°†Qwen-VLä¸æ›´å¤šæ¨¡æ€é›†æˆ,å¦‚è¯­éŸ³å’Œè§†é¢‘
   - æ„å»ºçœŸæ­£çš„å¤šæ¨¡æ€ç»Ÿä¸€æ¨¡å‹

2. **è§„æ¨¡æå‡**:
   - é€šè¿‡æ‰©å¤§æ¨¡å‹è§„æ¨¡ã€è®­ç»ƒæ•°æ®å’Œæ›´é«˜åˆ†è¾¨ç‡
   - ä½¿å…¶èƒ½å¤Ÿå¤„ç†æ›´å¤æ‚å’Œæ›´ç²¾ç»†çš„å¤šæ¨¡æ€æ•°æ®å…³ç³»

3. **ç”Ÿæˆèƒ½åŠ›å¢å¼º**:
   - æ‰©å±•Qwen-VLçš„å¤šæ¨¡æ€ç”Ÿæˆèƒ½åŠ›
   - ç‰¹åˆ«æ˜¯ç”Ÿæˆé«˜ä¿çœŸå›¾åƒå’Œæµç•…è¯­éŸ³

---

## åä¸€ã€æŠ€æœ¯äº®ç‚¹æ€»ç»“

### 11.1 æ¶æ„è®¾è®¡äº®ç‚¹

1. **Position-aware Adapter**:
   - ä½¿ç”¨2Dç»å¯¹ä½ç½®ç¼–ç çš„cross-attentionæœºåˆ¶
   - åœ¨å‹ç¼©è§†è§‰ç‰¹å¾æ—¶ä¿ç•™ä½ç½®ä¿¡æ¯,å¯¹ç»†ç²’åº¦ç†è§£è‡³å…³é‡è¦

2. **Bounding Boxå­—ç¬¦ä¸²åŒ–è¡¨ç¤º**:
   - å°†åæ ‡å½’ä¸€åŒ–åˆ°[0, 1000)èŒƒå›´
   - ä½¿ç”¨å­—ç¬¦ä¸²æ ¼å¼è¡¨ç¤º,æ— éœ€é¢å¤–çš„ä½ç½®è¯æ±‡è¡¨
   - é€šè¿‡ç‰¹æ®Štoken(<box>, </box>, <ref>, </ref>)æ ‡è¯†å®šä½ä¿¡æ¯

3. **ç®€æ´é«˜æ•ˆçš„æ¶æ„**:
   - æ€»å‚æ•°9.6B: ViT-bigG(1.9B) + Adapter(0.08B) + Qwen-7B(7.7B)
   - 256ä¸ªlearnable querieså°†è§†è§‰åºåˆ—å‹ç¼©åˆ°å›ºå®šé•¿åº¦

### 11.2 è®­ç»ƒç­–ç•¥äº®ç‚¹

1. **ä¸‰é˜¶æ®µæ¸è¿›å¼è®­ç»ƒ**:
   - Stage 1: å¤§è§„æ¨¡å›¾åƒ-æ–‡æœ¬å¯¹é¢„è®­ç»ƒ(1.4Bæ ·æœ¬)
   - Stage 2: å¤šä»»åŠ¡é¢„è®­ç»ƒ,å¼•å…¥ç»†ç²’åº¦æ ‡æ³¨
   - Stage 3: æŒ‡ä»¤å¾®è°ƒ(350Kå¯¹è¯æ•°æ®)

2. **åˆ†è¾¨ç‡é€æ­¥æå‡**:
   - Stage 1: 224Ã—224(å†»ç»“LLM)
   - Stage 2: 448Ã—448(è§£å†»LLM)
   - é€æ­¥æå‡é¿å…è®­ç»ƒä¸ç¨³å®š

3. **çº¯æ–‡æœ¬æ•°æ®æ··åˆè®­ç»ƒ**:
   - åœ¨Stage 2å’ŒStage 3æ··å…¥çº¯æ–‡æœ¬æ•°æ®
   - æˆåŠŸé˜²æ­¢ç¾éš¾æ€§é—å¿˜,ä¿æŒLLMåŸç”Ÿèƒ½åŠ›

### 11.3 æ•°æ®ç­–ç•¥äº®ç‚¹

1. **å¤§è§„æ¨¡æ•°æ®æ¸…æ´—**: ä»5BåŸå§‹æ•°æ®æ¸…æ´—è‡³1.4Bé«˜è´¨é‡æ ·æœ¬(ä¿ç•™ç‡28%)
2. **åˆæˆOCRæ•°æ®**: 24.8Mæ ·æœ¬,è¦†ç›–è‹±æ–‡å’Œä¸­æ–‡
3. **ç»†ç²’åº¦å®šä½æ•°æ®**: GRITã€Visual Genomeã€RefCOCOç³»åˆ—ç­‰
4. **ChatMLæ ¼å¼**: ä½¿ç”¨æ ‡å‡†å¯¹è¯æ ¼å¼,æ”¯æŒå¤šè½®å¯¹è¯

---

## åäºŒã€Qwen-VL vs ç«å“å¯¹æ¯”

### 12.1 ä¸å…¶ä»–å¼€æºæ¨¡å‹å¯¹æ¯”

**ä¼˜åŠ¿é¢†åŸŸ**:
1. **OCRå’Œæ–‡æ¡£ç†è§£**: åœ¨TextVQAã€DocVQAç­‰ä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶ŠBLIP-2ã€InstructBLIP
2. **ç»†ç²’åº¦å®šä½**: åœ¨RefCOCOç³»åˆ—ä¸Šæ¥è¿‘ä¸“ç”¨SOTAæ¨¡å‹
3. **ä¸­è‹±åŒè¯­èƒ½åŠ›**: TouchStoneä¸­æ–‡è¯„åˆ†401.2,è¿œè¶…å…¶ä»–æ¨¡å‹
4. **Few-shotå­¦ä¹ **: æ€§èƒ½æ¥è¿‘å‚æ•°é‡10å€çš„Flamingo-80B

**ä¸Specialist SOTAçš„å·®è·**:
- åœ¨æŸäº›ä»»åŠ¡ä¸Šä»æœ‰å·®è·(å¦‚Captionçš„CIDErã€Groundingçš„å‡†ç¡®ç‡)
- ä½†Qwen-VLä½œä¸ºé€šç”¨æ¨¡å‹,åœ¨å¹¿åº¦ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿

### 12.2 æŠ€æœ¯åˆ›æ–°ç‚¹å¯¹æ¯”

| æ¨¡å‹ | è§†è§‰ç¼–ç å™¨ | LLMåŸºåº§ | å®šä½èƒ½åŠ› | OCRèƒ½åŠ› | å¤šè¯­è¨€ |
|------|----------|---------|---------|---------|--------|
| BLIP-2 | EVA-CLIP | Vicuna-13B | âœ— | å¼± | è‹±æ–‡ä¸ºä¸» |
| InstructBLIP | EVA-CLIP | Vicuna-13B | âœ— | å¼± | è‹±æ–‡ä¸ºä¸» |
| Kosmos-2 | CLIP | Decoder | âœ“ | ä¸­ç­‰ | å¤šè¯­è¨€ |
| Shikra | CLIP | Vicuna-13B | âœ“ | å¼± | è‹±æ–‡ä¸ºä¸» |
| **Qwen-VL** | ViT-bigG | Qwen-7B | âœ“ | **å¼º** | **ä¸­è‹±åŒè¯­** |

---

## ç»“è®º

Qwen-VLç³»åˆ—æ¨¡å‹é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ¶æ„ã€æ¸è¿›å¼è®­ç»ƒç­–ç•¥å’Œå¤§è§„æ¨¡å¤šä»»åŠ¡æ•°æ®,å®ç°äº†åœ¨åŒç­‰è§„æ¨¡é€šç”¨æ¨¡å‹ä¸­çš„é¢†å…ˆæ€§èƒ½ã€‚å…¶åœ¨ç»†ç²’åº¦è§†è§‰ç†è§£(OCRã€å®šä½)ã€å¤šè¯­è¨€æ”¯æŒå’Œå¯¹è¯èƒ½åŠ›æ–¹é¢çš„ä¼˜åŠ¿,ä½¿å…¶æˆä¸ºè§†è§‰-è¯­è¨€ç ”ç©¶å’Œåº”ç”¨çš„é‡è¦åŸºç¡€æ¨¡å‹ã€‚

**è®ºæ–‡é¡µæ•°**: 24é¡µ  
**å‘è¡¨ä¿¡æ¯**: arXiv:2308.12966v3 [cs.CV] 13 Oct 2023  
**ä½œè€…æœºæ„**: Alibaba Group  
**ä»£ç å’Œæ¨¡å‹**: https://github.com/QwenLM/Qwen-VL




