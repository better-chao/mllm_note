

## ğŸ‘¤ User

åˆ†æä¸€ä¸‹qween vlçš„ä¸»è¦æ¶æ„,è®­ç»ƒæ•°æ®,è®­ç»ƒloss,è¯„ä¼°çš„ä»»åŠ¡æŒ‡æ ‡,å¦å¤–ç»“åˆè¿™ä¸ªè®ºæ–‡çš„é¡¹ç›®ä»£ç ç»™å‡ºåˆ†æ
# Qwen-VL è¯¦ç»†æŠ€æœ¯åˆ†ææŠ¥å‘Š

## ä¸€ã€æ¨¡å‹æ¶æ„è¯¦è§£

### 1.1 æ•´ä½“æ¶æ„è®¾è®¡

Qwen-VLé‡‡ç”¨**ä¸‰ç»„ä»¶æ¶æ„**,æ€»å‚æ•°é‡ä¸º**9.6B**:

| ç»„ä»¶ | å‚æ•°é‡ | åˆå§‹åŒ–æ¥æº |
|------|--------|-----------|
| Visual Encoder (ViT) | 1.9B | OpenCLIP ViT-bigG |
| VL Adapter | 0.08B | éšæœºåˆå§‹åŒ– |
| Large Language Model | 7.7B | Qwen-7B |

### 1.2 æ ¸å¿ƒç»„ä»¶åˆ†æ

#### **Visual Encoder (è§†è§‰ç¼–ç å™¨)**
- æ¶æ„:Vision Transformer (ViT)
- åˆå§‹åŒ–:OpenCLIPçš„ViT-bigGé¢„è®­ç»ƒæƒé‡
- è¾“å…¥å¤„ç†:
  - Stage 1:224Ã—224åˆ†è¾¨ç‡
  - Stage 2-3:448Ã—448åˆ†è¾¨ç‡(æå‡ç»†ç²’åº¦ç†è§£)
  - Patch stride:14
  - è¾“å‡ºåºåˆ—é•¿åº¦:(448/14)Â² = 1024

#### **Position-aware Vision-Language Adapter (ä½ç½®æ„ŸçŸ¥é€‚é…å™¨)**
è¿™æ˜¯Qwen-VLçš„**å…³é”®åˆ›æ–°**:

```
è¾“å…¥:ViTè¾“å‡ºçš„1024é•¿åº¦ç‰¹å¾åºåˆ—
æ ¸å¿ƒæœºåˆ¶:å•å±‚Cross-Attention
- Query:256ä¸ªå¯å­¦ä¹ çš„embeddingå‘é‡
- Key/Value:æ¥è‡ªViTçš„å›¾åƒç‰¹å¾
- ä½ç½®ç¼–ç :2Dç»å¯¹ä½ç½®ç¼–ç èå…¥query-keyå¯¹
è¾“å‡º:å‹ç¼©åçš„256é•¿åº¦ç‰¹å¾åºåˆ—
```

**è®¾è®¡äº®ç‚¹**:
1. **ç‰¹å¾å‹ç¼©**:1024â†’256,æå‡æ•ˆç‡
2. **ä½ç½®ä¿ç•™**:é€šè¿‡2Dä½ç½®ç¼–ç ä¿ç•™ç©ºé—´ä¿¡æ¯,æ”¯æŒç»†ç²’åº¦ç†è§£
3. **æ¶ˆèå®éªŒ**:è®ºæ–‡æµ‹è¯•äº†64/144/256/400ä¸ªæŸ¥è¯¢å‘é‡,256ä¸ºæœ€ä¼˜

#### **Large Language Model**
- åŸºåº§:Qwen-7B(7.7Bå‚æ•°)
- ç‰¹æ®Štokenæ‰©å±•:
  - å›¾åƒæ ‡è®°:`<img>` `</img>`
  - è¾¹ç•Œæ¡†:`<box>` `</box>`
  - å¼•ç”¨æ ‡è®°:`<ref>` `</ref>`
  - å¯¹è¯æ ‡è®°:`<im_start>` `<im_end>`

### 1.3 è¾“å…¥è¾“å‡ºæ¥å£

#### **è¾¹ç•Œæ¡†è¡¨ç¤ºåˆ›æ–°**
```
å½’ä¸€åŒ–åæ ‡:[0, 1000)èŒƒå›´
æ ¼å¼:
- è¾“å…¥/è¾“å‡º:"<box>(Xtopleft,Ytopleft),(Xbottomright,Ybottomright)</box>"
- å…³è”æ–‡æœ¬:"<ref>æè¿°æ–‡æœ¬</ref><box>...</box>"
```

**ä¼˜åŠ¿**:
- æ— éœ€é¢å¤–ä½ç½®è¯æ±‡è¡¨
- ç›´æ¥tokenizationä¸ºæ–‡æœ¬
- æ”¯æŒä¸­è‹±æ–‡grounding

---

## äºŒã€è®­ç»ƒæ•°æ®è¯¦è§£

### 2.1 ä¸‰é˜¶æ®µè®­ç»ƒæ•°æ®

#### **Stage 1: é¢„è®­ç»ƒ(1.4Bæ ·æœ¬)**

æ•°æ®æ¸…æ´—ç­–ç•¥:
1. è¿‡æ»¤è¿‡å¤§å®½é«˜æ¯”å›¾åƒ
2. ç§»é™¤è¿‡å°å›¾åƒ
3. æ•°æ®é›†ç‰¹å®šçš„CLIPåˆ†æ•°è¿‡æ»¤
4. ç§»é™¤éè‹±æ–‡/ä¸­æ–‡å­—ç¬¦
5. æ¸…ç†HTMLæ ‡ç­¾å’Œä¸è§„åˆ™æ¨¡å¼

| æ•°æ®æº | åŸå§‹é‡ | æ¸…æ´—å | ä¿ç•™ç‡ | è¯­è¨€ |
|--------|--------|--------|--------|------|
| LAION-en | 2B | 280M | 14% | è‹±æ–‡ |
| LAION-COCO | 600M | 300M | 50% | è‹±æ–‡ |
| DataComp | 1.4B | 300M | 21% | è‹±æ–‡ |
| Coyo | 700M | 200M | 28% | è‹±æ–‡ |
| CC12M | 12M | 8M | 66% | è‹±æ–‡ |
| CC3M | 3M | 3M | 100% | è‹±æ–‡ |
| SBU | 1M | 0.8M | 80% | è‹±æ–‡ |
| COCO Caption | 0.6M | 0.6M | 100% | è‹±æ–‡ |
| LAION-zh | 108M | 105M | 97% | ä¸­æ–‡ |
| In-house Data | 220M | 220M | 100% | ä¸­æ–‡ |
| **æ€»è®¡** | **5B** | **1.4B** | **28%** | 77.3%è‹±æ–‡/22.7%ä¸­æ–‡ |

#### **Stage 2: å¤šä»»åŠ¡é¢„è®­ç»ƒ**

7å¤§ä»»åŠ¡å¹¶è¡Œè®­ç»ƒ:

| ä»»åŠ¡ç±»å‹ | æ ·æœ¬æ•° | æ•°æ®é›† |
|---------|--------|-------|
| **Captioning** | 19.7M | LAION-en/zh, DataComp, Coyo, CC12M/3M, SBU, COCO |
| **VQA** | 3.6M | GQA, VGQA, VQAv2, DVQA, OCR-VQA, DocVQA, TextVQA, ChartQA, AI2D |
| **Grounding** | 3.5M | GRIT (ç”Ÿæˆåè¯/çŸ­è¯­grounded captions) |
| **Ref Grounding** | 8.7M | GRIT, Visual Genome, RefCOCOç³»åˆ— |
| **Grounded Cap** | 8.7M | åŒRef Grounding |
| **OCR** | 24.8M | **SynthDoG-en/zh**, Common Crawl PDF/HTML |
| **çº¯æ–‡æœ¬** | 7.8M | In-houseæ•°æ®(é˜²æ­¢ç¾éš¾æ€§é—å¿˜) |

**OCRæ•°æ®ç”Ÿæˆ**:
- èƒŒæ™¯:COCO train2017/unlabeled2017
- å­—ä½“:41ç§è‹±æ–‡ + 11ç§ä¸­æ–‡
- å·¥å…·:SynthDoG(Kim et al., 2022)
- åæ ‡æ ‡æ³¨:å››è¾¹å½¢åæ ‡

#### **Stage 3: æŒ‡ä»¤å¾®è°ƒ(350Kæ ·æœ¬)**

æ•°æ®æ„æˆ:
1. **LLMè‡ªæŒ‡ä»¤ç”Ÿæˆ**:caption/å¯¹è¯æ•°æ®
2. **äººå·¥æ ‡æ³¨**:å®šä½+å¤šå›¾ç†è§£å¯¹è¯
3. **æ¨¡å‹ç”Ÿæˆ**:ç»“åˆç­–ç•¥æ‹¼æ¥
4. **çº¯æ–‡æœ¬å¯¹è¯**:ä¿æŒé€šç”¨å¯¹è¯èƒ½åŠ›

### 2.2 æ•°æ®æ ¼å¼

**Multi-taské¢„è®­ç»ƒæ ¼å¼**:
```
Image Captioning:
<img>path</img>Generate the caption in English: [caption]<eos>

VQA:
<img>path</img> [question] Answer: [answer]<eos>

Caption with Grounding:
<img>path</img>Generate the caption in English with grounding:
<ref>object</ref><box>(x1,y1),(x2,y2)</box>...<eos>

OCR:
<img>path</img>OCR with grounding: 
<ref>text</ref><quad>(x1,y1),(x2,y2),(x3,y3),(x4,y4)</quad>...<eos>
```

**SFTå¯¹è¯æ ¼å¼(ChatML)**:
```
<im_start>user
Picture 1: <img>path</img>é—®é¢˜å†…å®¹<im_end>
<im_start>assistant
å›ç­”å†…å®¹<im_end>
```

---

## ä¸‰ã€è®­ç»ƒLossä¸ä¼˜åŒ–ç­–ç•¥

### 3.1 è®­ç»ƒç›®æ ‡

**ç»Ÿä¸€Loss**:æ ‡å‡†è¯­è¨€å»ºæ¨¡äº¤å‰ç†µæŸå¤±
```
Loss = CrossEntropy(text_tokens)
```

**å…³é”®è®¾è®¡**:
- ä»…å¯¹æ–‡æœ¬tokenè®¡ç®—loss
- å›¾åƒç‰¹å¾åºåˆ—ä¸å‚ä¸lossè®¡ç®—
- SFTé˜¶æ®µ:ä»…ç›‘ç£å›ç­”å’Œç‰¹æ®Štoken,ä¸ç›‘ç£è§’è‰²åå’Œé—®é¢˜

### 3.2 è®­ç»ƒè¶…å‚æ•°

#### **Stage 1: é¢„è®­ç»ƒ**
```python
Optimizer: AdamW (Î²1=0.9, Î²2=0.98, eps=1e-6)
Learning Rate: 
  - Peak: 2e-4
  - Min: 1e-6
  - Schedule: Cosine decay
  - Warmup: 500 steps
  - ViT layer-wise decay: 0.95

Batch Size: 30720
Training Steps: 50,000 (~1.5Bæ ·æœ¬)
Weight Decay: 0.05
Gradient Clip: 1.0
Precision: BFloat16
Trainable: ViT + VL Adapter (LLMå†»ç»“)
```

#### **Stage 2: å¤šä»»åŠ¡é¢„è®­ç»ƒ**
```python
Learning Rate:
  - Peak: 5e-5
  - Min: 1e-5
  - Warmup: 400 steps

Batch Size: 4096
Training Steps: 19,000
Model Parallelism: 2 (ViT + LLM)
Trainable: å…¨æ¨¡å‹è§£å†»
Resolution: 448Ã—448
```

#### **Stage 3: SFT**
```python
Learning Rate:
  - Peak: 1e-5
  - Min: 1e-6
  - Warmup: 3,000 steps

Batch Size: 128
Training Steps: 8,000
Trainable: LLM + VL Adapter (ViTå†»ç»“)
Data: 350Kå¯¹è¯æ ·æœ¬
```

### 3.3 æ”¶æ•›æ›²çº¿åˆ†æ

è®ºæ–‡Figure 6æ˜¾ç¤º:
- **Lossç¨³å®šä¸‹é™**:ä»3.0é™è‡³1.8
- **Captionèƒ½åŠ›æå‡**:Flickr CIDErä»62â†’76
- **Zero-shot VQAæ¶Œç°**:VQAv2ä»48%â†’56%(æ— VQAè®­ç»ƒæ•°æ®!)

---

## å››ã€è¯„ä¼°ä»»åŠ¡ä¸æŒ‡æ ‡ä½“ç³»

### 4.1 æ ‡å‡†Benchmarkè¯„ä¼°

#### **å›¾åƒæè¿°(Image Captioning)**

| Benchmark | Metric | Qwen-VL | SOTAå¯¹æ¯” |
|-----------|--------|---------|----------|
| Nocaps (val) | CIDErâ†‘ | 121.4 | InstructBLIP:121.9 |
| **Flickr30K** | CIDErâ†‘ | **85.8** | Flamingo-80B:67.2 |

#### **é€šç”¨VQA**

| Benchmark | Metric | Qwen-VL | Qwen-VL-Chat | Specialist SOTA |
|-----------|--------|---------|--------------|-----------------|
| **VQAv2** | Accâ†‘ | **79.5** | 78.2 | PALI-X-55B:86.1 |
| **OKVQA** | Accâ†‘ | **58.6** | 56.6 | PALI-X-55B:66.1 |
| **GQA** | EMâ†‘ | **59.3** | 57.5 | CFR:72.1 |
| ScienceQA-Img | Accâ†‘ | 67.1 | **68.2** | LLaVA+GPT4:92.5 |
| VizWiz | VQA Scoreâ†‘ | 35.2 | **38.9** | PALI-X-55B:70.9 |

#### **æ–‡æœ¬å¯¼å‘VQA**

| Benchmark | Metric | Qwen-VL | å¯¹æ¯”æ¨¡å‹æœ€ä½³ |
|-----------|--------|---------|-------------|
| **TextVQA** | VQA Scoreâ†‘ | **63.8** | InstructBLIP:50.7 |
| DocVQA | ANLSâ†‘ | 65.1 | Pix2Struct-Large:76.6 |
| **ChartQA** | Relaxed EMâ†‘ | **65.7** | Pix2Struct-Large:58.6 |
| **AI2D** | EMâ†‘ | **62.3** | Pix2Struct-Large:42.1 |
| **OCR-VQA** | EMâ†‘ | **75.7** | Pix2Struct-Large:71.3 |

**å…³é”®å‘ç°**:Qwen-VLåœ¨448åˆ†è¾¨ç‡ä¸‹è¶…è¶Š1024åˆ†è¾¨ç‡çš„Pix2Struct

#### **Referring Expression Comprehension**

åœ¨RefCOCO/RefCOCO+/RefCOCOg/GRITä¸Š**å…¨é¢SOTA**(generalistæ¨¡å‹):

| Dataset | Metric | Qwen-VL-7B | Shikra-13B |
|---------|--------|-----------|------------|
| RefCOCO val | Accâ†‘ | **89.36** | 87.83 |
| RefCOCO+ testB | Accâ†‘ | **77.21** | 74.41 |
| RefCOCOg test | Accâ†‘ | **85.48** | 83.16 |
| GRIT refexp | Accâ†‘ | **78.22** | 69.03 |

### 4.2 Few-shotå­¦ä¹ 

åœ¨OKVQA/VizWiz/TextVQA/Flickr30Kä¸Š:
- **Qwen-VL-9B** â‰ˆ **Flamingo-80B** (8.9xå‚æ•°ä¼˜åŠ¿)
- 4-shotæ€§èƒ½æ˜¾è‘—æå‡

### 4.3 æŒ‡ä»¤éµå¾ªè¯„ä¼°

#### **TouchStone**
| Model | è‹±æ–‡å¾—åˆ† | ä¸­æ–‡å¾—åˆ† |
|-------|---------|---------|
| mPLUG-Owl | 605.4 | - |
| Qwen-VL-Chat | **645.2** | **401.2** |
| Qwen-VL-Chat-1.1 | **711.6** | **481.7** |

#### **SEED-Bench**
| Model | Image | Video | All |
|-------|-------|-------|-----|
| InstructBLIP | 58.8 | 38.1 | 53.4 |
| Qwen-VL | **62.3** | **39.1** | **56.3** |
| Qwen-VL-Chat | **65.4** | 37.8 | **58.2** |

#### **MME**
| Model | Perception | Cognition |
|-------|-----------|-----------|
| InstructBLIP | 1212.82 | 291.79 |
| Qwen-VL-Chat | **1487.58** | **360.71** |

---

## äº”ã€ä»£ç å®ç°åˆ†æ

### 5.1 æ ¸å¿ƒä»£ç ç»“æ„

æ ¹æ®finetune.py:312-315:
```python
if training_args.fix_vit and hasattr(model,'transformer') and hasattr(model.transformer,'visual'):
    model.transformer.visual.requires_grad_(False)
    if hasattr(model.transformer.visual,'attn_pool'):
        model.transformer.visual.attn_pool.requires_grad_(True)
```

**è®­ç»ƒç­–ç•¥**:
- **Stage 1**: ViT + VL Adapterå¯è®­ç»ƒ,LLMå†»ç»“
- **Stage 2**: å…¨éƒ¨è§£å†»
- **Stage 3(SFT)**: LLM + VL Adapterå¯è®­ç»ƒ,ViTå†»ç»“(**VL Adapterä¸­çš„attn_poolé™¤å¤–**)

### 5.2 æ•°æ®å¤„ç†(finetune.py:124-175)

**é¢„å¤„ç†æµç¨‹**:
```python
1. è§’è‰²æ˜ å°„:
   roles = {"user": "<|im_start|>user", "assistant": "<|im_start|>assistant"}

2. Token IDä½¿ç”¨:
   - im_start = tokenizer.im_start_id
   - im_end = tokenizer.im_end_id
   
3. Lossæ©ç ç­–ç•¥:
   - System prompt: IGNORE_TOKEN_ID
   - Userè¾“å…¥: IGNORE_TOKEN_ID
   - Assistantå›ç­”: æ­£å¸¸tokenè®¡ç®—loss
   - ç‰¹æ®Štoken(<im_end>ç­‰): è®¡ç®—loss
```

### 5.3 LoRA/Q-LoRAæ”¯æŒ

#### **LoRAé…ç½®**(finetune.py:54-63)
```python
lora_r: 64
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ["c_attn", "attn.c_proj", "w1", "w2"]
```

#### **æ˜¾å­˜å¯¹æ¯”**(å•A100-80G,batch_size=1):

| æ–¹æ³• | 384 tokens | 2048 tokens |
|------|-----------|-------------|
| LoRA (Base) | 37.1G / 2.3s | 38.7G / 6.1s |
| LoRA (Chat) | 23.3G / 2.2s | 27.3G / 5.9s |
| Q-LoRA | 17.0G / 4.2s | 19.3G / 7.9s |

**å…³é”®å·®å¼‚**:
- **LoRA (Base)**:éœ€è®­ç»ƒembedding + output layer(æ–°å¢special tokens)
- **LoRA (Chat)**:æ— éœ€è®­ç»ƒè¿™äº›å±‚(å·²åŒ…å«ChatML tokens)
- **Q-LoRA**:4-bité‡åŒ–,æœ€çœæ˜¾å­˜ä½†é€Ÿåº¦è¾ƒæ…¢

### 5.4 è¯„ä¼°è„šæœ¬

æ ¹æ®README:
- æ‰€æœ‰benchmarkè¯„ä¼°è„šæœ¬ä½äº`eval_mm/`
- æ”¯æŒçš„è¯„ä¼°:
  - Image Captioning: eval_mm/evaluate_caption.py
  - VQA: eval_mm/evaluate_vqa.py
  - Grounding: eval_mm/evaluate_grounding.py
  - Multiple Choice: eval_mm/evaluate_multiple_choice.py
  - MME: eval_mm/mme/eval.py
  - SEED-Bench: eval_mm/seed_bench/eval.py

---

## å…­ã€æ ¸å¿ƒåˆ›æ–°ç‚¹æ€»ç»“

### 6.1 æ¶æ„åˆ›æ–°
1. **Position-aware Adapter**:2Dä½ç½®ç¼–ç  + Cross-Attentionå‹ç¼©
2. **ç»Ÿä¸€æ¥å£**:è¾¹ç•Œæ¡†ä½œä¸ºæ–‡æœ¬åºåˆ—,æ— éœ€ä¸“ç”¨è¯æ±‡è¡¨
3. **å¤šåˆ†è¾¨ç‡è®­ç»ƒ**:224â†’448æå‡ç»†ç²’åº¦èƒ½åŠ›

### 6.2 è®­ç»ƒåˆ›æ–°
1. **ä¸‰é˜¶æ®µè®­ç»ƒ**:é¢„è®­ç»ƒâ†’å¤šä»»åŠ¡â†’SFT,é€æ­¥å¢å¼ºèƒ½åŠ›
2. **OCRæ•°æ®åˆæˆ**:SynthDoGç”Ÿæˆ24.8Mé«˜è´¨é‡OCRæ ·æœ¬
3. **é˜²ç¾éš¾æ€§é—å¿˜**:Stage 2/3æ··åˆçº¯æ–‡æœ¬æ•°æ®

### 6.3 èƒ½åŠ›çªç ´
1. **é¦–ä¸ªæ”¯æŒä¸­æ–‡groundingçš„é€šç”¨æ¨¡å‹**
2. **Zero-shot grounding**:æœªè§ä¸­æ–‡groundingæ•°æ®ä¹Ÿèƒ½æ³›åŒ–
3. **å¤šå›¾äº¤é”™å¯¹è¯**:æ”¯æŒå›¾åƒæ¯”è¾ƒå’Œå¤šå›¾ç†è§£
4. **ç»†ç²’åº¦è¯†åˆ«**:448åˆ†è¾¨ç‡è¶…è¶Š1024åˆ†è¾¨ç‡æ¨¡å‹

---

## ä¸ƒã€æ€§èƒ½Benchmarkå¯¹æ¯”

### ç»¼åˆèƒ½åŠ›é›·è¾¾å›¾(è®ºæ–‡Figure 1)

åœ¨9å¤§ä»»åŠ¡ä¸Šä¸åŒè§„æ¨¡generalistæ¨¡å‹å¯¹æ¯”,Qwen-VLåœ¨**7/9ä»»åŠ¡ä¸Šé¢†å…ˆ**:
- âœ… é¢†å…ˆ:VQAv2, Flickr30K, RefCOCO, OKVQA, GQA, TextVQA, VizWiz
- â‰ˆ æŒå¹³:Nocaps
- âŒ è½å:AI2D(ä½†ä»æ˜¯generalistæ¨¡å‹æœ€ä½³)

### ä¸ä¸“ç”¨SOTAå¯¹æ¯”

| ä»»åŠ¡ | Qwen-VL | Specialist SOTA | å·®è· |
|------|---------|-----------------|------|
| Caption | 85.8 | 127.0 (PALI-17B) | -41.2 |
| VQAv2 | 79.5 | 86.1 (PALI-X-55B) | -6.6 |
| Grounding | 89.36 | 92.64 (UNINEXT-H) | -3.28 |
| OCR-VQA | 75.7 | 75.0 (PALI-X-55B) | **+0.7** |

**ç»“è®º**:åœ¨OCRä»»åŠ¡ä¸Šå·²**è¶…è¶Šä¸“ç”¨æ¨¡å‹**

---

## å…«ã€ä»£ç åº“åŠŸèƒ½å®Œæ•´æ€§

### 8.1 æ”¯æŒçš„åŠŸèƒ½

âœ… **æ¨ç†**:
- ğŸ¤— Transformersé›†æˆ
- ğŸ¤– ModelScopeé›†æˆ
- é‡åŒ–æ¨ç†(Int4)
- Web UI Demo(web_demo_mm.py)
- OpenAI APIå…¼å®¹(openai_api.py)

âœ… **å¾®è°ƒ**:
- Full-parameter finetuning
- LoRA (finetune/finetune_lora_ds.sh)
- Q-LoRA (finetune/finetune_qlora_ds.sh)
- DeepSpeed ZeRO-2/3æ”¯æŒ

âœ… **è¯„ä¼°**:
- å®Œæ•´benchmarkè¯„ä¼°è„šæœ¬(eval_mm/)
- TouchStoneè¯„ä¼°(touchstone/)
- MME/SEED-Benchè¯„ä¼°

### 8.2 é‡åŒ–æ€§èƒ½(Int4)

| Metric | BF16 | Int4 | å·®å¼‚ |
|--------|------|------|------|
| TouchStone-EN | 645.2 | 651.4 | +6.2 |
| TouchStone-CN | 401.2 | 386.6 | -14.6 |
| æ¨ç†é€Ÿåº¦(8192 tokens) | 24.32 tok/s | **34.34 tok/s** | +41% |
| æ˜¾å­˜(ç”Ÿæˆ8192 tokens) | 28.01GB | **17.23GB** | -38% |

**ç»“è®º**:Int4å‡ ä¹æ— æŸæ€§èƒ½,æ˜¾è‘—æå‡æ•ˆç‡

---

## ä¹ã€å±€é™æ€§ä¸æœªæ¥æ–¹å‘

### 9.1 å½“å‰å±€é™
1. **åˆ†è¾¨ç‡ä¸Šé™**:448Ã—448ç›¸æ¯”GPT-4Vçš„é«˜åˆ†è¾¨ç‡ä»æœ‰å·®è·
2. **ä¸“ç”¨ä»»åŠ¡å·®è·**:åœ¨æŸäº›ä»»åŠ¡ä¸Šä¸ä¸“ç”¨æ¨¡å‹ä»æœ‰6-40åˆ†å·®è·
3. **ä¸­æ–‡èƒ½åŠ›**:è™½é¢†å…ˆå¼€æºæ¨¡å‹,ä½†éƒ¨åˆ†ä»»åŠ¡å¼±äºGPT-4V/Gemini

### 9.2 è®ºæ–‡æå‡ºçš„æœªæ¥æ–¹å‘
1. âœ¨ å¤šæ¨¡æ€æ‰©å±•:é›†æˆè¯­éŸ³ã€è§†é¢‘
2. ğŸ“ˆ è§„æ¨¡å‡çº§:æ›´å¤§æ¨¡å‹ã€æ›´å¤šæ•°æ®ã€æ›´é«˜åˆ†è¾¨ç‡
3. ğŸ¨ ç”Ÿæˆèƒ½åŠ›:é«˜ä¿çœŸå›¾åƒç”Ÿæˆã€æµç•…è¯­éŸ³ç”Ÿæˆ

---

## åã€æ€»ç»“

Qwen-VLæ˜¯ä¸€ä¸ª**è®¾è®¡ç²¾å·§ã€è®­ç»ƒå……åˆ†ã€èƒ½åŠ›å…¨é¢**çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹:

**æ ¸å¿ƒä¼˜åŠ¿**:
1. ğŸ† åŒè§„æ¨¡generalistæ¨¡å‹æ€§èƒ½é¢†å…ˆ
2. ğŸŒ åŸç”Ÿä¸­è‹±åŒè¯­æ”¯æŒ
3. ğŸ¯ ç»†ç²’åº¦ç†è§£(grounding + OCR)
4. ğŸ’¡ æ¶æ„ç®€æ´é«˜æ•ˆ(9.6Bå‚æ•°)
5. ğŸ› ï¸ å®Œæ•´å·¥å…·é“¾(æ¨ç†/å¾®è°ƒ/è¯„ä¼°)

**é€‚ç”¨åœºæ™¯**:
- âœ… å¤šè¯­è¨€å›¾åƒç†è§£
- âœ… æ–‡æ¡£/å›¾è¡¨/OCRä»»åŠ¡
- âœ… ç»†ç²’åº¦å®šä½(grounding)
- âœ… å¤šå›¾å¯¹è¯äº¤äº’
- âœ… èµ„æºå—é™éƒ¨ç½²(Int4é‡åŒ–)

**ä»£ç è´¨é‡**:
- å®Œæ•´å¯å¤ç°çš„è®­ç»ƒæµç¨‹
- ä¸°å¯Œçš„å¾®è°ƒé€‰é¡¹(Full/LoRA/Q-LoRA)
- è¯¦ç»†çš„è¯„ä¼°è„šæœ¬
- æ¸…æ™°çš„æ•°æ®æ ¼å¼æ–‡æ¡£

è¿™æ˜¯ä¸€ä¸ª**ç”Ÿäº§çº§**çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹,é€‚åˆä½œä¸ºç ”ç©¶åŸºçº¿æˆ–å•†ä¸šåº”ç”¨çš„èµ·ç‚¹ã€‚

---

## ğŸ‘¤ User

æ—¢ç„¶ä»–æ˜¯ä¸€ä¸ªå¤šæ¨¡æ€æ¨¡å‹,èƒ½å¦ä»‹ç»ä¸€ä¸‹è§†è§‰éƒ¨åˆ†æ€ä¹ˆå¤„ç†,æ–‡æœ¬è¾“å…¥æ€ä¹ˆå¤„ç†,ä¹‹ååˆå¦‚ä½•å¾—åˆ°è¾“å‡º,ä¸‰ä¸ªé˜¶æ®µçš„æŸå¤±å‡½æ•°åˆ†å¸ƒæ˜¯ä»€ä¹ˆ?

---

## ğŸ¤– Claude

éå¸¸å¥½çš„é—®é¢˜ï¼è®©æˆ‘è¯¦ç»†è§£æQwen-VLçš„**å®Œæ•´æ•°æ®æµ**å’Œ**ä¸‰é˜¶æ®µæŸå¤±å‡½æ•°**ã€‚

---

# Qwen-VL æ•°æ®æµä¸æŸå¤±å‡½æ•°è¯¦è§£

## ä¸€ã€å®Œæ•´æ•°æ®æµæ¶æ„

### 1.1 æ€»ä½“æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input Imageâ”‚         â”‚  Input Text  â”‚
â”‚ (448Ã—448Ã—3) â”‚         â”‚  "è¿™æ˜¯ä»€ä¹ˆ?" â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                       â”‚
       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         STEP 1: ç‰¹å¾æå–é˜¶æ®µ              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                       â”‚
       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ViT-bigG   â”‚         â”‚  Tokenizer   â”‚
â”‚   Encoder    â”‚         â”‚  (Qwen-7B)   â”‚
â”‚   (1.9B)     â”‚         â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                        â”‚
       â”‚                        â–¼
       â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                 â”‚ Text Tokens  â”‚
       â”‚                 â”‚ [101, 256,..] â”‚
       â”‚                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â–¼                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚ Image Patchesâ”‚                â”‚
â”‚ (32Ã—32Ã—1536) â”‚                â”‚
â”‚ = 1024 tokensâ”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
       â”‚                        â”‚
       â–¼                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      STEP 2: è§†è§‰ç‰¹å¾å‹ç¼©é˜¶æ®µ             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                        â”‚
       â–¼                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  VL Adapter          â”‚        â”‚
â”‚  (Cross-Attention)   â”‚        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚        â”‚
â”‚  â”‚ Query: 256ä¸ª   â”‚  â”‚        â”‚
â”‚  â”‚ å¯å­¦ä¹ å‘é‡      â”‚  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚        â”‚
â”‚  â”‚ Key/Value:     â”‚  â”‚        â”‚
â”‚  â”‚ 1024ä¸ªpatchç‰¹å¾â”‚  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚        â”‚
â”‚  â”‚ 2D Position    â”‚  â”‚        â”‚
â”‚  â”‚ Encoding       â”‚  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
       â”‚                        â”‚
       â–¼                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚ å‹ç¼©åçš„å›¾åƒ  â”‚                â”‚
â”‚ ç‰¹å¾ (256ç»´)  â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
       â”‚                        â”‚
       â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      STEP 3: ç‰¹å¾æ‹¼æ¥ä¸æ ‡è®°é˜¶æ®µ           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                        â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Sequence Construction:    â”‚
   â”‚  <img> [256ä¸ªimage tokens] â”‚
   â”‚  </img> [text tokens]      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      STEP 4: LLMç”Ÿæˆé˜¶æ®µ                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Qwen-7B LLM  â”‚
   â”‚   (7.7B params)â”‚
   â”‚                â”‚
   â”‚  Transformer   â”‚
   â”‚  Decoder Layersâ”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Output Logits â”‚
   â”‚  (vocab_size)  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Generated     â”‚
   â”‚  Text:         â”‚
   â”‚  "è¿™æ˜¯ä¸€åªæ‹‰å¸ƒ â”‚
   â”‚   æ‹‰å¤šçŠ¬ã€‚"     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## äºŒã€è§†è§‰éƒ¨åˆ†è¯¦ç»†å¤„ç†æµç¨‹

### 2.1 å›¾åƒé¢„å¤„ç†

```python
# ä¼ªä»£ç ç¤ºæ„
def preprocess_image(image_path):
    """
    è¾“å…¥: PIL Image æˆ–å›¾åƒè·¯å¾„
    è¾“å‡º: tensor [3, 448, 448]
    """
    # 1. åŠ è½½å›¾åƒ
    image = load_image(image_path)
    
    # 2. Resizeåˆ°å›ºå®šåˆ†è¾¨ç‡
    # Stage 1: 224Ã—224
    # Stage 2/3: 448Ã—448
    image = resize(image, size=(448, 448))
    
    # 3. å½’ä¸€åŒ– (ImageNetç»Ÿè®¡é‡)
    mean = [0.48145466, 0.4578275, 0.40821073]
    std = [0.26862954, 0.26130258, 0.27577711]
    image = normalize(image, mean, std)
    
    return image  # shape: [3, 448, 448]
```

### 2.2 ViTç¼–ç è¿‡ç¨‹

```python
class VisionTransformer:
    """
    è¾“å…¥: [B, 3, 448, 448]
    è¾“å‡º: [B, 1024, 1536]  # 1024ä¸ªpatch, æ¯ä¸ª1536ç»´
    """
    def __init__(self):
        self.patch_size = 14
        self.patch_embed = Conv2d(3, 1536, kernel_size=14, stride=14)
        self.pos_embed = Parameter(torch.zeros(1, 1024, 1536))
        self.transformer_blocks = ModuleList([...])  # 48å±‚Transformer
        
    def forward(self, x):
        # x: [B, 3, 448, 448]
        
        # 1. Patch Embedding
        # (448/14) Ã— (448/14) = 32 Ã— 32 = 1024 patches
        x = self.patch_embed(x)  # [B, 1536, 32, 32]
        x = x.flatten(2).transpose(1, 2)  # [B, 1024, 1536]
        
        # 2. åŠ å…¥ä½ç½®ç¼–ç 
        x = x + self.pos_embed
        
        # 3. ç»è¿‡Transformerå±‚
        for block in self.transformer_blocks:
            x = block(x)  # Self-Attention + FFN
            
        return x  # [B, 1024, 1536]
```

**å…³é”®ç»†èŠ‚**:
- **Patchæ•°é‡**: (448/14)Â² = 1024ä¸ªpatches
- **ç‰¹å¾ç»´åº¦**: 1536 (ViT-bigGçš„hidden size)
- **æ— CLS token**: ç›´æ¥ä½¿ç”¨æ‰€æœ‰patch tokens

### 2.3 VL Adapterå‹ç¼©

```python
class PositionAwareVLAdapter(nn.Module):
    """
    è¾“å…¥: [B, 1024, 1536] (ViTè¾“å‡º)
    è¾“å‡º: [B, 256, 4096]  (LLMè¾“å…¥ç»´åº¦)
    """
    def __init__(self):
        self.num_queries = 256
        # å¯å­¦ä¹ çš„query embeddings
        self.query_tokens = Parameter(torch.randn(1, 256, 4096))
        
        # Cross-Attentionå±‚
        self.cross_attn = MultiheadAttention(
            embed_dim=4096,
            num_heads=32,
            kdim=1536,  # ViTç‰¹å¾ç»´åº¦
            vdim=1536
        )
        
        # 2Dä½ç½®ç¼–ç 
        self.position_encoding_2d = build_2d_sincos_position_embedding(
            h=32, w=32, embed_dim=1536
        )
        
    def forward(self, image_features):
        # image_features: [B, 1024, 1536]
        B = image_features.size(0)
        
        # 1. åŠ å…¥2Dä½ç½®ç¼–ç åˆ°Key
        pos_enc = self.position_encoding_2d  # [1024, 1536]
        keys = image_features + pos_enc.unsqueeze(0)
        
        # 2. Query: å¯å­¦ä¹ å‘é‡
        queries = self.query_tokens.expand(B, -1, -1)  # [B, 256, 4096]
        
        # 3. Cross-Attention
        # Q: [B, 256, 4096]
        # K, V: [B, 1024, 1536] (è‡ªåŠ¨æŠ•å½±åˆ°4096ç»´)
        output, _ = self.cross_attn(
            query=queries.transpose(0, 1),      # [256, B, 4096]
            key=keys.transpose(0, 1),           # [1024, B, 1536]
            value=image_features.transpose(0, 1)  # [1024, B, 1536]
        )
        
        output = output.transpose(0, 1)  # [B, 256, 4096]
        return output
```

**æ ¸å¿ƒæœºåˆ¶**:
1. **Query**: 256ä¸ªå¯å­¦ä¹ å‘é‡(ç±»ä¼¼DETRçš„object queries)
2. **Key/Value**: æ¥è‡ªViTçš„1024ä¸ªpatchç‰¹å¾
3. **ä½ç½®ä¿¡æ¯**: é€šè¿‡2Dä½ç½®ç¼–ç æ³¨å…¥åˆ°Keyä¸­
4. **è¾“å‡º**: 256ä¸ªå‹ç¼©åçš„è§†è§‰token,ç»´åº¦å¯¹é½LLM(4096)

---

## ä¸‰ã€æ–‡æœ¬éƒ¨åˆ†å¤„ç†æµç¨‹

### 3.1 Tokenization

```python
class QwenTokenizer:
    """
    åŸºäºQwen-7Bçš„tokenizer
    è¯æ±‡è¡¨å¤§å°: ~151,851
    """
    def __init__(self):
        # ç‰¹æ®Štoken
        self.img_start_id = 151857  # <img>
        self.img_end_id = 151858    # </img>
        self.im_start_id = 151644   # <im_start>
        self.im_end_id = 151645     # <im_end>
        self.box_start_id = 151859  # <box>
        self.box_end_id = 151860    # </box>
        self.ref_start_id = 151861  # <ref>
        self.ref_end_id = 151862    # </ref>
        
    def encode(self, text):
        """
        è¾“å…¥: "è¿™æ˜¯ä»€ä¹ˆ?"
        è¾“å‡º: [101, 256, 389, 102]
        """
        return self.tokenize(text)
    
    def from_list_format(self, content_list):
        """
        è¾“å…¥: [
            {'image': 'path/to/img.jpg'},
            {'text': 'è¿™æ˜¯ä»€ä¹ˆ?'}
        ]
        è¾“å‡º: "<img>path/to/img.jpg</img>è¿™æ˜¯ä»€ä¹ˆ?"
        """
        text = ""
        for item in content_list:
            if 'image' in item:
                text += f"<img>{item['image']}</img>"
            elif 'text' in item:
                text += item['text']
        return text
```

### 3.2 æ–‡æœ¬Embedding

```python
def text_embedding(text_tokens):
    """
    è¾“å…¥: [101, 256, 389, 102]  # token IDs
    è¾“å‡º: [B, seq_len, 4096]
    """
    # 1. Token Embedding
    token_embeds = embedding_layer(text_tokens)  # [B, seq_len, 4096]
    
    # 2. ä½ç½®ç¼–ç  (RoPE)
    # Qwenä½¿ç”¨RoPE (Rotary Position Embedding)
    # åœ¨Attentionå±‚ä¸­åŠ¨æ€è®¡ç®—ï¼Œæ­¤å¤„ä¸æ˜¾å¼æ·»åŠ 
    
    return token_embeds
```

---

## å››ã€å¤šæ¨¡æ€èåˆæœºåˆ¶

### 4.1 åºåˆ—æ„é€ 

```python
def construct_multimodal_sequence(image_features, text_tokens):
    """
    è¾“å…¥:
      - image_features: [B, 256, 4096] (æ¥è‡ªVL Adapter)
      - text_tokens: [B, text_len]
    
    è¾“å‡º:
      - input_embeds: [B, 256 + 2 + text_len, 4096]
    """
    B = image_features.size(0)
    
    # 1. è·å–ç‰¹æ®Štokençš„embedding
    img_start_embed = embedding_layer(torch.tensor([151857]))  # <img>
    img_end_embed = embedding_layer(torch.tensor([151858]))    # </img>
    
    # 2. è·å–æ–‡æœ¬embedding
    text_embeds = embedding_layer(text_tokens)  # [B, text_len, 4096]
    
    # 3. æ‹¼æ¥åºåˆ—
    # æ ¼å¼: <img> [256ä¸ªimage tokens] </img> [text tokens]
    input_embeds = torch.cat([
        img_start_embed.expand(B, 1, -1),  # [B, 1, 4096]
        image_features,                     # [B, 256, 4096]
        img_end_embed.expand(B, 1, -1),    # [B, 1, 4096]
        text_embeds                         # [B, text_len, 4096]
    ], dim=1)
    
    # æ€»é•¿åº¦: 1 + 256 + 1 + text_len = 258 + text_len
    return input_embeds
```

**åºåˆ—ç¤ºä¾‹**:
```
è¾“å…¥: å›¾åƒ + "è¿™æ˜¯ä»€ä¹ˆ?"

Tokenåºåˆ—:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ <img> â”‚ [256ä¸ªè§†è§‰token]      â”‚ </img> â”‚ è¿™ æ˜¯ ä»€ ä¹ˆ ? â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  ç‰¹æ®Š   â”‚    æ¥è‡ªVL Adapter     â”‚  ç‰¹æ®Š  â”‚   æ–‡æœ¬tokens   â”‚
  token  â”‚    çš„å›¾åƒç‰¹å¾          â”‚  token â”‚                â”‚
         â””â”€â”€â”€â”€â”€â”€â”€ 258 tokens â”€â”€â”€â”€â”€â”˜

Embeddingç»´åº¦:
æ¯ä¸ªä½ç½®: [4096ç»´å‘é‡]
```

### 4.2 LLMå¤„ç†

```python
class QwenLMWithVision(nn.Module):
    def forward(self, input_embeds):
        """
        è¾“å…¥: [B, seq_len, 4096]
        è¾“å‡º: [B, seq_len, vocab_size]
        """
        # 1. ç»è¿‡Transformer Decoderå±‚
        hidden_states = input_embeds
        
        for layer in self.transformer_layers:  # 32å±‚
            # Self-Attention (å¸¦RoPEä½ç½®ç¼–ç )
            hidden_states = layer.self_attn(hidden_states)
            # Feed-Forward
            hidden_states = layer.ffn(hidden_states)
        
        # 2. è¾“å‡ºæŠ•å½±
        logits = self.lm_head(hidden_states)  # [B, seq_len, 151851]
        
        return logits
```

**å…³é”®ç‚¹**:
- **å›¾åƒtokenå‚ä¸Attention**: è§†è§‰ç‰¹å¾ä¸æ–‡æœ¬tokené€šè¿‡Self-Attentionäº¤äº’
- **ä½ç½®ç¼–ç **: RoPEä¸ºæ‰€æœ‰token(åŒ…æ‹¬å›¾åƒ)æä¾›ä½ç½®ä¿¡æ¯
- **Causal Mask**: è®­ç»ƒæ—¶ä½¿ç”¨å› æœæ©ç ,ç¡®ä¿è‡ªå›å½’ç”Ÿæˆ

---

## äº”ã€ä¸‰é˜¶æ®µæŸå¤±å‡½æ•°è¯¦è§£

### 5.1 Stage 1: é¢„è®­ç»ƒé˜¶æ®µ

#### **æŸå¤±å‡½æ•°**

```python
def stage1_loss(logits, labels, image_mask):
    """
    ä»…å¯¹æ–‡æœ¬tokenè®¡ç®—æŸå¤±
    
    å‚æ•°:
      - logits: [B, seq_len, vocab_size]
      - labels: [B, seq_len]  # ground truth tokens
      - image_mask: [B, seq_len]  # æ ‡è®°å“ªäº›ä½ç½®æ˜¯å›¾åƒtoken
    
    è¿”å›:
      - loss: æ ‡é‡
    """
    # 1. æ„é€ æŸå¤±æ©ç 
    # å›¾åƒtokenä½ç½®: IGNORE (-100)
    # æ–‡æœ¬tokenä½ç½®: æ­£å¸¸è®¡ç®—
    loss_mask = (image_mask == 0)  # False for image, True for text
    
    # 2. å°†å›¾åƒtokenä½ç½®çš„labelè®¾ä¸º-100
    masked_labels = labels.clone()
    masked_labels[~loss_mask] = -100
    
    # 3. è®¡ç®—Cross-Entropy Loss
    # shift prediction: é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
    shift_logits = logits[:, :-1, :].contiguous()
    shift_labels = masked_labels[:, 1:].contiguous()
    
    loss_fct = CrossEntropyLoss(ignore_index=-100)
    loss = loss_fct(
        shift_logits.view(-1, shift_logits.size(-1)),
        shift_labels.view(-1)
    )
    
    return loss
```

#### **æŸå¤±è®¡ç®—ç¤ºä¾‹**

```
åºåˆ—:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚ <img> â”‚ [256 img toks] â”‚ </img> â”‚ è¿™  â”‚ æ˜¯ â”‚ ä¸€ â”‚ åª â”‚ ç‹— â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜

Labels (è¦é¢„æµ‹çš„ä¸‹ä¸€ä¸ªtoken):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ [256 img toks] â”‚ </img> â”‚ è¿™  â”‚ æ˜¯ â”‚ ä¸€ â”‚ åª â”‚ ç‹— â”‚ EOS â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

Loss Mask (å“ªäº›ä½ç½®è®¡ç®—æŸå¤±):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚    IGNORE      â”‚ IGNORE â”‚  âœ“  â”‚ âœ“ â”‚ âœ“ â”‚ âœ“ â”‚ âœ“ â”‚  âœ“  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
                           â””â”€â”€â”€â”€â”€â”€ ä»…è¿™äº›ä½ç½®è®¡ç®—loss â”€â”€â”€â”€â”€â”˜

å®é™…Loss:
loss = CE(pred_è¿™, label_è¿™) + CE(pred_æ˜¯, label_æ˜¯) + ... 
       + CE(pred_EOS, label_EOS)
```

**è®­ç»ƒç­–ç•¥**:
- **å†»ç»“**: LLMå…¨éƒ¨å‚æ•°
- **è®­ç»ƒ**: ViT + VL Adapter
- **æ•°æ®**: 1.4Bå›¾æ–‡å¯¹
- **ç›®æ ‡**: è®©è§†è§‰ç‰¹å¾å¯¹é½åˆ°LLMçš„è¯­ä¹‰ç©ºé—´

---

### 5.2 Stage 2: å¤šä»»åŠ¡é¢„è®­ç»ƒ

#### **æŸå¤±å‡½æ•° (æœ¬è´¨ç›¸åŒ,ä½†æ•°æ®æ ¼å¼å¤šæ ·)**

```python
def stage2_loss(logits, labels, task_type):
    """
    ä¸åŒä»»åŠ¡ä½¿ç”¨ç›¸åŒçš„losså‡½æ•°,ä½†labelæ„é€ ä¸åŒ
    """
    # åŸºç¡€lossä»æ˜¯Cross-Entropy
    loss = CrossEntropyLoss(ignore_index=-100)
    
    # æ ¹æ®ä»»åŠ¡ç±»å‹,labelæ ¼å¼ä¸åŒ:
    if task_type == "caption":
        # <img>...</img>Generate the caption: [caption]<eos>
        # ä»…å¯¹captionéƒ¨åˆ†è®¡ç®—loss
        
    elif task_type == "vqa":
        # <img>...</img>[question] Answer: [answer]<eos>
        # ä»…å¯¹answeréƒ¨åˆ†è®¡ç®—loss
        
    elif task_type == "grounding":
        # <img>...</img>Generate with grounding: 
        # <ref>text</ref><box>(x,y),(x,y)</box>...<eos>
        # å¯¹æ•´ä¸ªgrounded captionè®¡ç®—loss
        
    elif task_type == "ocr":
        # <img>...</img>OCR: <ref>text</ref><quad>...</quad><eos>
        # å¯¹OCRæ–‡æœ¬å’Œåæ ‡éƒ½è®¡ç®—loss
        
    return loss(logits, labels)
```

#### **å„ä»»åŠ¡çš„Lossè®¡ç®—**

**Captionä»»åŠ¡**:
```
è¾“å…¥åºåˆ—:
<img>[256 toks]</img>Generate the caption in English:

ç›®æ ‡åºåˆ— (è®¡ç®—loss):
                                                     A woman playing with her dog.<eos>
                                                     â””â”€â”€â”€â”€â”€â”€ ä»…æ­¤éƒ¨åˆ†å‚ä¸loss â”€â”€â”€â”€â”€â”€â”˜
```

**Groundingä»»åŠ¡**:
```
è¾“å…¥åºåˆ—:
<img>[256 toks]</img>Generate with grounding:

ç›®æ ‡åºåˆ— (è®¡ç®—loss):
                                            <ref>woman</ref><box>(100,200),(300,400)</box>
                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ æ–‡æœ¬+åæ ‡éƒ½å‚ä¸loss â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**VQAä»»åŠ¡**:
```
è¾“å…¥åºåˆ—:
<img>[256 toks]</img>What is in the image? Answer:

ç›®æ ‡åºåˆ—:
                                                        A dog.<eos>
                                                        â””â”€ loss â”€â”˜
```

**è®­ç»ƒç­–ç•¥**:
- **è§£å†»**: å…¨æ¨¡å‹å‚æ•°å¯è®­ç»ƒ
- **åˆ†è¾¨ç‡**: 448Ã—448
- **æ•°æ®**: 70M+ å¤šä»»åŠ¡æ ·æœ¬
- **ç›®æ ‡**: å¢å¼ºå¤šä»»åŠ¡èƒ½åŠ›,å¼•å…¥groundingå’ŒOCR

---

### 5.3 Stage 3: æŒ‡ä»¤å¾®è°ƒ

#### **æŸå¤±å‡½æ•° (ChatMLæ ¼å¼)**

```python
def stage3_sft_loss(logits, labels, role_tokens):
    """
    ä»…å¯¹assistantå›ç­”è®¡ç®—æŸå¤±
    
    ChatMLæ ¼å¼:
    <im_start>user\n{user_input}<im_end>\n
    <im_start>assistant\n{assistant_output}<im_end>\n
    """
    masked_labels = labels.clone()
    
    # 1. System prompt â†’ IGNORE
    # 2. User input â†’ IGNORE
    # 3. Assistant output â†’ è®¡ç®—loss
    # 4. ç‰¹æ®Štoken (<im_end>ç­‰) â†’ è®¡ç®—loss
    
    for i, role in enumerate(role_tokens):
        if role == "user" or role == "system":
            # æ‰¾åˆ°è¿™æ®µçš„èµ·å§‹å’Œç»“æŸ
            start_idx = find_role_start(i)
            end_idx = find_role_end(i)
            # é™¤äº†<im_end>,å…¶ä½™éƒ½IGNORE
            masked_labels[start_idx:end_idx-1] = -100
            
        elif role == "assistant":
            # ä¿ç•™è¿™æ®µç”¨äºè®¡ç®—loss
            # ä½†role tokenæœ¬èº«è¦IGNORE
            role_token_idx = find_role_token_idx(i)
            masked_labels[role_token_idx] = -100
    
    loss = CrossEntropyLoss(ignore_index=-100)
    return loss(logits, masked_labels)
```

#### **SFT Lossè®¡ç®—ç¤ºä¾‹**

```
å®Œæ•´åºåˆ—:
<im_start>system
You are a helpful assistant.<im_end>
<im_start>user
Picture 1: <img>[256 toks]</img>
è¿™æ˜¯ä»€ä¹ˆ?<im_end>
<im_start>assistant
è¿™æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚<im_end>

Loss Mask:
<im_start>system                              â† IGNORE
You are a helpful assistant.                  â† IGNORE
<im_end>                                       â† âœ“ è®¡ç®—loss (ç‰¹æ®Štoken)

<im_start>user                                 â† IGNORE
Picture 1: <img>[256 toks]</img>è¿™æ˜¯ä»€ä¹ˆ?      â† IGNORE
<im_end>                                       â† âœ“ è®¡ç®—loss

<im_start>assistant                            â† IGNORE (role token)
è¿™æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚                             â† âœ“âœ“âœ“ è®¡ç®—loss
<im_end>                                       â† âœ“ è®¡ç®—loss

å®é™…Lossä»…æ¥è‡ª:
1. ç‰¹æ®Štoken: <im_end> (3æ¬¡)
2. Assistantå›ç­”: "è¿™æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚"
```

**ä»£ç å®ç°** (æ¥è‡ªfinetune.py:124-175):

```python
def preprocess(sources, tokenizer, max_len):
    im_start = tokenizer.im_start_id
    im_end = tokenizer.im_end_id
    
    for source in sources:
        input_id, target = [], []
        
        # System prompt
        system = [im_start] + _system + system_content + [im_end]
        input_id += system
        target += [im_start] + [IGNORE_TOKEN_ID]*(len(system)-3) + [im_end]
        
        # User/Assistant turns
        for sentence in source:
            role = sentence["from"]
            _input_id = tokenizer(role).input_ids + \
                        tokenizer(sentence["value"]).input_ids + [im_end]
            input_id += _input_id
            
            if role == 'user':
                # Userè¾“å…¥: å…¨éƒ¨IGNORE,é™¤äº†<im_end>
                _target = [im_start] + [IGNORE_TOKEN_ID]*(len(_input_id)-3) + [im_end]
            elif role == 'assistant':
                # Assistantå›ç­”: è®¡ç®—loss,ä½†role tokenè¦IGNORE
                _target = [im_start] + [IGNORE_TOKEN_ID]*len(role_tokens) + \
                          _input_id[len(role_tokens)+1:-2] + [im_end]
            
            target += _target
    
    return input_id, target
```

**è®­ç»ƒç­–ç•¥**:
- **å†»ç»“**: ViT
- **è®­ç»ƒ**: LLM + VL Adapter
- **æ•°æ®**: 350Kå¯¹è¯æ ·æœ¬
- **ç›®æ ‡**: å¯¹é½äººç±»åå¥½,æå‡å¯¹è¯èƒ½åŠ›

---

## å…­ã€ä¸‰é˜¶æ®µå¯¹æ¯”æ€»ç»“

| ç»´åº¦ | Stage 1 | Stage 2 | Stage 3 |
|------|---------|---------|---------|
| **åç§°** | é¢„è®­ç»ƒ | å¤šä»»åŠ¡é¢„è®­ç»ƒ | æŒ‡ä»¤å¾®è°ƒ |
| **åˆ†è¾¨ç‡** | 224Ã—224 | 448Ã—448 | 448Ã—448 |
| **å¯è®­ç»ƒå‚æ•°** | ViT + Adapter | å…¨éƒ¨ | LLM + Adapter |
| **æ•°æ®é‡** | 1.4B | 70M+ | 350K |
| **æ•°æ®ç±»å‹** | å›¾æ–‡å¯¹ | 7ç§ä»»åŠ¡ | å¯¹è¯æ•°æ® |
| **LossèŒƒå›´** | æ–‡æœ¬caption | ä»»åŠ¡ç­”æ¡ˆ | Assistantå›ç­” |
| **Losså‡½æ•°** | CrossEntropy | CrossEntropy | CrossEntropy |
| **ç‰¹æ®Šä¹‹å¤„** | å›¾åƒtokenä¸è®¡ç®—loss | ä¸åŒä»»åŠ¡labelæ ¼å¼ä¸åŒ | user/systeméƒ¨åˆ†IGNORE |
| **è®­ç»ƒæ­¥æ•°** | 50K | 19K | 8K |
| **Batch Size** | 30720 | 4096 | 128 |
| **Peak LR** | 2e-4 | 5e-5 | 1e-5 |

---

## ä¸ƒã€å®Œæ•´æ¨ç†è¿‡ç¨‹ç¤ºä¾‹

è®©æˆ‘ç”¨ä¸€ä¸ªå®Œæ•´ä¾‹å­ä¸²è”æ•´ä¸ªæµç¨‹:

### è¾“å…¥
```python
image = "dog.jpg"  # ä¸€å¼ æ‹‰å¸ƒæ‹‰å¤šçš„ç…§ç‰‡
query = "è¿™æ˜¯ä»€ä¹ˆå“ç§çš„ç‹—?"
```

### Step 1: è§†è§‰ç¼–ç 
```
dog.jpg (åŸå§‹å›¾åƒ)
  â†“ resize
[3, 448, 448] (å½’ä¸€åŒ–tensor)
  â†“ ViT Encoder
[1024, 1536] (1024ä¸ªpatchç‰¹å¾)
  â†“ VL Adapter (Cross-Attention + 2D PE)
[256, 4096] (å‹ç¼©åçš„è§†è§‰token)
```

### Step 2: æ–‡æœ¬ç¼–ç 
```
"è¿™æ˜¯ä»€ä¹ˆå“ç§çš„ç‹—?"
  â†“ Tokenizer
[151857, 28450, 25608, 31893, 102, 151858]
# <img>   è¿™     æ˜¯    ä»€ä¹ˆ   ç‹—    </img>
  â†“ Embedding
[6, 4096] (6ä¸ªtokençš„embedding)
```

### Step 3: åºåˆ—æ‹¼æ¥
```
[1 + 256 + 1 + 6, 4096]
= [264, 4096]

è¯¦ç»†ç»“æ„:
Position 0:     <img> embedding
Position 1-256: 256ä¸ªè§†è§‰token (æ¥è‡ªVL Adapter)
Position 257:   </img> embedding
Position 258:   "è¿™" embedding
Position 259:   "æ˜¯" embedding
Position 260:   "ä»€ä¹ˆ" embedding
Position 261:   "å“ç§" embedding
Position 262:   "çš„" embedding
Position 263:   "ç‹—" embedding
```

### Step 4: LLMç”Ÿæˆ
```
[264, 4096] (è¾“å…¥embeddings)
  â†“ 32å±‚Transformer Decoder
[264, 4096] (hidden states)
  â†“ LM Head
[264, 151851] (logits over vocabulary)
  â†“ Sampling (æœ€åä¸€ä¸ªä½ç½®)
Token ID: 45892 ("è¿™")
  â†“ è‡ªå›å½’ç»§ç»­ç”Ÿæˆ
"è¿™æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚"
```

### Step 5: è¾“å‡º
```
å®Œæ•´è¾“å‡ºåºåˆ—:
<img>[256 toks]</img>è¿™æ˜¯ä»€ä¹ˆå“ç§çš„ç‹—?è¿™æ˜¯ä¸€åªæ‹‰å¸ƒæ‹‰å¤šçŠ¬ã€‚<eos>
                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      ç”Ÿæˆçš„å›ç­”éƒ¨åˆ†
```

---

## å…«ã€å…³é”®è®¾è®¡æ€æƒ³æ€»ç»“

### 8.1 ä¸ºä»€ä¹ˆå›¾åƒtokenä¸è®¡ç®—loss?

**åŸå› **:
1. **æ— ç›‘ç£ä¿¡å·**: å›¾åƒpatchæ²¡æœ‰"æ­£ç¡®ç­”æ¡ˆ"
2. **å¯¹é½ç›®æ ‡**: è®©è§†è§‰ç‰¹å¾å­¦ä¼šé¢„æµ‹æ–‡æœ¬,è€Œéé‡å»ºå›¾åƒ
3. **æ•ˆç‡**: é¿å…åœ¨é«˜ç»´è§†è§‰ç©ºé—´è®¡ç®—æ— æ„ä¹‰çš„loss

### 8.2 ä¸ºä»€ä¹ˆéœ€è¦VL Adapter?

**åŸå› **:
1. **ç»´åº¦å‹ç¼©**: 1024â†’256,é™ä½è®¡ç®—æˆæœ¬
2. **ç‰¹å¾å¯¹é½**: ViTç‰¹å¾(1536ç»´) â†’ LLMç©ºé—´(4096ç»´)
3. **ä½ç½®ä¿ç•™**: 2Dä½ç½®ç¼–ç ä¿ç•™ç©ºé—´ä¿¡æ¯,æ”¯æŒgrounding

### 8.3 ä¸ºä»€ä¹ˆä¸‰é˜¶æ®µè®­ç»ƒ?

**åŸå› **:
1. **Stage 1**: å¤§è§„æ¨¡å¼±ç›‘ç£æ•°æ®å»ºç«‹è§†è§‰-è¯­è¨€æ˜ å°„
2. **Stage 2**: é«˜è´¨é‡ä»»åŠ¡æ•°æ®å¢å¼ºç»†ç²’åº¦èƒ½åŠ›
3. **Stage 3**: å¯¹è¯æ•°æ®å¯¹é½äººç±»åå¥½

### 8.4 ç»Ÿä¸€Lossçš„ä¼˜åŠ¿

**ä¼˜åŠ¿**:
- **ç®€æ´**: æ‰€æœ‰ä»»åŠ¡å…±äº«åŒä¸€losså‡½æ•°
- **çµæ´»**: é€šè¿‡æ•°æ®æ ¼å¼æ§åˆ¶ä¸åŒä»»åŠ¡
- **å¯æ‰©å±•**: æ·»åŠ æ–°ä»»åŠ¡æ— éœ€ä¿®æ”¹æ¨¡å‹æ¶æ„




